{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do(f,n):\n",
    "    b = []\n",
    "    for i in range(n):\n",
    "        b.append(f())\n",
    "    return np.array(b)\n",
    "\n",
    "def rrow(a):\n",
    "    ri = np.random.randint(0,np.shape(a)[0])\n",
    "    return a[ri]\n",
    "\n",
    "def rmult(r,n):\n",
    "    es=np.random.uniform(0,1,r)\n",
    "    es2 = es/np.mean(es)\n",
    "    np.random.seed(514)\n",
    "    x = sp.stats.random_correlation.rvs(es2)\n",
    "\n",
    "    return do(lambda : sp.stats.multivariate_normal.rvs(mean=[100.0]*d, cov=x),n)\n",
    "\n",
    "def irange(a,b):\n",
    "    return np.arange(a,b+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rts = irange(0,30)\n",
    "Fts = [37,52,73,84,90,109,128,140,152,155]\n",
    "Fts2 = np.concatenate(([30],Fts))\n",
    "Fvs = []\n",
    "for i in range(len(Fts2)-1):\n",
    "    Fvs.append(irange(Fts2[i]+1,Fts2[i+1]-1))\n",
    "Ft_v = {}\n",
    "Ft_i = {}\n",
    "for i,t,v in zip(range(len(Fts)),Fts,Fvs):\n",
    "    Ft_v[t] = v\n",
    "    Ft_i[t] = i\n",
    "\n",
    "    \n",
    "actions = np.concatenate((Rts,Fts))\n",
    "states = np.concatenate((Fts, np.concatenate(Fvs)))\n",
    "\n",
    "var_to_state = {}\n",
    "for F in states:\n",
    "    var_to_state[F] = np.where(states==F)[0][0]\n",
    "\n",
    "def action_to_choice(action):\n",
    "    x=actions[action]\n",
    "    if x in Rts:\n",
    "        return \"exploit\", x\n",
    "    else:\n",
    "        return \"explore\", Ft_i[x]\n",
    "    \n",
    "class SAT_env:\n",
    "    def __init__(self, da):\n",
    "        self.da = da\n",
    "        #self.num_r = num_r\n",
    "        #self.num_f = num_f\n",
    "        #self.n = num_r + num_f\n",
    "        #self.s = num_f\n",
    "        self.unknown = -100.0\n",
    "        self.n = len(actions)\n",
    "        self.s = len(states)\n",
    "    def reset(self):\n",
    "        self.state = np.array([self.unknown] * len(states))\n",
    "        self.truth = rrow(self.da)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        choice_type,choice_i = action_to_choice(action)\n",
    "        \n",
    "        if(choice_type == \"exploit\"):\n",
    "            Rt = Rts[choice_i]\n",
    "            done=True\n",
    "            reward = -self.truth[Rt]\n",
    "        else:\n",
    "            done = False\n",
    "            Ft = Fts[choice_i]\n",
    "            #if self.state[var_to_state[Ft]] != self.unknown:\n",
    "                #return self.step(np.random.randint(0,self.n))\n",
    "            \n",
    "            Fvs = Ft_v[Ft]\n",
    "            reward = -self.truth[Ft]\n",
    "            for F in np.concatenate(([Ft],Fvs)):\n",
    "                self.state[var_to_state[F]] = self.truth[F]\n",
    "        '''    \n",
    "        if(self.choose_solver(action)):\n",
    "            done = True\n",
    "            #self.timesofar += self.solver_time(action)\n",
    "            reward = -self.solver_time(action)#self.timesofar\n",
    "            done = True\n",
    "        else:\n",
    "            if self.state[action-self.num_r] != self.unknown:\n",
    "                return self.step(np.random.randint(0,self.n))\n",
    "\n",
    "            done = False\n",
    "            #self.timesofar += self.feature_time(action)\n",
    "            reward = -self.feature_time(action)\n",
    "            self.state[action - self.num_r] = self.feature_value(action)\n",
    "        ,\"\"\n",
    "        '''\n",
    "        return self.state,reward,done\n",
    "        \n",
    "    def solver_time(self,action):\n",
    "        return self.truth[action]\n",
    "    def choose_solver(self,action):\n",
    "        return action < self.num_r\n",
    "    def feature_value(self,action):\n",
    "        return self.truth[action]\n",
    "    def feature_time(self,action):\n",
    "        return self.truth[action + self.num_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iss=[]\n",
    "for i in range(len(actions)):\n",
    "    if actions[i] in Rts:\n",
    "        iss.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "excoef = 1\n",
    "#adapted with modifications from https://adventuresinmachinelearning.com/double-q-reinforcement-learning-in-tensorflow-2/\n",
    "def dqn(env):\n",
    "    STORE_PATH = ''\n",
    "    MAX_EPSILON = 1\n",
    "    MIN_EPSILON = 0.01\n",
    "    LAMBDA = 0.0005\n",
    "    GAMMA = 0.95\n",
    "    BATCH_SIZE = 32\n",
    "    TAU = 0.08\n",
    "    RANDOM_REWARD_STD = 1.0\n",
    "\n",
    "    #env = gym.make(\"CartPole-v0\")\n",
    "    state_size = env.s\n",
    "    num_actions = env.n\n",
    "\n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "        keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "        keras.layers.Dense(num_actions)\n",
    "    ])\n",
    "\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "        keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "        keras.layers.Dense(num_actions)\n",
    "    ])\n",
    "\n",
    "    primary_network.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "\n",
    "\n",
    "    class Memory:\n",
    "        def __init__(self, max_memory):\n",
    "            self._max_memory = max_memory\n",
    "            self._samples = []\n",
    "\n",
    "        def add_sample(self, sample):\n",
    "            self._samples.append(sample)\n",
    "            if len(self._samples) > self._max_memory:\n",
    "                self._samples.pop(0)\n",
    "\n",
    "        def sample(self, no_samples):\n",
    "            if no_samples > len(self._samples):\n",
    "                return random.sample(self._samples, len(self._samples))\n",
    "            else:\n",
    "                return random.sample(self._samples, no_samples)\n",
    "\n",
    "        @property\n",
    "        def num_samples(self):\n",
    "            return len(self._samples)\n",
    "\n",
    "    memory = Memory(500000)\n",
    "    #actions = np.concatenate((Rts,Fts))\n",
    "    #states = np.concatenate((Fts, np.concatenate(Fvs)))\n",
    "    min_explore = 0\n",
    "    def choose_action(state, primary_network, eps):\n",
    "        allqs = np.array(primary_network(state.reshape(1, -1)))[0]\n",
    "        choices_i = []\n",
    "        qs = []\n",
    "        known = 0\n",
    "        for i in range(len(allqs)):\n",
    "            if ((not actions[i] in Rts) and state[var_to_state[actions[i]]] != env.unknown):\n",
    "                known += 1\n",
    "        #print(known)\n",
    "\n",
    "        for i in range(len(allqs)):\n",
    "            if (known >= min_explore and actions[i] in Rts) or ((not actions[i] in Rts) and state[var_to_state[actions[i]]] == env.unknown):\n",
    "                if actions[i] in Rts:\n",
    "                    qs.append(excoef * allqs[i])\n",
    "                else:\n",
    "                    qs.append(allqs[i])\n",
    "                choices_i.append(i) \n",
    "        if random.random() < eps:\n",
    "            exs = np.setdiff1d(choices_i,iss)\n",
    "            if random.random() < 0.5 and not len(exs)==0:\n",
    "                ret = np.random.choice(exs,1)[0] \n",
    "            else:\n",
    "                ret = np.random.choice(iss,1)[0]\n",
    "            #return random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            #print(allqs)\n",
    "            #print(qs)\n",
    "            #print(state)\n",
    "            ret= choices_i[np.argmax(qs)]\n",
    "        #print(ret)\n",
    "\n",
    "        return int(ret)\n",
    "        \n",
    "    def plus(a,b):\n",
    "        return np.add(a, b, out=a, casting=\"unsafe\")\n",
    "\n",
    "    def train(primary_network, memory, target_network=None):\n",
    "        if memory.num_samples < BATCH_SIZE * 3:\n",
    "            return 0\n",
    "        batch = memory.sample(BATCH_SIZE)\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        actions = np.array([val[1] for val in batch])\n",
    "        rewards = np.array([val[2] for val in batch])\n",
    "        next_states = np.array([(np.zeros(state_size)\n",
    "                                if val[3] is None else val[3]) for val in batch])\n",
    "        # predict Q(s,a) given the batch of states\n",
    "        prim_qt = primary_network(states)\n",
    "        # predict Q(s',a') from the evaluation network\n",
    "        prim_qtp1 = primary_network(next_states)\n",
    "        # copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\n",
    "        target_q = prim_qt.numpy()\n",
    "        updates = rewards\n",
    "        valid_idxs = np.array(next_states).sum(axis=1) != 0\n",
    "        batch_idxs = np.arange(BATCH_SIZE)\n",
    "        if target_network is None:\n",
    "            #updates[valid_idxs] += GAMMA * np.amax(prim_qtp1.numpy()[valid_idxs, :], axis=1)\n",
    "            #arr = np.add(arr, image.flatten(), out=arr, casting=\"unsafe\")\n",
    "            updates[valid_idxs] = plus(updates[valid_idxs], GAMMA * np.amax(prim_qtp1.numpy()[valid_idxs, :], axis=1))\n",
    "        else:\n",
    "            prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "            q_from_target = target_network(next_states)\n",
    "            #updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]]\n",
    "            updates[valid_idxs] = plus(updates[valid_idxs], GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]])\n",
    "        target_q[batch_idxs, actions] = updates\n",
    "        loss = primary_network.train_on_batch(states, target_q)\n",
    "        if target_network is not None:\n",
    "            # update target network parameters slowly from primary network\n",
    "            for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "                t.assign(t * (1 - TAU) + e * TAU)\n",
    "        return loss\n",
    "\n",
    "    num_episodes = 1000\n",
    "    eps = MAX_EPSILON\n",
    "    render = False\n",
    "    train_writer = tf.summary.create_file_writer(STORE_PATH + f\"/DoubleQ_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "    double_q = False\n",
    "    steps = 0\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        cnt = 1\n",
    "        avg_loss = 0\n",
    "        t_reward = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            t_reward += reward\n",
    "            #reward = np.random.normal(1.0, RANDOM_REWARD_STD)\n",
    "            if done:\n",
    "                next_state = None\n",
    "            # store in memory\n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            loss = train(primary_network, memory, target_network if double_q else None)\n",
    "            avg_loss += loss\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # exponentially decay the eps value\n",
    "            steps += 1\n",
    "            eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * steps)\n",
    "\n",
    "            if done:\n",
    "                avg_loss /= cnt\n",
    "                print(f\"Episode: {i}, Reward: {t_reward}, avg loss: {avg_loss:.3f}, eps: {eps:.3f}\")\n",
    "                with train_writer.as_default():\n",
    "                    tf.summary.scalar('reward', t_reward, step=i)\n",
    "                    tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "                break\n",
    "\n",
    "            cnt += 1\n",
    "    def Ksks_to_state(Ks,ks):\n",
    "        state = np.full(env.s,env.unknown)\n",
    "        for Ksv,ksv in zip(Ks,ks):\n",
    "            state[var_to_state[int(Ksv)]] = ksv\n",
    "        return state\n",
    "    \n",
    "                                                 \n",
    "    def f(Ks,ks):\n",
    "        k_state = Ksks_to_state(Ks,ks)\n",
    "        action = choose_action(k_state, primary_network, 0)\n",
    "        return action_to_choice(action)\n",
    "\n",
    "        '''\n",
    "        def lf(action):\n",
    "            typ,i = action_to_choice(action)\n",
    "            if typ == \"exploit\":\n",
    "                return typ,i\n",
    "            else:\n",
    "                Ft = Fts[i]\n",
    "                if k_state[var_to_state[Ft]] != env.unknown:\n",
    "                    print(\"TRY AGAIN!\")\n",
    "                    return lf(np.random.randint(0,env.n))\n",
    "                else:\n",
    "                    return typ,i\n",
    "        return lf(action)\n",
    "        '''\n",
    "                                                 \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn_get_model(data):\n",
    "    myS = SAT_env(data)\n",
    "    return dqn(myS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setexcoef(v):\n",
    "    global excoef\n",
    "    excoef = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getexcoef():\n",
    "    return excoef"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
