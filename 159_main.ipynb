{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import model as MVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp(f):\n",
    "    return np.loadtxt(open(f, \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "def do(f,n):\n",
    "    b = []\n",
    "    for i in range(n):\n",
    "        b.append(f())\n",
    "    return np.array(b)\n",
    "\n",
    "def inv(m):\n",
    "    return np.linalg.inv(m)\n",
    "\n",
    "def join(a,b):\n",
    "    return np.concatenate((a,b))\n",
    "\n",
    "def mp(f,a):\n",
    "    b = []\n",
    "    for i in range(np.shape(a)[0]):\n",
    "        b.append(f(a[i]))\n",
    "    return np.array(b)\n",
    "\n",
    "def irange(a,b):\n",
    "    return np.arange(a,b+1)\n",
    "\n",
    "def sm(m,rs,cs):\n",
    "    return m[np.ix_(rs,cs)]\n",
    "\n",
    "def means_g(As, Bs, bs):\n",
    "    return means[As] + sm(covs, As,Bs) @ inv(sm(covs, Bs,Bs)) @ (bs-means[Bs])\n",
    "\n",
    "def covs_g(As,Bs,bs):\n",
    "    sm(covs,As,As) - sm(covs,As,Bs) @ sm(covs,Bs,Bs) @ sm(covs,Bs,As)\n",
    "\n",
    "def sample(As,Bs,bs,n):\n",
    "    means = means_g(As,Bs,bs)\n",
    "    covs = covs_g(As,Bs,bs)\n",
    "    return do(lambda : sp.stats.multivariate_normal.rvs(means, covs),n)\n",
    "\n",
    "def ftime_v(f_v):\n",
    "    return f_v + len(Fsall)\n",
    "\n",
    "def rcovs(r):\n",
    "    es=np.random.uniform(0,1,r)\n",
    "    es2 = es/np.mean(es)\n",
    "    np.random.seed(514)\n",
    "    covs = sp.stats.random_correlation.rvs(es2)\n",
    "    return covs \n",
    "    \n",
    "def rmeans(r):\n",
    "    means = np.random.uniform(-1,1,r)\n",
    "    return means\n",
    "\n",
    "def exploit_time(Fs,fs):\n",
    "    return min(means_g(Rsall, Fs, fs))\n",
    "\n",
    "def feature_time(F,Fs,fs):\n",
    "    return means_g([F], Fs, fs)[0]\n",
    "\n",
    "def sample(As,Bs,bs,n):\n",
    "    means = means_g(As,Bs,bs)\n",
    "    covs = covs_g(As,Bs,bs)\n",
    "    return do(lambda : sp.stats.multivariate_normal.rvs(means, covs),n)\n",
    "\n",
    "def ftime_v(f_v):\n",
    "    return f_v + len(Fsall)\n",
    "\n",
    "#mutli-variate gaussian monte carlo n-greedy\n",
    "def time_left(Fs, fs, ns):\n",
    "    #x is the exploit time\n",
    "    choice = -1\n",
    "    x = exploit_time(Fs,fs)#expected runtimes given info\n",
    "    if ns==[]:\n",
    "        return x,choice #just run a solver\n",
    "    #for every feature you could calculate\n",
    "    for F_opt in np.setdiff1d(Fsall,Fs):\n",
    "        #compute the expected feature compute time\n",
    "        y = feature_time(ftime_v(F_opt), Fs,fs)#expected feature compute time given info\n",
    "        #sample some points\n",
    "        frs = sample([F_opt],Fs,fs,ns[0])#random outcomes for feature value, compute time\n",
    "        s = 0\n",
    "        #compute their time left\n",
    "        for fr in frs:\n",
    "            t,_ = time_left(join(Fs,[F_opt]),join(fs,[fr]),ns[1:]) #act like the random outcomes happened\n",
    "            s += t \n",
    "        #store the average of the points + feature time\n",
    "        m = s/len(frs) + y\n",
    "        if m < x:\n",
    "            choice = F_opt \n",
    "            x = m\n",
    "    #return the min of exploit time and feature times\n",
    "    return x, choice\n",
    "\n",
    "def select(f,a):\n",
    "    b = []\n",
    "    for i in range(np.shape(a)[0]):\n",
    "        if(f(a[i])):\n",
    "            b.append(a[i])\n",
    "    return np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt(a):\n",
    "    b = []\n",
    "    for v in a:\n",
    "        if v>0:\n",
    "            b.append(v)\n",
    "    if len(b)==0:\n",
    "        b.append(1200)\n",
    "    return np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mutli-variate gaussian monte carlo n-greedy\n",
    "def time_left_2(Ks, ks, ns, c_samp):\n",
    "    #try:\n",
    "        n_x = ns[0][0]\n",
    "        n_ft = ns[0][1]\n",
    "        n_branch = ns[0][2]\n",
    "\n",
    "        #sample n_x Rts from the posterior\n",
    "        #global Ws\n",
    "        #Ws=Ks\n",
    "        #print(Ks)\n",
    "        samp_x = c_samp(Rts,Ks,ks,n_x)\n",
    "        e_xs = np.mean(samp_x, axis=0)\n",
    "        best_i = np.argmin(e_xs)\n",
    "        best_t = e_xs[best_i]\n",
    "        best_type = \"exploit\"  \n",
    "        #print(best_t)\n",
    "\n",
    "        if n_branch==0:\n",
    "            return best_t,best_i,best_type #just run a solver\n",
    "        #for every feature you could calculate\n",
    "        for Ft_opt in np.sort(np.setdiff1d(Fts,Ks)):\n",
    "            Fv_opt = Ft_v[Ft_opt]\n",
    "            #sample feature times\n",
    "            samp_ft = c_samp([Ft_opt],Ks,ks,n_ft)\n",
    "            e_ft = np.mean(samp_ft)\n",
    "\n",
    "            #sample features and runtimes\n",
    "            samp_fvft = c_samp(join(Fv_opt,[Ft_opt]),Ks,ks,n_branch)\n",
    "\n",
    "            ts = []\n",
    "            for fvft in samp_fvft:\n",
    "                FVFT = join(Fv_opt,[Ft_opt])\n",
    "\n",
    "                best_t,_,_ = time_left_2(join(Ks,FVFT),join(ks,fvft),ns[1:],c_samp)\n",
    "                ts.append(best_t)\n",
    "            e_t = np.mean(ts)\n",
    "            \n",
    "            \n",
    "            t_t = e_ft + e_t\n",
    "            if t_t < best_t:\n",
    "                best_t = t_t\n",
    "                best_i = Ft_i[Ft_opt]\n",
    "                best_type = \"explore\" \n",
    "            #print(best_t)\n",
    "        #return the min of exploit time and feature times\n",
    "        return best_t,best_i,best_type\n",
    "    #except:\n",
    "        #print(\"error\")\n",
    "        #return 1201,0,\"exploit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = np.min(data_all,axis=0)\n",
    "maxs = np.max(data_all,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "        def c_samp(As,Bs,bs,n):\n",
    "            s1 = mixture_model.sample_from_conditional(model,Bs,bs,As,n)\n",
    "            s2 = np.exp(s1)\n",
    "            minsa = mins[As]\n",
    "            maxsa = maxs[As]\n",
    "            lb = 0.1\n",
    "            ub = 1\n",
    "            s3 = (s2-lb) * 1/(ub-lb) * (maxsa-minsa) + minsa\n",
    "            return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_all[0] < 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_nn = []\n",
    "i=0\n",
    "for r in data_all:\n",
    "    if (r[Rts]>0).all() and (r[Fts]>0).all():\n",
    "        data_all_nn.append(r)\n",
    "    else\n",
    "data_all_nn = np.array(data_all_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp(logify,data)(data_all_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(c_samp([10],np.array([],dtype=int),np.array([],dtype=int),10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201.0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data_all_nn[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MVG.MVGModel(data_all_nn)\n",
    "c_samp2 = lambda As,Bs,bs,n : a.sample_from_conditional(Bs,bs,As,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MVG.MVGModel(mp(logify,data_all_nn))\n",
    "        \n",
    "def c_samp(As,Bs,bs,n):\n",
    "    #print(As)\n",
    "    #print(Bs)\n",
    "    #print(bs)\n",
    "    bs2 = []\n",
    "    for i in range(len(bs)):\n",
    "        if Bs[i] in Fts or Bs[i] in Rts:\n",
    "            bs2.append(np.log(bs[i]))\n",
    "        else:\n",
    "            bs2.append(bs[i])\n",
    "    bs2 = np.array(bs2)\n",
    "\n",
    "    rsamp = a.sample_from_conditional(Bs,bs2,As,n)\n",
    "    igs = []\n",
    "    for i in range(len(As)):\n",
    "        if As[i] in Rts or As[i] in Fts:\n",
    "            igs.append(np.exp(rsamp[:,i]))\n",
    "        else:\n",
    "            igs.append(rsamp[:,i])\n",
    "    return np.transpose(igs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logify(r):\n",
    "    r2 = []\n",
    "    for i in range(len(r)):\n",
    "        if i in Rts or i in Fts:\n",
    "            r2.append(np.log(max(0.1,r[i])))\n",
    "        else:\n",
    "            r2.append(r[i])\n",
    "    return np.array(r2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_r = imp(r\"C:\\Users\\DFCTech\\Downloads\\SAT_data\\cs159\\SATRAND12S_data\\SATRAND12Sft_and_slv_times_with_vals.csv\")[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290.38132986901076"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data_all_r[Rts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses m, ns, data\n",
    "def get_model(data,ns,m):\n",
    "    if m == \"MVG\":\n",
    "        a = MVG.MVGModel(data)\n",
    "        def c_samp(As,Bs,bs,n):\n",
    "            global s1\n",
    "            s1 = a.sample_from_conditional(Bs,bs,As,n)\n",
    "            \n",
    "            global s2\n",
    "            s2 = []\n",
    "            for i in range(len(As)):\n",
    "                if As[i] in Rts or As[i] in Fts:\n",
    "                    s2.append(mp(lambda v: max(v,0), s1[:,i]))\n",
    "                else:\n",
    "                    s2.append(s1[:,i])\n",
    "            #return s1\n",
    "            return np.transpose(s2)\n",
    "        \n",
    "    if m ==\"logMVG\":\n",
    "        a = MVG.MVGModel(mp(logify,data))\n",
    "        \n",
    "        def c_samp(As,Bs,bs,n):\n",
    "            #print(As)\n",
    "            #print(Bs)\n",
    "            #print(bs)\n",
    "            bs2 = []\n",
    "            for i in range(len(bs)):\n",
    "                if Bs[i] in Fts or Bs[i] in Rts:\n",
    "                    bs2.append(np.log(bs[i]))\n",
    "                else:\n",
    "                    bs2.append(bs[i])\n",
    "            bs2 = np.array(bs2)\n",
    "            \n",
    "            rsamp = a.sample_from_conditional(Bs,bs2,As,n)\n",
    "            igs = []\n",
    "            for i in range(len(As)):\n",
    "                if As[i] in Rts or As[i] in Fts:\n",
    "                    igs.append(np.exp(rsamp[:,i]))\n",
    "                else:\n",
    "                    igs.append(rsamp[:,i])\n",
    "            return np.transpose(igs)\n",
    "        \n",
    "    if m == \"MMVG\":\n",
    "        import mixture_model\n",
    "        mins = mp(min,data_all)\n",
    "        \n",
    "        model = pickle.load(open(\"./GaussianMixtureSATALL.pickle\", \"rb\"))\n",
    "        \n",
    "        def c_samp(As,Bs,bs,n):\n",
    "            s1 = mixture_model.sample_from_conditional(model,Bs,bs,As,n)\n",
    "            s2 = np.exp(s1)\n",
    "            minsa = mins[As]\n",
    "            maxsa = maxs[As]\n",
    "            lb = 0.1\n",
    "            ub = 1\n",
    "            s3 = (s2-lb) * 1/(ub-lb) * (maxsa-minsa) + minsa\n",
    "            return s3\n",
    "            \n",
    "        \n",
    "    def f(Ks,ks):\n",
    "        best_t,best_i,best_type = time_left_2(Ks,ks,ns,c_samp)\n",
    "        return best_type, best_i \n",
    "    return f\n",
    "\n",
    "def run_model(truth, model,verbose):\n",
    "    try:\n",
    "        global i\n",
    "        print(\"running\")\n",
    "        moves = []\n",
    "        Bs = np.array([],dtype=int)\n",
    "        bs = []\n",
    "        timesofar = 0\n",
    "        while True:\n",
    "            typ, i = model(Bs,bs)\n",
    "            moves.append(i)\n",
    "            if typ == \"explore\":\n",
    "                Fv = Fvs[i]\n",
    "                Ft = Fts[i]\n",
    "                fv = truth[Fv]\n",
    "                ft = truth[Ft]\n",
    "\n",
    "                timesofar += ft\n",
    "                Bs = np.concatenate((Bs,Fv,[Ft]))\n",
    "                bs = np.concatenate((bs,fv,[ft]))\n",
    "            if typ == \"exploit\":\n",
    "                timesofar += truth[Rts[i]]\n",
    "                if verbose:\n",
    "                    return timesofar, moves\n",
    "                else:\n",
    "                    return timesofar\n",
    "    except:\n",
    "        return \"error\"\n",
    "data_all = imp(r\"C:\\Users\\DFCTech\\Downloads\\SAT_data\\cs159\\SATALL12S_data\\SATALL12Sft_and_slv_times_with_vals.csv\")[:,1:]\n",
    "data_all = np.where(data_all==1201, 1201, data_all)\n",
    "#np.random.shuffle(data_all)\n",
    "\n",
    "Rts = irange(0,30)\n",
    "Fts = [37,52,73,84,90,109,128,140,152,155]\n",
    "Fts2 = np.concatenate(([30],Fts))\n",
    "Fvs = []\n",
    "for i in range(len(Fts2)-1):\n",
    "    Fvs.append(irange(Fts2[i]+1,Fts2[i+1]-1))\n",
    "Ft_v = {}\n",
    "Ft_i = {}\n",
    "for i,t,v in zip(range(len(Fts)),Fts,Fvs):\n",
    "    Ft_v[t] = v\n",
    "    Ft_i[t] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(model, test_data,verbose):\n",
    "    global i\n",
    "    i = 0\n",
    "    return mp(lambda truth : run_model(truth, model,verbose), test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(len(Rts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys1 = mp(lambda i : np.mean(mp(lambda r : np.sort(r)[i],data_all[:,Rts])), range(len(Rts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys2 = mp(lambda i : np.sort(np.mean(data_all_r[:,Rts],axis=0))[i],range(len(Rts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "916.2414921131165"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data_all_r[:,Rts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 369.20271953,  429.18725698,  449.2851138 ,  461.01751468,\n",
       "        486.86449046,  561.6785837 ,  593.80759178,  602.5405793 ,\n",
       "        606.13716373,  669.35701982,  886.41470631, 1042.16266226,\n",
       "       1042.64301689, 1060.28530543, 1064.45095154, 1078.78624963,\n",
       "       1087.59113877, 1089.87387592, 1106.31457636, 1120.07192217,\n",
       "       1125.99655213, 1127.37114097, 1129.47891336, 1134.92900661,\n",
       "       1136.09996035, 1138.20546623, 1148.04514905, 1152.78048605,\n",
       "       1156.46502056, 1161.44941777, 1184.99270338])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs,ys1,label=\"n-th best per instance\")\n",
    "plt.plot(xs,ys2,label=\"n-th best overall\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"mean time\")\n",
    "plt.legend()\n",
    "plt.savefig(\"comparison.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ys1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DFCTech\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.mixture.gaussian_mixture module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.mixture. Anything that cannot be imported from sklearn.mixture is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\DFCTech\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator GaussianMixture from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import mixture_model\n",
    "import pickle\n",
    "model = pickle.load(open(\"./GaussianMixtureSATALL.pickle\", \"rb\"))\n",
    "c_samp = lambda As,Bs,bs,n : mixture_model.sample_from_conditional(model,Bs,bs,As,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samp(np.array([],dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_all[:99]\n",
    "train_data = data_all[100:]\n",
    "clf_data = data_all[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns4=[[100,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=results(mMMVG,test_data[1:10],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns4=[[200,0,0]]\n",
    "mMVG = get_model(train_data, ns4,\"MVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(1223.13, [9, 3, 0, 16]), (7.1000000000000005, [7, 3, 9, 5, 18]),\n",
       "       (5.069999999999999, [8, 5, 2, 16]), ..., (9.92, [8, 9, 18]),\n",
       "       (1205.01, [9, 5, 0, 28]), (147.54, [0, 4, 3, 3])], dtype=object)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432.996483508619\n",
      "467.3396704766334\n",
      "437.5962076746909\n",
      "408.2585528221926\n",
      "389.62616819599634\n",
      "431.9244821093281\n",
      "396.3012653785208\n",
      "411.08535730393777\n",
      "395.97164915626473\n",
      "457.09641439874673\n",
      "403.95892207421673\n",
      "453.8448562050225\n",
      "439.35168108273444\n",
      "431.68399664166844\n",
      "440.74521006208886\n",
      "353.97385973010637\n",
      "437.01771717075\n",
      "438.67994501359647\n",
      "427.79138365444726\n",
      "487.95540686978325\n",
      "409.7549506089462\n",
      "418.05476406010666\n",
      "366.0979741068523\n",
      "372.01179372010904\n",
      "408.4226167277147\n",
      "458.5727439651589\n",
      "372.87054012486\n",
      "432.2729266013287\n",
      "377.1218478026354\n",
      "409.6930053124264\n",
      "463.2649224274667\n",
      "416.64622731767815\n",
      "442.88107075513994\n",
      "471.11769147259577\n",
      "429.21825489617424\n",
      "411.4954413648557\n",
      "467.3650595907966\n",
      "385.5626055202683\n",
      "407.00057542171356\n",
      "374.9143609999797\n",
      "448.7541067567072\n",
      "388.0556224764165\n",
      "403.26065370844606\n",
      "409.2209548969065\n",
      "463.7370518042821\n",
      "447.5734984466845\n",
      "415.6334965778637\n",
      "332.1925149003714\n",
      "432.96018473737274\n",
      "382.02409304911737\n",
      "399.3126279843143\n",
      "424.1409166630085\n",
      "420.78195717415673\n",
      "422.86258813198833\n",
      "416.3204139680269\n",
      "402.1470153727922\n",
      "456.6975764356594\n",
      "417.58527106131135\n",
      "458.0316895964749\n",
      "424.9069724285889\n",
      "423.99319900040615\n",
      "405.03606137119\n",
      "309.88820236250956\n",
      "407.8916507989762\n",
      "395.6657894742706\n",
      "456.4470371013722\n",
      "480.9698180702892\n",
      "390.47949947907915\n",
      "434.8241919574469\n",
      "428.64765962304637\n",
      "438.69709258423904\n",
      "373.45696311999853\n",
      "420.95805192645724\n",
      "375.06634942245887\n",
      "448.8716134202986\n",
      "444.53987631494476\n",
      "355.44828971592034\n",
      "474.13573259140196\n",
      "424.6123995136994\n",
      "370.4945089751434\n",
      "356.2864382399493\n",
      "461.2505721746395\n",
      "379.33143084605763\n",
      "450.2845151754042\n",
      "402.70987176716767\n",
      "442.05689575000054\n",
      "371.17991669713024\n",
      "416.17246975112374\n",
      "471.9360054609468\n",
      "312.04386354688177\n",
      "438.4612178079918\n",
      "426.8638742260639\n",
      "405.61788162998243\n",
      "459.7640888005124\n",
      "419.22177039561296\n",
      "451.792330123001\n",
      "410.1606891411077\n",
      "412.8990115072546\n",
      "477.7110777886954\n",
      "409.1710914476953\n",
      "397.2818954628363\n",
      "372.6668820775614\n",
      "431.63867976079155\n",
      "455.15407535642976\n",
      "428.11318522372244\n",
      "374.8777934553477\n",
      "403.1025103321411\n",
      "392.3328735226502\n",
      "435.82762157695913\n",
      "462.0051453128341\n",
      "418.96688065507084\n",
      "402.8435559642418\n",
      "396.0212151777394\n",
      "382.3555565900206\n",
      "415.4872547248952\n",
      "404.17195907221713\n",
      "333.32749848824847\n",
      "427.0512423406862\n",
      "495.74744773369173\n",
      "377.15171256579765\n",
      "437.52254608827695\n",
      "434.76739286783925\n",
      "384.66191941542536\n",
      "443.2686156107354\n",
      "408.6353999273183\n",
      "408.5318784219485\n",
      "420.6785748140053\n",
      "392.31809288154955\n",
      "466.8295779283588\n",
      "386.22108000138144\n",
      "384.2980252420804\n",
      "397.5196633095291\n",
      "399.71032990405365\n",
      "462.18728634520346\n",
      "430.7146563284687\n",
      "435.03405564073864\n",
      "326.3063290635974\n",
      "374.3043879687216\n",
      "441.8193025822844\n",
      "382.3869145331024\n",
      "452.3331367583641\n",
      "407.87693536987064\n",
      "465.3357734393807\n",
      "441.6814139826126\n",
      "398.19870314961685\n",
      "447.686757350796\n",
      "409.14386480303176\n",
      "423.56293000150805\n",
      "366.379234523935\n",
      "465.00669959915876\n",
      "427.2013747988336\n",
      "391.45398934606857\n",
      "450.38538671476766\n",
      "366.9465506201266\n",
      "450.611599379297\n",
      "322.7355294823453\n",
      "344.10771931542723\n",
      "413.1351856896194\n",
      "484.2163961832149\n",
      "409.17991799492324\n",
      "440.2471190160801\n",
      "411.13802208791805\n",
      "457.5330964883465\n",
      "426.0991850259708\n",
      "363.2559799962011\n",
      "430.2188982463231\n",
      "439.9719309605159\n",
      "455.87995032579494\n",
      "389.83935291347416\n",
      "347.69580988450053\n",
      "460.2570663709846\n",
      "420.93118881719425\n",
      "410.8968173893282\n",
      "413.20462115196625\n",
      "366.5506524792024\n",
      "433.43060157676734\n",
      "468.90024255407275\n",
      "376.0722015783431\n",
      "388.4595974185428\n",
      "442.59949941179497\n",
      "401.2594079946952\n",
      "397.156787710716\n",
      "444.34962243434404\n",
      "368.2758508048869\n",
      "473.44500334710034\n",
      "434.39329540210724\n",
      "429.1556792263598\n",
      "403.37996346828686\n",
      "339.9087513990097\n",
      "396.9028217675316\n",
      "362.6657960661186\n",
      "442.0953090153076\n",
      "449.7878956340479\n",
      "405.73724718220313\n",
      "423.2536234684732\n",
      "494.1247674726638\n",
      "450.7322357234593\n",
      "372.96494870754503\n",
      "354.5527040961969\n",
      "439.55184432588226\n",
      "353.2275950612022\n",
      "460.8116936492063\n",
      "443.44347934089143\n",
      "399.58589733919433\n",
      "440.2107863541997\n",
      "481.531550419258\n",
      "382.9953501718002\n",
      "383.9270834184172\n",
      "435.24405181094494\n",
      "431.368528243979\n",
      "375.3255891183261\n",
      "427.8318182334192\n",
      "358.3983037056808\n",
      "395.4845139517591\n",
      "388.1722585552705\n",
      "436.4618496833084\n",
      "483.71180817701185\n",
      "415.63757063682016\n",
      "359.19170691037726\n",
      "478.3244840110272\n",
      "434.91754553733307\n",
      "479.55610717090474\n",
      "486.3762637901782\n",
      "403.3628325905047\n",
      "448.4640483741104\n",
      "423.27195111185966\n",
      "427.08413570075254\n",
      "390.82587579033816\n",
      "458.61260586395\n",
      "459.0533716493882\n",
      "382.1064846691464\n",
      "472.98021172969885\n",
      "432.7160743136039\n",
      "388.09506806289386\n",
      "456.78175579463584\n",
      "402.5300061359614\n",
      "364.2529962942183\n",
      "398.02482749747884\n",
      "488.06921375398423\n",
      "406.9629070748384\n",
      "443.53101231408976\n",
      "390.35861492240554\n",
      "373.8418848706562\n",
      "399.31897544031983\n",
      "460.85184398965475\n",
      "431.94014101052204\n",
      "417.7504383757728\n",
      "391.78542447833854\n",
      "355.7495590877272\n",
      "390.4033035971451\n",
      "511.3078340072784\n",
      "384.51195889791893\n",
      "416.3587647319167\n",
      "383.9523543334864\n",
      "360.6817662429574\n",
      "425.2586764594109\n",
      "457.7210302780713\n",
      "443.50279508207996\n",
      "428.8593491088248\n",
      "487.8828240404073\n",
      "414.54973422666103\n",
      "437.91010361823675\n",
      "357.67618348130566\n",
      "323.80163030721604\n",
      "376.95146360584636\n",
      "395.3449203104756\n",
      "403.9163247649571\n",
      "421.50656866109335\n",
      "401.90422966946625\n",
      "422.1010141572036\n",
      "470.21433674336834\n",
      "463.3771057832145\n",
      "428.8994004151206\n",
      "419.30035088859114\n",
      "374.11502490065425\n",
      "393.7847851597461\n",
      "346.15945320883657\n",
      "387.2040275022453\n",
      "466.8431006221282\n",
      "380.0222452684512\n",
      "396.6520345247505\n",
      "388.09947474868574\n",
      "448.5062376372001\n",
      "430.5885040220541\n",
      "422.1954926015908\n",
      "427.37876939827305\n",
      "439.6609135289845\n",
      "343.75307357994353\n",
      "415.2227074439943\n",
      "466.180418568992\n",
      "409.0303092747879\n",
      "385.1186273141712\n",
      "365.38808646853795\n",
      "444.7200513420212\n",
      "431.1251919800444\n",
      "399.35863113559884\n",
      "396.5303155392752\n",
      "418.68579395575927\n",
      "378.2392213964064\n",
      "407.8436531267637\n",
      "460.3509389293961\n",
      "403.18043064480736\n",
      "429.3484915052219\n",
      "416.18280576679433\n",
      "420.067299615874\n",
      "404.9178154796423\n",
      "353.84773971723206\n",
      "358.7728773302261\n",
      "433.8458072570171\n",
      "385.8293949765221\n",
      "327.5650927597889\n",
      "402.0216054872438\n",
      "449.7260389017145\n",
      "401.3899639106006\n",
      "392.5767680397745\n",
      "412.0724871048862\n",
      "406.65154978362597\n",
      "435.40219905870157\n",
      "482.63065834369394\n",
      "453.39153623306964\n",
      "410.77366267477294\n",
      "445.197605280688\n",
      "478.05102406988806\n",
      "452.67359816650094\n",
      "464.5276603282176\n",
      "356.6672547185353\n",
      "387.8045321497059\n",
      "443.75695266573587\n",
      "419.502885884008\n",
      "426.1114205815704\n",
      "371.53713414218527\n",
      "408.3096059062845\n",
      "401.77696041447564\n",
      "400.8996920751487\n",
      "440.4925000624283\n",
      "459.09830193842487\n",
      "435.05436459259204\n",
      "467.9509426096709\n",
      "487.57598577921215\n",
      "447.660324645648\n",
      "466.47038322038765\n",
      "417.9232174083817\n",
      "376.6186297709195\n",
      "412.4497505869875\n",
      "434.1925367761786\n",
      "428.07367322745745\n",
      "383.59635232563136\n",
      "466.56525034509644\n",
      "464.58901346529166\n",
      "385.3121697928394\n",
      "428.97664776608656\n",
      "434.7852611580241\n",
      "429.2501655239183\n",
      "469.8104620059917\n",
      "411.2015074855717\n",
      "442.63591488183897\n",
      "414.98475836070384\n",
      "407.16353589200776\n",
      "473.69749876178616\n",
      "424.6577189957858\n",
      "362.73737944064106\n",
      "426.59512165737885\n",
      "442.3542829405235\n",
      "421.77627999699956\n",
      "393.89429604573087\n",
      "414.02087124566026\n",
      "451.0765674912233\n",
      "367.0369586651781\n",
      "373.6149098800961\n",
      "417.8134519738068\n",
      "435.4252929618773\n",
      "390.774676697016\n",
      "422.23852135956986\n",
      "427.62782160661794\n",
      "426.20216185087077\n",
      "439.54190688221223\n",
      "450.20684236217255\n",
      "463.17829390929916\n",
      "392.77933606931254\n",
      "381.21163683372345\n",
      "357.19458931988936\n",
      "450.42444145574467\n",
      "380.3173292012971\n",
      "452.62174495294596\n",
      "448.06764823286017\n",
      "417.91236556288464\n",
      "436.90292893175166\n",
      "343.49136034720755\n",
      "407.49328639610025\n",
      "357.45509458865337\n",
      "445.53613713531144\n",
      "381.0658566166608\n",
      "397.69305119338856\n",
      "471.93058785017945\n",
      "457.7164384937212\n",
      "405.1526778843611\n",
      "414.68187001644253\n",
      "466.36823615766093\n",
      "385.1343128786999\n",
      "423.73257834830326\n",
      "345.91579907881345\n",
      "487.0059869238251\n",
      "456.0090585925723\n",
      "437.7517339471477\n",
      "385.96645740776546\n",
      "409.28585032221486\n",
      "410.5550164566241\n",
      "452.59051516414684\n",
      "435.8588725173697\n",
      "419.94007724597145\n",
      "385.6329519911298\n",
      "411.5872028231777\n",
      "485.52654754265234\n",
      "432.2438942260097\n",
      "380.55206996450886\n",
      "380.10670470025815\n",
      "422.82027270109137\n",
      "435.00530735109686\n",
      "437.6303046295984\n",
      "399.07130250832995\n",
      "418.04967840948007\n",
      "470.98775680137044\n",
      "394.0986711499182\n",
      "445.8072346917891\n",
      "429.37003168584096\n",
      "424.71777176016093\n",
      "415.4819177851669\n",
      "472.6654715495449\n",
      "382.68681623444724\n",
      "396.9056236466993\n",
      "397.8698438147484\n",
      "439.74612168060673\n",
      "381.70112533777996\n",
      "410.0621671536963\n",
      "426.16231932157143\n",
      "407.4533999316146\n",
      "427.05472615851414\n",
      "439.7791408469538\n",
      "469.6339937635181\n",
      "366.36342028134067\n",
      "368.0125293666834\n",
      "421.5853434981506\n",
      "453.75377275193955\n",
      "392.7515470259139\n",
      "405.15868360868905\n",
      "486.91785904086566\n",
      "327.0281930117868\n",
      "348.89661350461205\n",
      "424.0760847289613\n",
      "431.59367303481747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384.09250340205983\n",
      "443.4154043302666\n",
      "383.3404400056704\n",
      "449.7743871908477\n",
      "405.3752649194736\n",
      "365.4054797607251\n",
      "408.58337102864965\n",
      "469.5255129907432\n",
      "426.03994031380074\n",
      "391.178408726513\n",
      "442.04335413828625\n",
      "385.8737072872567\n",
      "443.791070556695\n",
      "434.1228097137362\n",
      "406.16651564920886\n",
      "468.2746040164706\n",
      "418.20242197261643\n",
      "433.1310147992296\n",
      "397.90877250247905\n",
      "384.68807720365015\n",
      "404.68056724762994\n",
      "383.8684297872129\n",
      "434.3561435836337\n",
      "442.80095202028696\n",
      "463.56694167147464\n",
      "393.9710698165386\n",
      "378.1684609940481\n",
      "390.3690141718359\n",
      "371.3822379382592\n",
      "363.9333523151613\n",
      "415.89989496697757\n",
      "449.3374842729507\n",
      "394.1043123006255\n",
      "435.8328518726409\n",
      "410.1055454300082\n",
      "463.30667514687707\n",
      "439.1304399907886\n",
      "437.1696019422211\n",
      "455.92754896594255\n",
      "359.7144366506967\n",
      "383.1300140666639\n",
      "437.11935113792134\n",
      "386.7242795951955\n",
      "365.2695887421835\n",
      "387.71010027147963\n",
      "413.0315017928704\n",
      "399.6421419890521\n",
      "483.69518852623764\n",
      "419.24867151841596\n",
      "373.9393389227548\n",
      "417.991206868124\n",
      "423.78214881618374\n",
      "433.2875673370449\n",
      "424.0454638999669\n",
      "413.8402996612598\n",
      "395.50497301186465\n",
      "433.2786796759759\n",
      "489.1862155802873\n",
      "445.46729619782184\n",
      "378.9420829744655\n",
      "349.3376248878545\n",
      "410.28153597738344\n",
      "468.8308497018578\n",
      "449.0477779487727\n",
      "350.7649978085811\n",
      "351.99911362037705\n",
      "459.53657877847286\n",
      "438.01612538418317\n",
      "377.14102439013527\n",
      "442.10286657099755\n",
      "402.5287305102013\n",
      "402.07733389199683\n",
      "393.9192784174578\n",
      "397.32211962890455\n",
      "427.66639625094496\n",
      "402.09544569971894\n",
      "419.0888049448069\n",
      "385.25453362409155\n",
      "367.2365613054787\n",
      "417.83577114463054\n",
      "415.99992124553273\n",
      "428.22580202401747\n",
      "334.40293026732064\n",
      "384.1299911523089\n",
      "408.1255422417271\n",
      "438.5469500477916\n",
      "449.6636560367799\n",
      "430.34191024588085\n",
      "475.9644721931124\n",
      "386.27196739081234\n",
      "446.7264722791982\n",
      "413.7441920901453\n",
      "413.11037243180857\n",
      "443.46587425129314\n",
      "424.10185436894784\n",
      "428.8209298898806\n",
      "409.40299608147194\n",
      "426.1448421329608\n",
      "365.296655192341\n",
      "390.8494348907685\n",
      "418.1363093888098\n",
      "416.8834253804173\n",
      "398.66898012618896\n",
      "439.62707643545923\n",
      "411.4544714061977\n",
      "405.77369781274086\n",
      "440.8172055904519\n",
      "423.3303129670782\n",
      "408.1136354333388\n",
      "385.74314824700616\n",
      "382.33880627282883\n",
      "332.979877320112\n",
      "422.5452656400274\n",
      "355.86991363746347\n",
      "393.17275393058827\n",
      "442.3886042135384\n",
      "401.998976255652\n",
      "464.2763806290298\n",
      "379.3088893061808\n",
      "442.85084959986926\n",
      "490.7989273594462\n",
      "401.1804933763531\n",
      "380.68477267409935\n",
      "398.37784942276573\n",
      "375.45825611726553\n",
      "410.89578380477406\n",
      "493.40910576321266\n",
      "469.0553595516322\n",
      "385.4706461648033\n",
      "423.14116562879497\n",
      "403.809706487113\n",
      "390.57465577084423\n",
      "461.3383594147576\n",
      "386.18402822548626\n",
      "473.8316291739881\n",
      "381.1333909983924\n",
      "447.5449219814781\n",
      "368.45309281474675\n",
      "367.354400296813\n",
      "396.5312184463432\n",
      "426.15768769445674\n",
      "438.42646016889455\n",
      "444.8767975423681\n",
      "384.0201746187447\n",
      "348.26090749339664\n",
      "421.6470243133665\n",
      "442.5578252045166\n",
      "430.0598545432944\n",
      "397.1031187525095\n",
      "343.6273075647598\n",
      "390.9819105504195\n",
      "411.585813932576\n",
      "368.37196334306617\n",
      "465.31645880154827\n",
      "440.0844615407426\n",
      "429.65612220659455\n",
      "368.9845757153076\n",
      "410.22103991445806\n",
      "392.70286312138495\n",
      "426.13027668549506\n",
      "419.3713238817014\n",
      "359.1792672435373\n",
      "396.7694422843776\n",
      "415.7395210847852\n",
      "374.07619200453644\n",
      "421.6087197910181\n",
      "477.31066979488224\n",
      "459.13891594276305\n",
      "386.1903193719199\n",
      "418.1765010893738\n",
      "367.6476466611802\n",
      "420.64443484249523\n",
      "417.3905380708675\n",
      "391.98830538963955\n",
      "455.3346603304379\n",
      "354.9446125493843\n",
      "450.09492460330466\n",
      "425.66412202737905\n",
      "372.5949500835948\n",
      "468.16246338667247\n",
      "395.88930055588963\n",
      "335.97074746617017\n",
      "456.1992840131154\n",
      "462.38281561923793\n",
      "436.3349300350924\n",
      "431.8606740790415\n",
      "480.0686155706184\n",
      "396.70769364535386\n",
      "407.85687832499883\n",
      "374.14103489765535\n",
      "438.3980664219452\n",
      "420.28163441916956\n",
      "387.91469819800045\n",
      "411.1152091419732\n",
      "389.2795725000308\n",
      "424.63600458793303\n",
      "373.5444674187372\n",
      "397.54850952894907\n",
      "422.4973888176066\n",
      "405.9774809896073\n",
      "423.67183889582617\n",
      "371.37837319858625\n",
      "372.77416108121884\n",
      "417.5374314030757\n",
      "357.25675312270306\n",
      "413.71172006604525\n",
      "420.9365347604237\n",
      "425.37866858957915\n",
      "401.5453301763721\n",
      "418.6719746033167\n",
      "421.629761006623\n",
      "410.89212608648273\n",
      "458.59969201561756\n",
      "399.6973131692028\n",
      "394.99484897125103\n",
      "453.3593718639346\n",
      "401.3661920712209\n",
      "452.47960512567397\n",
      "350.0728061098057\n",
      "369.4807286646381\n",
      "398.17163359782177\n",
      "363.08351074459364\n",
      "441.51355675069476\n",
      "438.5657093407751\n",
      "395.878161836971\n",
      "415.32383014954996\n",
      "393.87314574373374\n",
      "443.258554492941\n",
      "385.246615533164\n",
      "464.11907502627184\n",
      "404.2797032580935\n",
      "448.0784223518164\n",
      "365.42539316651016\n",
      "443.7270156720718\n",
      "482.9387037028288\n",
      "403.43676751590834\n",
      "426.1107897767551\n",
      "379.29556908927816\n",
      "392.192127211287\n",
      "453.7669103528923\n",
      "384.55614826331873\n",
      "480.5913169414276\n",
      "392.16401267764917\n",
      "410.0725500189489\n",
      "451.1549922292048\n",
      "336.3171892330181\n",
      "428.0206662243475\n",
      "367.9887344319579\n",
      "388.21290863826783\n",
      "419.4241205147596\n",
      "456.96953844126597\n",
      "403.6958097135792\n",
      "498.9163664586758\n",
      "425.64133158984737\n",
      "435.9760888768869\n",
      "423.30399287140835\n",
      "392.7945762352702\n",
      "374.21891998208065\n",
      "445.07050788592534\n",
      "354.9552437159345\n",
      "422.7699467439283\n",
      "437.77713650661434\n",
      "407.6323416546578\n",
      "410.5894769259599\n",
      "422.5474738219951\n",
      "440.173762379801\n",
      "474.9432698003351\n",
      "392.70030758460547\n",
      "439.9580163964544\n",
      "411.69977691872407\n",
      "414.1826277687858\n",
      "376.6094983080716\n",
      "386.00399149893246\n",
      "372.50571676947897\n",
      "376.7900502527921\n",
      "370.21734289626744\n",
      "384.33948589885307\n",
      "397.8234390558698\n",
      "438.2743267342987\n",
      "391.5544349340977\n",
      "469.29919574164023\n",
      "489.96508551531065\n",
      "474.8511928481475\n",
      "358.38471031190437\n",
      "482.41499139541395\n",
      "441.87593900246907\n",
      "419.4044458501744\n",
      "425.0667696743791\n",
      "369.0042696654268\n",
      "448.9469729502104\n",
      "446.24182548182836\n",
      "447.70404045971196\n",
      "467.0892374338815\n",
      "363.761527351593\n",
      "378.27218362130486\n",
      "419.0329033588366\n",
      "421.09289492605296\n",
      "415.00006827297676\n",
      "401.0249438724054\n",
      "373.87958520116894\n",
      "398.2290624058271\n",
      "385.1020633479682\n",
      "463.2154334843056\n",
      "457.9103248196181\n",
      "359.9698564976592\n",
      "434.80206764675\n",
      "402.3825569659103\n",
      "364.0626754905807\n",
      "376.8894244647815\n",
      "356.60401346011713\n",
      "443.35104603807895\n",
      "450.99119306663647\n",
      "429.26671894466676\n",
      "422.0294704825339\n",
      "467.0674189360996\n",
      "418.2532990991229\n",
      "371.9757046649582\n",
      "440.9151008839882\n",
      "425.24367624369677\n",
      "407.2874059817583\n",
      "422.4586817668612\n",
      "373.1601141761315\n",
      "424.5992517347784\n",
      "401.4120718529422\n",
      "386.9052608571877\n",
      "447.11377681526164\n",
      "414.73341559249457\n",
      "447.8891403065205\n",
      "474.31399807097915\n",
      "402.41990333255666\n",
      "477.75382228364106\n",
      "399.0471526549879\n",
      "434.74249371452765\n",
      "441.68884183303976\n",
      "455.4221005436151\n",
      "401.4865534875085\n",
      "456.73920007374136\n",
      "386.48338099397256\n",
      "415.0306817203911\n",
      "376.4852704829002\n",
      "446.1938595692263\n",
      "405.1047754479929\n",
      "433.58015819114183\n",
      "384.2269690242273\n",
      "431.6732090349207\n",
      "433.7624016146434\n",
      "436.10913442677355\n",
      "461.4539790333477\n",
      "384.2433437545221\n",
      "436.7575285353801\n",
      "394.8703798486559\n",
      "463.7032792050374\n",
      "395.8782136992011\n",
      "348.01547339420375\n",
      "460.07048243761574\n",
      "444.9533284519956\n",
      "425.20116397997367\n",
      "400.5459977383907\n",
      "422.4491347675644\n",
      "391.35371987583335\n",
      "427.73028250417934\n",
      "348.7708916071125\n",
      "413.7314391016721\n",
      "447.76045633411815\n",
      "430.4627666716017\n",
      "453.83080111348715\n",
      "429.3696308907119\n",
      "362.1177521401559\n",
      "374.9010926550685\n",
      "432.577153664424\n",
      "434.337938364724\n",
      "454.04417113129347\n",
      "428.6970513773937\n",
      "415.75860201527206\n",
      "408.74620751104436\n",
      "444.1063789092883\n",
      "391.9554129972154\n",
      "421.35944642089174\n",
      "421.23000315949304\n",
      "403.0966021731227\n",
      "394.8928808610334\n",
      "426.314387277178\n",
      "446.3902574015636\n",
      "350.7504787989923\n",
      "432.8719628261271\n",
      "379.1222740786891\n",
      "400.4136317877377\n",
      "386.5407977411913\n",
      "390.8076114808272\n",
      "374.54631307413445\n",
      "455.03085434723033\n",
      "456.3390747322577\n",
      "383.49677984435556\n",
      "356.32973190472535\n",
      "418.36527482942444\n",
      "441.81504808108724\n",
      "402.0914090723418\n",
      "445.321376394793\n",
      "375.11721247530187\n",
      "455.10763067425216\n",
      "376.80863310300816\n",
      "396.1091248336971\n",
      "402.48699046055566\n",
      "402.4156711671001\n",
      "347.80528687685495\n",
      "420.3830769975076\n",
      "444.9937334506452\n",
      "360.07777760600527\n",
      "409.9340680772723\n",
      "446.40183744207593\n",
      "433.57310818196135\n",
      "448.43706053055234\n",
      "456.5718214656184\n",
      "383.0773485991385\n",
      "439.334140962193\n",
      "393.8802871588824\n",
      "387.477564076367\n",
      "405.5582828994317\n",
      "413.0323666017112\n",
      "417.7224519634517\n",
      "384.8141928427208\n",
      "417.3365736080676\n",
      "323.31508131862296\n",
      "450.04989707219283\n",
      "452.0174406084495\n",
      "416.55927795329524\n",
      "377.02508795989144\n",
      "415.49328938645954\n",
      "402.53918093652754\n",
      "385.4897559842695\n",
      "414.5454979507001\n",
      "378.8071015477328\n",
      "406.7960392996272\n",
      "459.422095561569\n",
      "369.4447925583121\n",
      "437.02979938753356\n",
      "434.3291056716656\n",
      "432.03710049150004\n",
      "412.51854445219897\n",
      "401.3736520765771\n",
      "412.56766781796745\n",
      "464.3007617057612\n",
      "492.4568037107229\n",
      "474.1027469333205\n",
      "399.6556729135648\n",
      "421.1339101471825\n",
      "441.9857881640262\n",
      "428.73862808896104\n",
      "399.8189437142803\n",
      "349.1618019269638\n",
      "380.72095308349947\n",
      "416.8434385564903\n",
      "361.3710825872648\n",
      "458.0365627381127\n",
      "475.8160858743678\n",
      "390.7517111112398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385.84362165555933\n",
      "380.5199908058528\n",
      "454.6210190648478\n",
      "460.7036218216404\n",
      "407.6021078041186\n",
      "385.44533056233047\n",
      "389.3816781818964\n",
      "340.25696911108736\n",
      "395.35300248460675\n",
      "466.14652720882225\n",
      "400.1035468576363\n",
      "358.1978381425881\n",
      "389.02902530109964\n",
      "401.7839125056627\n",
      "464.29037509055314\n",
      "386.5186626519391\n",
      "380.36241780330755\n",
      "416.3516742847007\n",
      "396.0822734029361\n",
      "410.0311029090224\n",
      "396.8558379146156\n",
      "451.0575881571477\n",
      "396.76649848313406\n",
      "458.69767938547767\n",
      "414.1159844312318\n",
      "422.23898255707206\n",
      "386.2473537968204\n",
      "326.63505258720613\n",
      "369.5591307455016\n",
      "442.75919983557475\n",
      "457.5745236826587\n",
      "392.8990485147891\n",
      "442.81198006757893\n",
      "453.2358480404527\n",
      "436.8672471178502\n",
      "436.8997814029696\n",
      "446.11801143703946\n",
      "444.063341674535\n",
      "389.6915327898565\n",
      "424.5882868220852\n",
      "386.76193536029064\n",
      "402.67816448499246\n",
      "403.1051683315837\n",
      "402.40959155676404\n",
      "377.2082533301423\n",
      "434.8792970817421\n",
      "384.53451852026177\n",
      "430.59060824681114\n",
      "433.634453999839\n",
      "460.2422173483816\n",
      "406.2589391730768\n",
      "425.17150656871075\n",
      "412.657271848429\n",
      "340.22055703199635\n",
      "421.47894350245724\n",
      "426.0104776324226\n",
      "432.10035141398663\n",
      "385.3735244288118\n",
      "464.99996551395714\n",
      "453.3687788613362\n",
      "450.1551278911835\n",
      "338.25874449792286\n",
      "393.39547512328426\n",
      "380.07247422059135\n",
      "418.2625201745308\n",
      "407.4256330471721\n",
      "426.0222667617808\n",
      "494.93892309554246\n",
      "414.7189855013479\n",
      "408.9709536502079\n",
      "369.10013290740886\n",
      "409.42324272976157\n",
      "425.13049657533657\n",
      "441.0513068883531\n",
      "423.3726099945672\n",
      "418.8956187958709\n",
      "403.1450518351054\n",
      "374.5981380052072\n",
      "464.53503055944327\n",
      "405.6233220554139\n",
      "412.132600038519\n",
      "437.8361650237876\n",
      "407.374369863401\n",
      "421.73456772565\n",
      "395.93103179394524\n",
      "487.42814598933677\n",
      "444.5477992332375\n",
      "407.5090598013278\n",
      "412.1287281750582\n",
      "406.9683018508779\n",
      "417.0535518767756\n",
      "477.0147137202315\n",
      "405.4166263110544\n",
      "429.91908318259067\n",
      "454.3603805364111\n",
      "432.9485882496455\n",
      "445.5829237523085\n",
      "432.1445538301084\n",
      "408.6273505664537\n",
      "375.5635233344255\n",
      "419.0561558585915\n",
      "403.69878948344785\n",
      "437.3839289203611\n",
      "409.07381573542995\n",
      "409.77051412473986\n",
      "414.2206642118316\n",
      "399.07395141484676\n",
      "421.8736564653143\n",
      "371.9868212513454\n",
      "380.0390877039991\n",
      "431.0234176000238\n",
      "379.8619869418318\n",
      "411.82110328062004\n",
      "357.22528128797427\n",
      "406.7739492327406\n",
      "371.6677201867384\n",
      "396.3457450176803\n",
      "374.2123093322657\n",
      "508.1444504127071\n",
      "355.75318933610527\n",
      "389.50419608022634\n",
      "432.7909822114325\n",
      "361.33262021373366\n",
      "397.5934345960344\n",
      "486.1690740454669\n",
      "454.182149077372\n",
      "432.17934292689375\n",
      "347.23864068946983\n",
      "402.8068124174217\n",
      "462.7458569987782\n",
      "389.5571459872734\n",
      "422.70958106271615\n",
      "447.8179927255731\n",
      "447.2804843596337\n",
      "379.12761346856564\n",
      "418.20475699427254\n",
      "429.0167497492927\n",
      "426.6535803012367\n",
      "425.5554104139847\n",
      "348.3837595014159\n",
      "379.90021257821786\n",
      "474.04181463398925\n",
      "412.10177412138785\n",
      "425.9653975047353\n",
      "440.57534230644205\n",
      "429.0000952882711\n",
      "454.7811499656511\n",
      "397.8265370908853\n",
      "439.3236156187541\n",
      "364.74793185084695\n",
      "453.2468899915651\n",
      "433.570594208254\n",
      "387.65864034054334\n",
      "439.14637287435806\n",
      "407.6230963223511\n",
      "399.98005526074326\n",
      "402.19873324806184\n",
      "391.32117984099307\n",
      "424.00759087611283\n",
      "391.5668941874686\n",
      "410.69988983641747\n",
      "379.75672459813853\n",
      "413.0800437194856\n",
      "414.90724021506657\n",
      "406.3348541072615\n",
      "392.12769851161096\n",
      "417.5832034864359\n",
      "340.0430313447468\n",
      "399.73971808597724\n",
      "445.76826903605996\n",
      "412.46069005674946\n",
      "388.25305395666527\n",
      "461.31546758390704\n",
      "399.5536105944448\n",
      "449.67206440274987\n",
      "467.6929170657067\n",
      "368.53809266426185\n",
      "389.85094549329847\n",
      "410.2929948235702\n",
      "432.3432024190175\n",
      "432.9522045648236\n",
      "417.47852781749026\n",
      "379.8014236867927\n",
      "365.9279636768066\n",
      "464.92043106051466\n",
      "370.9576474317954\n",
      "441.5682115933856\n",
      "368.0423952441837\n",
      "426.51080028108424\n",
      "418.6168472660618\n",
      "446.35890945510926\n",
      "398.70209847059186\n",
      "422.9089964987326\n",
      "358.77642939075537\n",
      "425.12196260236726\n",
      "425.1624478722158\n",
      "375.6739461615266\n",
      "504.58968359109264\n",
      "382.65716490988547\n",
      "393.88490223862243\n",
      "413.0165406338949\n",
      "392.3327238677378\n",
      "423.22971424510115\n",
      "450.67809289305643\n",
      "449.65775869529693\n",
      "426.5779336018404\n",
      "393.33234400083705\n",
      "433.0798479796387\n",
      "491.7526468995501\n",
      "411.3122866257879\n",
      "426.63263127210274\n",
      "375.2294091371527\n",
      "412.1959382873694\n",
      "452.811136295092\n",
      "423.3997868943691\n",
      "461.92543333996247\n",
      "350.065249702993\n",
      "447.41229955618377\n",
      "416.25879982841957\n",
      "455.97435913728543\n",
      "394.656971861779\n",
      "437.7385455840464\n",
      "357.4022547518839\n",
      "368.74027688289823\n",
      "375.2345133601803\n",
      "345.1449516835151\n",
      "456.74521268974104\n",
      "391.9004505782058\n",
      "402.2231892556541\n",
      "399.0800282448356\n",
      "395.91326157328353\n",
      "383.35418417886916\n",
      "389.3710899535581\n",
      "403.6852233846234\n",
      "439.57770311641866\n",
      "406.72212759529447\n",
      "391.3300799449307\n",
      "389.0439026079505\n",
      "468.9852258781757\n",
      "394.0168025952143\n",
      "432.0537988581086\n",
      "330.35735226240473\n",
      "398.03689112749635\n",
      "347.52247299569143\n",
      "367.2091728941068\n",
      "373.88544717938464\n",
      "455.405622597796\n",
      "461.48484772229693\n",
      "393.68444686546195\n",
      "444.3257202808579\n",
      "406.88151222584906\n",
      "379.376472899389\n",
      "385.478238414378\n",
      "392.970293261134\n",
      "396.8991436697554\n",
      "416.5120094854258\n",
      "440.3974779052605\n",
      "383.17387899489154\n",
      "371.13224722539087\n",
      "427.42491969106385\n",
      "420.9298421619208\n",
      "452.8665961670561\n",
      "442.32585126872067\n",
      "435.9107882649158\n",
      "424.71412356332127\n",
      "404.77910339356595\n",
      "436.59094738073577\n",
      "451.55550859254566\n",
      "394.87795962525297\n",
      "429.7657314074419\n",
      "473.9025482413437\n",
      "409.33611402913374\n",
      "434.6979742059833\n",
      "395.5124991992268\n",
      "451.5340319213296\n",
      "426.31695073154276\n",
      "331.8982156575984\n",
      "365.9893081994448\n",
      "418.28965582933944\n",
      "354.48818305637644\n",
      "416.935250563842\n",
      "387.3092404932236\n",
      "392.4683535507852\n",
      "411.8380550784763\n",
      "477.7140259780972\n",
      "439.17835115229536\n",
      "408.09854177017354\n",
      "435.7596810900471\n",
      "392.8316505878944\n",
      "433.79326362755427\n",
      "338.1742940657175\n",
      "369.0497095962288\n",
      "340.4360626990284\n",
      "394.6621546686779\n",
      "417.2339161119678\n",
      "409.1587871916262\n",
      "478.00651897010886\n",
      "420.6585657243112\n",
      "394.83391426003584\n",
      "402.1689597796038\n",
      "409.790784481491\n",
      "391.602552384678\n",
      "413.2028827116035\n",
      "459.9632053825295\n",
      "439.56279334571934\n",
      "371.38003245991115\n",
      "439.38609304835904\n",
      "414.27185014183226\n",
      "370.2488114525599\n",
      "383.6515068606798\n",
      "422.48618462076547\n",
      "429.90225491835884\n",
      "365.3449825075864\n",
      "423.35044543347874\n",
      "426.4256730733874\n",
      "380.49700072414225\n",
      "486.46301778220106\n",
      "452.84373275081066\n",
      "391.66147915861\n",
      "397.85694304218583\n",
      "474.8947997225257\n",
      "439.15188074737785\n",
      "381.6101280099384\n",
      "392.71684735901056\n",
      "401.90800956492575\n",
      "412.4618893755107\n",
      "453.18151010712694\n",
      "481.14006882863634\n",
      "394.8039550847953\n",
      "374.2536398933251\n",
      "432.5040499702368\n",
      "370.34767925753505\n",
      "431.96752011561983\n",
      "461.784787974388\n",
      "429.05851085602995\n",
      "434.7449864146157\n",
      "470.3338911505887\n",
      "392.8555946783509\n",
      "369.68270000777056\n",
      "370.70603871281213\n",
      "369.4860156844287\n",
      "462.59386969287954\n",
      "324.4270138806524\n",
      "367.23696146119045\n",
      "394.2991079772013\n",
      "447.3462295145899\n",
      "408.6894020641125\n",
      "455.79780243284983\n",
      "374.74714997793\n",
      "407.5361875546616\n",
      "442.5910543929872\n",
      "376.31679613979964\n",
      "399.52370539629044\n",
      "395.1320276320292\n",
      "455.83292805385827\n",
      "399.4922800013556\n",
      "435.0044975941926\n",
      "397.40203636988605\n",
      "406.6683651868657\n",
      "440.4805258481538\n",
      "436.0165752165043\n",
      "446.70689687043574\n",
      "451.8175243466419\n",
      "442.86936235440714\n",
      "401.6202169098094\n",
      "433.4008705158738\n",
      "368.48563696369825\n",
      "387.05753254930113\n",
      "421.20653607151195\n",
      "386.4896505427851\n",
      "395.12080427484193\n",
      "392.7296815917728\n",
      "398.76424695211006\n",
      "417.23271095595305\n",
      "361.54697270846026\n",
      "413.2656468351456\n",
      "442.8022129041649\n",
      "417.6826244077276\n",
      "402.1205254053733\n",
      "413.44640005307593\n",
      "343.40760753813584\n",
      "422.07041891575193\n",
      "417.6575297937564\n",
      "410.6669587842479\n",
      "436.8338558204607\n",
      "473.14993559254026\n",
      "475.03910508646754\n",
      "417.35326261599795\n",
      "376.6742307050827\n",
      "362.49635221300485\n",
      "404.2346807469186\n",
      "428.51735109957866\n",
      "459.6079653395563\n",
      "380.543522647184\n",
      "379.50292667812073\n",
      "431.0124279525763\n",
      "390.89284118776965\n",
      "418.0395919833066\n",
      "412.2184041006596\n",
      "474.1691436585486\n",
      "415.81041029470305\n",
      "462.08670191630733\n",
      "385.6028824006664\n",
      "398.21638738758617\n",
      "405.36609730625673\n",
      "405.4624739357458\n",
      "463.62211465200994\n",
      "398.5757870050409\n",
      "422.0243966514034\n",
      "417.74210932950587\n",
      "461.1998327976692\n",
      "434.23451120360465\n",
      "390.23975385031065\n",
      "459.7458368011941\n",
      "385.68677078952084\n",
      "389.0598836651981\n",
      "409.9849905966138\n",
      "395.1925594036763\n",
      "454.84135814774606\n",
      "426.17688565587844\n",
      "402.223245408128\n",
      "422.6450687255967\n",
      "427.25534536844805\n",
      "453.660281811751\n",
      "433.5241436025688\n",
      "416.85741827019467\n",
      "447.1613938660598\n",
      "439.8205461241442\n",
      "425.52672317891233\n",
      "476.42733542110307\n",
      "404.0188861278338\n",
      "449.57262518383976\n",
      "336.11003819937673\n",
      "420.29032846365374\n",
      "410.08094536850274\n",
      "454.5315720977258\n",
      "408.8937064331422\n",
      "429.0877495055311\n",
      "444.33363772655093\n",
      "424.4165931989428\n",
      "363.96491380164434\n",
      "417.67292656005645\n",
      "410.78772084830683\n",
      "404.60428752963446\n",
      "479.3769613789469\n",
      "400.3877194945842\n",
      "404.5198331618053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359.01850047376195\n",
      "351.022812065781\n",
      "344.7799295198776\n",
      "406.59393232280706\n",
      "435.3003510731659\n",
      "364.9032937204606\n",
      "441.70930600013054\n",
      "451.4664976534799\n",
      "447.2778896322771\n",
      "421.0114871363302\n",
      "424.60323994692044\n",
      "435.1887599005159\n",
      "362.1127034949613\n",
      "424.35589630167686\n",
      "418.04688486689656\n",
      "364.7003981465976\n",
      "427.93576630137096\n",
      "395.6930114253884\n",
      "439.43997048347256\n",
      "401.83865342115473\n",
      "393.1549966814823\n",
      "406.1535538753019\n",
      "397.26042597241366\n",
      "359.80480144578655\n",
      "428.03812415754595\n",
      "431.3964055910191\n",
      "442.66676143718854\n",
      "440.2181959755487\n",
      "386.8097097039437\n",
      "401.5077117687999\n",
      "371.94266370084546\n",
      "446.7304798745164\n",
      "457.37184402764433\n",
      "389.3629902072579\n",
      "428.258288212274\n",
      "495.5292650820476\n",
      "405.3087922597126\n",
      "428.2789077128457\n",
      "473.82715330858207\n",
      "420.9769919982059\n",
      "404.608883634002\n",
      "383.1435289149553\n",
      "340.11082827214454\n",
      "405.39776437470357\n",
      "351.96913094158657\n",
      "438.1038904833677\n",
      "434.55677393367705\n",
      "359.6966661599374\n",
      "404.77670602599824\n",
      "455.1453886212349\n",
      "436.37132974377937\n",
      "366.5458057551057\n",
      "467.2610501671915\n",
      "391.3951075844354\n",
      "428.33259351834187\n",
      "322.76413152079033\n",
      "432.6809306897917\n",
      "415.6399234256251\n",
      "463.26162946272547\n",
      "427.9070130485781\n",
      "428.38716811269927\n",
      "426.16061984383055\n",
      "423.63441339202194\n",
      "416.5169553397925\n",
      "490.4139277996645\n",
      "447.1034506227997\n",
      "408.8800840980625\n",
      "438.7905852399068\n",
      "425.8964144206344\n",
      "411.4525711259596\n",
      "457.85125248271214\n",
      "351.40965666198286\n",
      "422.3082933848075\n",
      "405.0114306285205\n",
      "410.7332541012271\n",
      "405.20090207407225\n",
      "443.8393914984727\n",
      "391.5549750619498\n",
      "406.0141972736399\n",
      "393.8886491755249\n",
      "387.1671552349627\n",
      "468.5393701818905\n",
      "364.8259925825476\n",
      "466.19274212871505\n",
      "421.2830488685372\n",
      "444.9042217751126\n",
      "414.55170605214937\n",
      "347.5348462171003\n",
      "374.0005398968245\n",
      "364.3185885465189\n",
      "423.64819918873053\n",
      "473.4967327498598\n",
      "406.461182870299\n",
      "369.47231632814805\n",
      "394.6641209160615\n",
      "412.03964208241524\n",
      "406.199942891378\n",
      "389.20521180648325\n",
      "429.1447482716961\n",
      "450.91955670687065\n",
      "445.14222665877753\n",
      "393.9537327974451\n",
      "440.8823106433134\n",
      "358.8748474020509\n",
      "411.05758057347776\n",
      "412.0492758176045\n",
      "372.4968991376294\n",
      "440.6818937428101\n",
      "407.22319767448425\n",
      "440.2896595304329\n",
      "280.06011769927574\n",
      "436.03172750098105\n",
      "412.2774410307658\n",
      "382.21320930223806\n",
      "435.0958714724516\n",
      "364.3020093727523\n",
      "395.3902657936703\n",
      "433.2482902141338\n",
      "446.47621184164854\n",
      "443.79439649139204\n",
      "391.40179601647907\n",
      "384.4349893119077\n",
      "403.90772761917594\n",
      "425.898384732691\n",
      "445.1654458191512\n",
      "370.3254596856568\n",
      "393.0311024310382\n",
      "443.20694037981093\n",
      "433.4689180303656\n",
      "356.18933285015805\n",
      "349.90542362542374\n",
      "456.64024658320756\n",
      "397.0206538498068\n",
      "359.3746917856889\n",
      "384.5914776455135\n",
      "372.09986109476915\n",
      "419.36414802445375\n",
      "411.71157387336416\n",
      "446.7065274274612\n",
      "449.2512288251857\n",
      "437.9374624694792\n",
      "407.1769620738892\n",
      "368.63781820857986\n",
      "473.7147486196016\n",
      "378.7905639183434\n",
      "354.4047953110657\n",
      "408.33106083818063\n",
      "397.1819847458322\n",
      "414.9548025681677\n",
      "470.3673907909376\n",
      "475.85885791328093\n",
      "357.8980399770399\n",
      "411.05611032532533\n",
      "432.7166285988496\n",
      "358.0971647211988\n",
      "388.0287686114563\n",
      "422.1664760034411\n"
     ]
    }
   ],
   "source": [
    "w1=results(mMVG,train_data,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogMVG = get_model(data_all, ns1,\"MVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogMVG2 = get_model(data_all, [[20, 20, 10], [20, 0, 0]],\"MVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_c = data_all.copy()\n",
    "data_all_c.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "w3=results(mlogMVG2,data_all,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.1000000000000005, [7, 3, 9, 5, 18])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for v,i in zip(w2,range(len(w2))):\n",
    "    if v == \"error\":\n",
    "        h.append(data_all[i][Rts[0]])\n",
    "    else:\n",
    "        h.append(v[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303.207571251549"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"ouralgo.csv\",h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[108.96,\n",
       " 7.1000000000000005,\n",
       " 5.069999999999999,\n",
       " 50.09,\n",
       " 128.29,\n",
       " 24.33,\n",
       " 5.63,\n",
       " 23.740000000000002,\n",
       " 91.2,\n",
       " 17.47,\n",
       " 55.49,\n",
       " 73.2,\n",
       " 4.84,\n",
       " 3.27,\n",
       " 1.19,\n",
       " 54.07,\n",
       " 6.46,\n",
       " 121.02000000000001,\n",
       " 7.09,\n",
       " 39.18,\n",
       " 132.85,\n",
       " 2.91,\n",
       " 5.75,\n",
       " 24.13,\n",
       " 1146.47,\n",
       " 1203.12,\n",
       " 5.16,\n",
       " 69.62,\n",
       " 4.6899999999999995,\n",
       " 112.49000000000001,\n",
       " 132.04,\n",
       " 5.03,\n",
       " 148.15,\n",
       " 4.97,\n",
       " 12.969999999999999,\n",
       " 453.65,\n",
       " 5.279999999999999,\n",
       " 3.6900000000000004,\n",
       " 27.009999999999998,\n",
       " 535.02,\n",
       " 6.1,\n",
       " 191.34,\n",
       " 3.0700000000000003,\n",
       " 1205.16,\n",
       " 46.46,\n",
       " 1205.12,\n",
       " 354.92,\n",
       " 1204.47,\n",
       " 27.34,\n",
       " 528.49,\n",
       " 1025.8,\n",
       " 7.039999999999999,\n",
       " 12.0,\n",
       " 21.34,\n",
       " 404.28000000000003,\n",
       " 4.83,\n",
       " 7.71,\n",
       " 160.18,\n",
       " 6.8999999999999995,\n",
       " 4.779999999999999,\n",
       " 0.59,\n",
       " 9.23,\n",
       " 2.75,\n",
       " 454.79,\n",
       " 622.94,\n",
       " 179.41,\n",
       " 419.98,\n",
       " 0.67,\n",
       " 125.82,\n",
       " 3.5100000000000002,\n",
       " 24.84,\n",
       " 0.81,\n",
       " 1203.0,\n",
       " 1100.1,\n",
       " 1192.97,\n",
       " 1205.21,\n",
       " 112.62,\n",
       " 67.65,\n",
       " 2.81,\n",
       " 21.46,\n",
       " 49.47,\n",
       " 57.71,\n",
       " 92.85,\n",
       " 0.01,\n",
       " 4.68,\n",
       " 186.93,\n",
       " 9.22,\n",
       " 7.47,\n",
       " 3.29,\n",
       " 2.52,\n",
       " 19.68,\n",
       " 20.13,\n",
       " 1203.3,\n",
       " 3.01,\n",
       " 1205.8,\n",
       " 2.92,\n",
       " 57.040000000000006,\n",
       " 3.2,\n",
       " 15.739999999999998,\n",
       " 27.21,\n",
       " 110.69,\n",
       " 255.82,\n",
       " 3.02,\n",
       " 1203.11,\n",
       " 648.26,\n",
       " 98.74,\n",
       " 554.11,\n",
       " 11.53,\n",
       " 86.31,\n",
       " 444.67,\n",
       " 3.02,\n",
       " 2.75,\n",
       " 35.05,\n",
       " 445.78,\n",
       " 153.88,\n",
       " 378.2,\n",
       " 5.33,\n",
       " 1.34,\n",
       " 14.17,\n",
       " 566.63,\n",
       " 27.32,\n",
       " 429.3,\n",
       " 4.930000000000001,\n",
       " 34.5,\n",
       " 3.3,\n",
       " 4.86,\n",
       " 1203.06,\n",
       " 31.259999999999998,\n",
       " 50.14,\n",
       " 1.07,\n",
       " 43.900000000000006,\n",
       " 5.22,\n",
       " 7.62,\n",
       " 3.3599999999999994,\n",
       " 907.76,\n",
       " 46.92,\n",
       " 1203.09,\n",
       " 6.390000000000001,\n",
       " 4.93,\n",
       " 2.87,\n",
       " 188.6,\n",
       " 345.48,\n",
       " 3.21,\n",
       " 606.99,\n",
       " 27.26,\n",
       " 47.620000000000005,\n",
       " 15.540000000000001,\n",
       " 53.32,\n",
       " 19.38,\n",
       " 29.28,\n",
       " 1203.31,\n",
       " 10.74,\n",
       " 172.79999999999998,\n",
       " 13.620000000000001,\n",
       " 5.43,\n",
       " 3.0300000000000002,\n",
       " 125.77,\n",
       " 4.88,\n",
       " 3.19,\n",
       " 3.05,\n",
       " 221.73,\n",
       " 12.23,\n",
       " 0.15,\n",
       " 73.21000000000001,\n",
       " 5.44,\n",
       " 4.69,\n",
       " 1225.15,\n",
       " 4.01,\n",
       " 1203.08,\n",
       " 4.220000000000001,\n",
       " 1223.14,\n",
       " 509.67,\n",
       " 2.83,\n",
       " 1204.48,\n",
       " 569.2099999999999,\n",
       " 1203.0,\n",
       " 975.98,\n",
       " 47.63,\n",
       " 5.32,\n",
       " 223.39,\n",
       " 2.9,\n",
       " 415.45,\n",
       " 0.1,\n",
       " 3.3200000000000003,\n",
       " 199.18,\n",
       " 47.13,\n",
       " 425.45000000000005,\n",
       " 3.52,\n",
       " 1067.0,\n",
       " 1201.49,\n",
       " 28.560000000000002,\n",
       " 51.330000000000005,\n",
       " 44.07,\n",
       " 20.61,\n",
       " 935.3199999999999,\n",
       " 1203.34,\n",
       " 3.33,\n",
       " 223.48000000000002,\n",
       " 255.96,\n",
       " 37.1,\n",
       " 448.0,\n",
       " 12.71,\n",
       " 1205.2,\n",
       " 1205.2,\n",
       " 1129.32,\n",
       " 1.45,\n",
       " 4.62,\n",
       " 230.62,\n",
       " 54.34,\n",
       " 55.1,\n",
       " 3.1500000000000004,\n",
       " 1201.22,\n",
       " 2.91,\n",
       " 28.07,\n",
       " 101.60000000000001,\n",
       " 23.95,\n",
       " 1205.03,\n",
       " 5.0,\n",
       " 55.269999999999996,\n",
       " 24.130000000000003,\n",
       " 1205.93,\n",
       " 22.84,\n",
       " 265.1,\n",
       " 236.06,\n",
       " 175.24,\n",
       " 9.14,\n",
       " 1203.07,\n",
       " 3.1,\n",
       " 420.62,\n",
       " 0.76,\n",
       " 305.78999999999996,\n",
       " 1003.77,\n",
       " 351.21999999999997,\n",
       " 34.24,\n",
       " 29.44,\n",
       " 3.01,\n",
       " 77.52,\n",
       " 6.76,\n",
       " 178.86,\n",
       " 1204.11,\n",
       " 3.26,\n",
       " 134.96,\n",
       " 5.6499999999999995,\n",
       " 11.469999999999999,\n",
       " 6.659999999999999,\n",
       " 1201.02,\n",
       " 410.13,\n",
       " 10.74,\n",
       " 145.44000000000003,\n",
       " 132.51,\n",
       " 107.68,\n",
       " 79.3,\n",
       " 0.03,\n",
       " 318.21000000000004,\n",
       " 5.6899999999999995,\n",
       " 103.59,\n",
       " 19.509999999999998,\n",
       " 33.33,\n",
       " 809.77,\n",
       " 38.93,\n",
       " 4.83,\n",
       " 9.049999999999999,\n",
       " 73.51,\n",
       " 683.72,\n",
       " 1203.01,\n",
       " 48.790000000000006,\n",
       " 43.01,\n",
       " 468.93,\n",
       " 31.28,\n",
       " 28.35,\n",
       " 3.19,\n",
       " 3.2,\n",
       " 4.71,\n",
       " 3.5,\n",
       " 1203.0,\n",
       " 2.94,\n",
       " 4.6899999999999995,\n",
       " 1.02,\n",
       " 3.1399999999999997,\n",
       " 3.0900000000000003,\n",
       " 75.57,\n",
       " 5.3100000000000005,\n",
       " 4.859999999999999,\n",
       " 1203.02,\n",
       " 32.57,\n",
       " 2.8,\n",
       " 5.4,\n",
       " 7.290000000000001,\n",
       " 937.29,\n",
       " 226.53,\n",
       " 39.75,\n",
       " 27.11,\n",
       " 889.32,\n",
       " 49.04,\n",
       " 5.220000000000001,\n",
       " 13.7,\n",
       " 5.04,\n",
       " 2.73,\n",
       " 4.01,\n",
       " 46.2,\n",
       " 498.53,\n",
       " 1225.2,\n",
       " 1226.18,\n",
       " 121.53999999999999,\n",
       " 520.62,\n",
       " 1203.18,\n",
       " 14.91,\n",
       " 1203.26,\n",
       " 124.36,\n",
       " 20.740000000000002,\n",
       " 6.46,\n",
       " 2.95,\n",
       " 147.66,\n",
       " 1203.0,\n",
       " 6.950000000000001,\n",
       " 1228.22,\n",
       " 6.49,\n",
       " 44.6,\n",
       " 2.6699999999999995,\n",
       " 0.96,\n",
       " 439.86,\n",
       " 24.97,\n",
       " 153.42,\n",
       " 112.65,\n",
       " 1203.31,\n",
       " 1203.01,\n",
       " 55.19,\n",
       " 59.209999999999994,\n",
       " 1205.0,\n",
       " 425.43,\n",
       " 1201.03,\n",
       " 144.48,\n",
       " 5.27,\n",
       " 1205.0,\n",
       " 4.779999999999999,\n",
       " 3.04,\n",
       " 873.3000000000001,\n",
       " 655.51,\n",
       " 239.28,\n",
       " 10.9,\n",
       " 5.42,\n",
       " 843.22,\n",
       " 1205.0,\n",
       " 7.219999999999999,\n",
       " 778.66,\n",
       " 13.3,\n",
       " 1205.12,\n",
       " 1.21,\n",
       " 56.79,\n",
       " 20.39,\n",
       " 10.0,\n",
       " 365.02,\n",
       " 15.57,\n",
       " 6.92,\n",
       " 29.159999999999997,\n",
       " 4.789999999999999,\n",
       " 6.9399999999999995,\n",
       " 7.24,\n",
       " 30.150000000000002,\n",
       " 1201.0,\n",
       " 1.03,\n",
       " 5.08,\n",
       " 63.74,\n",
       " 499.53999999999996,\n",
       " 1203.06,\n",
       " 4.47,\n",
       " 259.43,\n",
       " 118.67999999999999,\n",
       " 568.86,\n",
       " 77.09,\n",
       " 12.809999999999999,\n",
       " 1203.09,\n",
       " 16.189999999999998,\n",
       " 335.84,\n",
       " 30.439999999999998,\n",
       " 154.3,\n",
       " 1205.0,\n",
       " 4.06,\n",
       " 17.45,\n",
       " 534.8,\n",
       " 27.09,\n",
       " 115.15,\n",
       " 7.24,\n",
       " 25.86,\n",
       " 1207.07,\n",
       " 4.82,\n",
       " 99.9,\n",
       " 582.4,\n",
       " 1203.01,\n",
       " 714.1999999999999,\n",
       " 217.96,\n",
       " 22.029999999999998,\n",
       " 423.77,\n",
       " 1203.08,\n",
       " 3.45,\n",
       " 11.92,\n",
       " 7.08,\n",
       " 2.84,\n",
       " 3.31,\n",
       " 18.09,\n",
       " 34.1,\n",
       " 63.370000000000005,\n",
       " 1205.88,\n",
       " 38.71,\n",
       " 11.6,\n",
       " 9.37,\n",
       " 8.649999999999999,\n",
       " 849.8199999999999,\n",
       " 566.78,\n",
       " 4.66,\n",
       " 2.93,\n",
       " 51.53,\n",
       " 42.65,\n",
       " 49.089999999999996,\n",
       " 1207.36,\n",
       " 7.109999999999999,\n",
       " 1206.32,\n",
       " 1202.81,\n",
       " 1205.35,\n",
       " 3.69,\n",
       " 174.72,\n",
       " 265.53000000000003,\n",
       " 18.759999999999998,\n",
       " 18.64,\n",
       " 4.68,\n",
       " 1203.29,\n",
       " 10.12,\n",
       " 112.26,\n",
       " 5.859999999999999,\n",
       " 1203.12,\n",
       " 5.55,\n",
       " 4.51,\n",
       " 1203.87,\n",
       " 735.46,\n",
       " 108.98,\n",
       " 31.71,\n",
       " 43.949999999999996,\n",
       " 5.04,\n",
       " 12.799999999999999,\n",
       " 5.1000000000000005,\n",
       " 102.22,\n",
       " 5.79,\n",
       " 451.86999999999995,\n",
       " 414.21999999999997,\n",
       " 1223.83,\n",
       " 65.91999999999999,\n",
       " 53.8,\n",
       " 418.20000000000005,\n",
       " 3.0300000000000002,\n",
       " 225.68,\n",
       " 0.53,\n",
       " 3.17,\n",
       " 2.69,\n",
       " 1.02,\n",
       " 10.719999999999999,\n",
       " 21.779999999999998,\n",
       " 0.91,\n",
       " 107.41,\n",
       " 31.919999999999998,\n",
       " 1205.28,\n",
       " 321.1,\n",
       " 58.74,\n",
       " 3.8899999999999997,\n",
       " 31.940000000000005,\n",
       " 302.74,\n",
       " 2.7199999999999998,\n",
       " 105.3,\n",
       " 49.03,\n",
       " 0.02,\n",
       " 5.76,\n",
       " 1203.56,\n",
       " 4.82,\n",
       " 1205.01,\n",
       " 5.39,\n",
       " 16.72,\n",
       " 2.71,\n",
       " 1135.25,\n",
       " 39.75,\n",
       " 7.470000000000001,\n",
       " 7.279999999999999,\n",
       " 1203.45,\n",
       " 1203.0,\n",
       " 15.22,\n",
       " 18.72,\n",
       " 17.24,\n",
       " 2.9899999999999998,\n",
       " 466.49,\n",
       " 50.86,\n",
       " 3.06,\n",
       " 277.18,\n",
       " 3.2800000000000002,\n",
       " 180.30999999999997,\n",
       " 57.94,\n",
       " 1.54,\n",
       " 3.42,\n",
       " 106.31,\n",
       " 136.30999999999997,\n",
       " 55.35,\n",
       " 165.33999999999997,\n",
       " 229.59,\n",
       " 7.53,\n",
       " 0.66,\n",
       " 4.800000000000001,\n",
       " 7.449999999999999,\n",
       " 1223.2,\n",
       " 2.93,\n",
       " 1205.59,\n",
       " 5.18,\n",
       " 3.39,\n",
       " 1223.06,\n",
       " 4.82,\n",
       " 5.1000000000000005,\n",
       " 1207.95,\n",
       " 51.31999999999999,\n",
       " 1203.1,\n",
       " 74.66,\n",
       " 1203.29,\n",
       " 5.94,\n",
       " 3.62,\n",
       " 5.319999999999999,\n",
       " 1203.18,\n",
       " 3.31,\n",
       " 62.269999999999996,\n",
       " 411.85999999999996,\n",
       " 21.38,\n",
       " 2.99,\n",
       " 22.49,\n",
       " 3.57,\n",
       " 4.04,\n",
       " 199.15,\n",
       " 8.84,\n",
       " 36.14,\n",
       " 1203.9,\n",
       " 51.050000000000004,\n",
       " 1203.22,\n",
       " 31.040000000000003,\n",
       " 1202.1,\n",
       " 57.74,\n",
       " 298.98,\n",
       " 1203.0,\n",
       " 26.14,\n",
       " 9.24,\n",
       " 25.45,\n",
       " 294.01,\n",
       " 30.68,\n",
       " 249.63,\n",
       " 258.84,\n",
       " 1205.5,\n",
       " 0.22,\n",
       " 3.44,\n",
       " -1024.0,\n",
       " 68.06,\n",
       " 16.52,\n",
       " 3.6199999999999997,\n",
       " 1208.77,\n",
       " 16.580000000000002,\n",
       " 202.35,\n",
       " 5.98,\n",
       " 69.75,\n",
       " 4.17,\n",
       " 5.09,\n",
       " 1223.01,\n",
       " 1203.0,\n",
       " 2.64,\n",
       " 31.82,\n",
       " 46.98,\n",
       " 1207.62,\n",
       " 6.08,\n",
       " 1203.1,\n",
       " 238.09,\n",
       " 61.33,\n",
       " 33.41,\n",
       " 482.61,\n",
       " 2.0,\n",
       " 1046.35,\n",
       " 198.63,\n",
       " 2.4099999999999997,\n",
       " 130.64,\n",
       " 67.37,\n",
       " 2.92,\n",
       " 74.99,\n",
       " 4.09,\n",
       " 242.28,\n",
       " 56.839999999999996,\n",
       " 276.3,\n",
       " 7.48,\n",
       " 1202.45,\n",
       " 1159.8799999999999,\n",
       " 193.4,\n",
       " 132.93,\n",
       " 2.0,\n",
       " 0.92,\n",
       " 80.64,\n",
       " 59.24,\n",
       " 2.5199999999999996,\n",
       " 3.46,\n",
       " 46.220000000000006,\n",
       " 11.190000000000001,\n",
       " 5.74,\n",
       " 1.73,\n",
       " 255.98999999999998,\n",
       " 6.04,\n",
       " 6.68,\n",
       " 3.34,\n",
       " 3.73,\n",
       " 1203.08,\n",
       " 1204.44,\n",
       " 1210.76,\n",
       " 1119.5,\n",
       " 100.60000000000001,\n",
       " 181.72,\n",
       " 4.75,\n",
       " 27.16,\n",
       " 1.48,\n",
       " 40.7,\n",
       " 54.82,\n",
       " 342.83,\n",
       " 1203.05,\n",
       " 109.52000000000001,\n",
       " 21.79,\n",
       " 895.11,\n",
       " 4.79,\n",
       " 62.519999999999996,\n",
       " 65.28999999999999,\n",
       " -511.96,\n",
       " 54.41,\n",
       " 7.65,\n",
       " 25.54,\n",
       " 3.2,\n",
       " 1222.08,\n",
       " 5.12,\n",
       " 89.0,\n",
       " 1177.32,\n",
       " 4.12,\n",
       " 7.43,\n",
       " 4.16,\n",
       " 98.61,\n",
       " 754.25,\n",
       " 0.5900000000000001,\n",
       " 34.51,\n",
       " 87.64,\n",
       " 2.06,\n",
       " 411.71,\n",
       " 106.98,\n",
       " 2.0399999999999996,\n",
       " 22.79,\n",
       " 2.65,\n",
       " 316.89,\n",
       " 248.65,\n",
       " 4.22,\n",
       " 1227.21,\n",
       " 70.11,\n",
       " 0.22,\n",
       " 4.14,\n",
       " 22.86,\n",
       " 1065.1,\n",
       " 748.99,\n",
       " 407.8,\n",
       " 85.9,\n",
       " 1203.0,\n",
       " 2.05,\n",
       " 2.26,\n",
       " 45.62,\n",
       " 1205.09,\n",
       " 2.57,\n",
       " 41.4,\n",
       " 6.72,\n",
       " 4.1,\n",
       " 1080.1100000000001,\n",
       " 1205.63,\n",
       " 22.02,\n",
       " 65.07,\n",
       " 1205.48,\n",
       " 12.66,\n",
       " 26.9,\n",
       " 1204.82,\n",
       " 4.5,\n",
       " 1203.02,\n",
       " 796.21,\n",
       " 1.9,\n",
       " 1205.63,\n",
       " 1203.16,\n",
       " 14.559999999999999,\n",
       " 248.98000000000002,\n",
       " 2.3600000000000003,\n",
       " 24.98,\n",
       " 1203.2,\n",
       " 203.37,\n",
       " 111.05999999999999,\n",
       " 167.34,\n",
       " 354.27,\n",
       " 12.71,\n",
       " 22.37,\n",
       " 919.6800000000001,\n",
       " 1227.55,\n",
       " 2.4,\n",
       " 0.87,\n",
       " 213.93,\n",
       " 2.0,\n",
       " 1218.42,\n",
       " 30.479999999999997,\n",
       " 250.92000000000002,\n",
       " 5.36,\n",
       " 18.64,\n",
       " 22.68,\n",
       " 0.04,\n",
       " 1093.0,\n",
       " 4.369999999999999,\n",
       " 14.260000000000002,\n",
       " 16.21,\n",
       " 509.26,\n",
       " 5.87,\n",
       " 14.8,\n",
       " 1223.58,\n",
       " 127.42,\n",
       " 32.45,\n",
       " 79.9,\n",
       " 4.49,\n",
       " 1.69,\n",
       " 2.0,\n",
       " 2.61,\n",
       " 1205.09,\n",
       " -1021.46,\n",
       " 14.1,\n",
       " 1205.0,\n",
       " 3.33,\n",
       " 1205.97,\n",
       " 4.01,\n",
       " 26.14,\n",
       " 1208.71,\n",
       " 72.64,\n",
       " 2.27,\n",
       " 623.35,\n",
       " 1223.15,\n",
       " 16.34,\n",
       " 41.98,\n",
       " 52.08,\n",
       " 10.42,\n",
       " 66.74,\n",
       " 1203.0,\n",
       " 7.29,\n",
       " 48.34,\n",
       " 3.98,\n",
       " 659.77,\n",
       " 376.01,\n",
       " 30.279999999999998,\n",
       " 1.35,\n",
       " 329.15999999999997,\n",
       " 2.48,\n",
       " 1204.19,\n",
       " 9.120000000000001,\n",
       " 2.59,\n",
       " 1204.99,\n",
       " 0.36,\n",
       " 54.68,\n",
       " 1206.5,\n",
       " 893.21,\n",
       " 1201.41,\n",
       " 5.16,\n",
       " 29.26,\n",
       " 6.720000000000001,\n",
       " 5.83,\n",
       " 178.64000000000001,\n",
       " 24.97,\n",
       " 22.51,\n",
       " 1205.24,\n",
       " 2.0,\n",
       " 732.11,\n",
       " 0.01,\n",
       " 4.82,\n",
       " 195.79,\n",
       " 2.02,\n",
       " 1203.01,\n",
       " 1227.31,\n",
       " 0.98,\n",
       " 1203.18,\n",
       " 8.78,\n",
       " 209.44,\n",
       " 1007.69,\n",
       " 22.630000000000003,\n",
       " 0.01,\n",
       " 1208.51,\n",
       " 3.91,\n",
       " 99.23,\n",
       " 4.2,\n",
       " 4.0,\n",
       " 2.4099999999999997,\n",
       " 0.44,\n",
       " 16.740000000000002,\n",
       " 24.540000000000003,\n",
       " 29.57,\n",
       " 2.2199999999999998,\n",
       " 43.84,\n",
       " 462.2,\n",
       " 16.62,\n",
       " 1203.01,\n",
       " 1223.49,\n",
       " 19.24,\n",
       " 7.890000000000001,\n",
       " 118.09,\n",
       " 0.85,\n",
       " 49.480000000000004,\n",
       " 14.18,\n",
       " 231.25,\n",
       " 2.14,\n",
       " 229.45999999999998,\n",
       " 702.88,\n",
       " 24.009999999999998,\n",
       " 64.24000000000001,\n",
       " 0.42,\n",
       " 66.03999999999999,\n",
       " 48.59,\n",
       " 2.26,\n",
       " 1203.42,\n",
       " 4.22,\n",
       " 1223.11,\n",
       " 82.07000000000001,\n",
       " 6.28,\n",
       " 1203.0,\n",
       " 124.5,\n",
       " 22.79,\n",
       " 1205.14,\n",
       " 22.86,\n",
       " 6.31,\n",
       " 82.91,\n",
       " 205.32,\n",
       " 3.02,\n",
       " 4.28,\n",
       " 6.67,\n",
       " 774.0,\n",
       " 1203.13,\n",
       " 8.64,\n",
       " 0.92,\n",
       " 370.63,\n",
       " 48.019999999999996,\n",
       " 13.780000000000001,\n",
       " 1205.97,\n",
       " 396.26,\n",
       " 82.41,\n",
       " 46.24,\n",
       " 1204.99,\n",
       " 359.25,\n",
       " 590.46,\n",
       " 3.42,\n",
       " 23.02,\n",
       " 1203.23,\n",
       " 1205.13,\n",
       " 5.46,\n",
       " 7.47,\n",
       " 7.040000000000001,\n",
       " 23.89,\n",
       " 1205.0,\n",
       " 4.319999999999999,\n",
       " 1203.0,\n",
       " 8.14,\n",
       " 1225.67,\n",
       " 40.75,\n",
       " 1225.31,\n",
       " 2.7399999999999998,\n",
       " 34.1,\n",
       " 109.77,\n",
       " 189.88,\n",
       " 1205.61,\n",
       " 11.309999999999999,\n",
       " 530.94,\n",
       " 1205.09,\n",
       " 0.02,\n",
       " 172.76999999999998,\n",
       " 483.45,\n",
       " 15.05,\n",
       " 1203.0,\n",
       " 68.56,\n",
       " 825.96,\n",
       " 126.00999999999999,\n",
       " 1206.57,\n",
       " 4.38,\n",
       " 1203.0,\n",
       " 62.589999999999996,\n",
       " 3.4299999999999997,\n",
       " 57.26,\n",
       " 4.59,\n",
       " 454.1,\n",
       " 973.11,\n",
       " 2.19,\n",
       " 1203.43,\n",
       " 1225.54,\n",
       " 80.27,\n",
       " 172.29,\n",
       " 334.27,\n",
       " 152.12,\n",
       " 1205.77,\n",
       " 8.04,\n",
       " 618.85,\n",
       " 15.809999999999999,\n",
       " -1023.1500000000001,\n",
       " 1109.8,\n",
       " 1050.48,\n",
       " 211.57,\n",
       " 26.430000000000003,\n",
       " 87.49,\n",
       " 2.01,\n",
       " 31.05,\n",
       " 1202.99,\n",
       " 2.5599999999999996,\n",
       " 495.51,\n",
       " 6.23,\n",
       " 4.3,\n",
       " 5.23,\n",
       " 790.47,\n",
       " 21.39,\n",
       " 40.690000000000005,\n",
       " 320.39,\n",
       " 8.120000000000001,\n",
       " 1202.27,\n",
       " 346.84999999999997,\n",
       " 1015.3100000000001,\n",
       " 1205.0,\n",
       " 954.03,\n",
       " 182.09,\n",
       " 51.39,\n",
       " 1203.08,\n",
       " 3.96,\n",
       " 1205.47,\n",
       " 31.74,\n",
       " 61.47,\n",
       " 1205.24,\n",
       " 12.99,\n",
       " 1203.18,\n",
       " 728.4000000000001,\n",
       " 11.18,\n",
       " 587.68,\n",
       " 903.99,\n",
       " 1205.15,\n",
       " 222.33,\n",
       " 7.1000000000000005,\n",
       " 1227.92,\n",
       " 1205.12,\n",
       " 52.82,\n",
       " 1227.82,\n",
       " 16.45,\n",
       " 61.39,\n",
       " 1203.04,\n",
       " 2.16,\n",
       " 4.1899999999999995,\n",
       " 141.53,\n",
       " 15.27,\n",
       " 6.38,\n",
       " 1.82,\n",
       " 178.88000000000002,\n",
       " 20.0,\n",
       " 0.02,\n",
       " 768.19,\n",
       " 62.83,\n",
       " 1203.98,\n",
       " 1203.4,\n",
       " 22.16,\n",
       " 182.48000000000002,\n",
       " 945.5,\n",
       " 379.95,\n",
       " 26.159999999999997,\n",
       " 1223.12,\n",
       " 2.07,\n",
       " 3.0799999999999996,\n",
       " 38.199999999999996,\n",
       " 43.61,\n",
       " 1203.05,\n",
       " 331.99,\n",
       " 3.0100000000000002,\n",
       " 1203.88,\n",
       " 5.249999999999999,\n",
       " 10.48,\n",
       " 14.53,\n",
       " 0.01,\n",
       " 1202.6,\n",
       " 1223.12,\n",
       " 6.53,\n",
       " 1203.42,\n",
       " 3.9299999999999997,\n",
       " 1225.94,\n",
       " 12.67,\n",
       " 220.86,\n",
       " 259.94,\n",
       " 75.54,\n",
       " 36.5,\n",
       " 1213.35,\n",
       " 24.900000000000002,\n",
       " 57.72,\n",
       " 2.1,\n",
       " 50.99,\n",
       " 11.030000000000001,\n",
       " 29.81,\n",
       " 4.4,\n",
       " 4.709999999999999,\n",
       " 1203.89,\n",
       " 4.21,\n",
       " 316.94000000000005,\n",
       " 481.77,\n",
       " 163.60999999999999,\n",
       " 2.57,\n",
       " 86.6,\n",
       " ...]"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343.68555416405826"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(w2g[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogMVG2 = get_model(data_all, ns4,\"MVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "z=[]\n",
    "for r in w2g[:,1]:\n",
    "    #print(r[0])\n",
    "    z.append(r[0])\n",
    "\n",
    "    \n",
    "z2=[]\n",
    "for r in range(len(Fts)):\n",
    "    z2.append(z.count(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModeResult(mode=array([9], dtype=int64), count=array([568]))"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 13, 11, 10, 34, 78, 152, 227, 488, 568]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "w2=results(mlogMVG,data_all,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3g=select(lambda v: v!=\"error\",w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1614"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343.68555416405826"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(w2g[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471.27799999999996"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(w2[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=results(mlogMVG2,data_all[100:110],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108.69, 1201.0, 0.79, 1201.0, 1012.2, 1201.0, 553.7, 1201.0, 34.34,\n",
       "       444.45], dtype=object)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.44504788],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30029665],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(s2)-s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=mp(logify,data_all_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-512.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-1140a29a4606>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   3333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3334\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 3335\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   3336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "np.mean(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-288-9b27ced9290b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmMVG3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"MVG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmlogMVG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"logMVG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"spec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ns' is not defined"
     ]
    }
   ],
   "source": [
    "ns1 = [[10,10,3],[10,0,0]]\n",
    "ns2 = [[20,20,5],[20,20,5],[20,0,0]]\n",
    "ns3 = [[10,10,10],[10,10,10],[10,0,0]]\n",
    "mMVG1 = get_model(train_data, ns1,\"MVG\")\n",
    "mMVG2 = get_model(train_data, ns2,\"MVG\")\n",
    "mMVG3 = get_model(train_data, ns3,\"MVG\")\n",
    "\n",
    "mlogMVG = get_model(train_data, ns,\"logMVG\")\n",
    "mspec = get_model(train_data, ns,\"spec\")\n",
    "\n",
    "results(mMVG,test_data)\n",
    "\n",
    "MVG_clf_r = results(mMVG,clf_data)\n",
    "_,MVG_f1 = mMVG(np.array([],dtype=int),[])\n",
    "\n",
    "clf_DDQN = clf.get_clf(clf_data,DDQN_clf_r, DDQN_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mMVG1 = get_model(train_data, ns1,\"MVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DFCTech\\mixture_model.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cond_weights /= np.sum(cond_weights)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-9096bc9daff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmMMVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-4914cae86d30>\u001b[0m in \u001b[0;36mf\u001b[1;34m(Ks, ks)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mbest_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_left_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_samp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbest_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8d3c014e9a5c>\u001b[0m in \u001b[0;36mtime_left_2\u001b[1;34m(Ks, ks, ns, c_samp)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mFVFT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFv_opt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFt_opt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mbest_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_left_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFVFT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfvft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_samp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0me_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8d3c014e9a5c>\u001b[0m in \u001b[0;36mtime_left_2\u001b[1;34m(Ks, ks, ns, c_samp)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#Ws=Ks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#print(Ks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0msamp_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_samp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0me_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mbest_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me_xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-4914cae86d30>\u001b[0m in \u001b[0;36mc_samp\u001b[1;34m(As, Bs, bs, n)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mc_samp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0ms1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixture_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_from_conditional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0ms2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mminsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\mixture_model.py\u001b[0m in \u001b[0;36msample_from_conditional\u001b[1;34m(model, inds, vals, sample_inds, size)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mind_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mout_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_inds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     return multivariate_normal.rvs(mean=cond_means[component], \n\u001b[0;32m     73\u001b[0m                                    \u001b[0mcov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond_covs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "mMMVG(np.array([],dtype=int),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7070575 ,  0.49625763, -0.83767706, -2.20337326],\n",
       "       [ 0.10672104, -0.18369957, -0.51444226, -2.24974072],\n",
       "       [-0.07300023, -0.67187412, -0.14772439, -2.15015201],\n",
       "       [ 0.06262223,  0.38130998,  0.11413885, -2.20817153],\n",
       "       [ 0.74629937,  0.74376604,  0.40545314, -2.26275541],\n",
       "       [ 0.20913755, -0.51351663,  0.55211281, -2.25210996],\n",
       "       [ 0.09155777, -0.18722461,  0.24011183, -2.24071431],\n",
       "       [-0.02564298, -0.75768903,  0.31984521, -2.3139385 ],\n",
       "       [-0.73692817, -0.59450435, -0.36959817, -2.27986289],\n",
       "       [ 0.36916779, -0.09344115, -0.65100586, -2.23639052]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_samp([0,1,2,52],np.array([],dtype=int),[],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "52",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-9096bc9daff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmMMVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-11637a687697>\u001b[0m in \u001b[0;36mf\u001b[1;34m(Ks, ks)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mbest_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_left_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_samp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbest_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-8d3c014e9a5c>\u001b[0m in \u001b[0;36mtime_left_2\u001b[1;34m(Ks, ks, ns, c_samp)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mFv_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFt_v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFt_opt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m#sample feature times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0msamp_ft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_samp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFt_opt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_ft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0me_ft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamp_ft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-11637a687697>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(As, Bs, bs, n)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mmixture_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./GaussianMixture.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mc_samp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mAs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mmixture_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_from_conditional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\mixture_model.py\u001b[0m in \u001b[0;36msample_from_conditional\u001b[1;34m(model, inds, vals, sample_inds, size)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mind_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mout_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_inds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     return multivariate_normal.rvs(mean=cond_means[component], \n",
      "\u001b[1;32m~\\mixture_model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mind_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mout_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_inds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     return multivariate_normal.rvs(mean=cond_means[component], \n",
      "\u001b[1;31mKeyError\u001b[0m: 52"
     ]
    }
   ],
   "source": [
    "mMMVG(np.array([],dtype=int),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "w1=results(mMVG1,test_data[0:10],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.32499999999993"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "w2=results(mMVG2,test_data[0:10],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "ns3 = [[20,20,8],[20,20,3],[20,0,0]]\n",
    "mMVG3 = get_model(train_data, ns3,\"MVG\")\n",
    "w3=results(mMVG3,test_data[0:10],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.969999999999999, 1229.64, 1215.8, 1211.37, 24.32, 22.93, 188.38,\n",
       "       37.32, 72.94], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(w2[:,0],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443.2944444444445"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([8.969999999999999, 1229.64, 1215.8, 1211.37, 24.32, 22.93, 188.38,\n",
    "       37.32, 72.94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562.5544444444445"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([8.819999999999999, 47.82, 15.719999999999999, 10.4, 1212.33,\n",
    "       39.30999999999999, 1247.59, 1214.04, 1266.96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1209.62, 1229.63,   17.82,   10.38,   17.47,   67.66,  194.66,\n",
       "       1212.68,   29.73])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(w1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.819999999999999, 47.82, 15.719999999999999, 10.4, 1212.33,\n",
       "       39.30999999999999, 1247.59, 1214.04, 1266.96], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(w3[:,0],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = pickle.load(open(r\"C:\\Users\\DFCTech\\Downloads\\GaussianMixture.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
       "                means_init=None, n_components=28, n_init=1,\n",
       "                precisions_init=None, random_state=None, reg_covar=1e-10,\n",
       "                tol=0.001, verbose=0, verbose_interval=10, warm_start=False,\n",
       "                weights_init=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = ('MVG','LogMVG', 'LogMixtureMVG','TruncatedMVG')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = np.array([471.28,343.69,403.32,303.21])#taken from scattered outputs\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('time')\n",
    "plt.title('tuning MBNG joint dist')\n",
    "\n",
    "plt.savefig(\"dist_tune.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = ('n1=100','n1=5, n2=5','n1=8, n2=3')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [np.mean(np.delete(w1,3)),np.mean(np.delete(w2[:,0],3)),np.mean(np.delete(w3[:,0],3))]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('time')\n",
    "plt.title('tuning MBNG recursive depth')\n",
    "\n",
    "#plt.savefig(\"recursive_depth.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([323.32, 363.29, 482.55])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bs = np.array([],dtype=int)\n",
    "bs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('explore', 9)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mMVG3(Bs,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode: 0, Reward: -1201.0, avg loss: 0.000, eps: 1.000\n",
      "Episode: 1, Reward: -546.47, avg loss: 0.000, eps: 0.999\n",
      "Episode: 2, Reward: -1202.46, avg loss: 0.000, eps: 0.998\n",
      "Episode: 3, Reward: -1203.65, avg loss: 0.000, eps: 0.996\n",
      "Episode: 4, Reward: -204.25, avg loss: 0.000, eps: 0.994\n",
      "Episode: 5, Reward: -12.3, avg loss: 0.000, eps: 0.993\n",
      "Episode: 6, Reward: -1207.42, avg loss: 0.000, eps: 0.989\n",
      "Episode: 7, Reward: -1205.57, avg loss: 0.000, eps: 0.988\n",
      "Episode: 8, Reward: -1201.0, avg loss: 0.000, eps: 0.988\n",
      "Episode: 9, Reward: -20.32, avg loss: 0.000, eps: 0.987\n",
      "Episode: 10, Reward: -394.28, avg loss: 0.000, eps: 0.986\n",
      "Episode: 11, Reward: -1203.3, avg loss: 0.000, eps: 0.983\n",
      "Episode: 12, Reward: -1201.0, avg loss: 0.000, eps: 0.983\n",
      "Episode: 13, Reward: -0.34, avg loss: 0.000, eps: 0.982\n",
      "Episode: 14, Reward: -0.07, avg loss: 0.000, eps: 0.981\n",
      "Episode: 15, Reward: -10.75, avg loss: 0.000, eps: 0.980\n",
      "Episode: 16, Reward: -1201.0, avg loss: 0.000, eps: 0.979\n",
      "Episode: 17, Reward: -1201.03, avg loss: 0.000, eps: 0.978\n",
      "Episode: 18, Reward: -1202.89, avg loss: 0.000, eps: 0.977\n",
      "Episode: 19, Reward: -1201.06, avg loss: 0.000, eps: 0.976\n",
      "Episode: 20, Reward: -1201.0, avg loss: 0.000, eps: 0.976\n",
      "Episode: 21, Reward: -1203.43, avg loss: 0.000, eps: 0.974\n",
      "Episode: 22, Reward: -1201.0, avg loss: 0.000, eps: 0.974\n",
      "Episode: 23, Reward: -79.53999999999999, avg loss: 0.000, eps: 0.973\n",
      "Episode: 24, Reward: -0.09, avg loss: 0.000, eps: 0.972\n",
      "Episode: 25, Reward: -1201.0, avg loss: 0.000, eps: 0.972\n",
      "Episode: 26, Reward: -22.32, avg loss: 0.000, eps: 0.971\n",
      "Episode: 27, Reward: -94.95, avg loss: 0.000, eps: 0.968\n",
      "Episode: 28, Reward: -1212.79, avg loss: 0.000, eps: 0.966\n",
      "Episode: 29, Reward: -1203.7, avg loss: 0.000, eps: 0.965\n",
      "Episode: 30, Reward: -22.220000000000002, avg loss: 0.000, eps: 0.965\n",
      "Episode: 31, Reward: -234.25, avg loss: 0.000, eps: 0.963\n",
      "Episode: 32, Reward: -16.35, avg loss: 0.000, eps: 0.963\n",
      "Episode: 33, Reward: -1201.0, avg loss: 0.000, eps: 0.962\n",
      "Episode: 34, Reward: -1201.03, avg loss: 0.000, eps: 0.961\n",
      "Episode: 35, Reward: -90.02, avg loss: 0.000, eps: 0.960\n",
      "Episode: 36, Reward: -1201.0, avg loss: 0.000, eps: 0.960\n",
      "Episode: 37, Reward: -0.1, avg loss: 0.000, eps: 0.959\n",
      "Episode: 38, Reward: -1121.6, avg loss: 0.000, eps: 0.959\n",
      "Episode: 39, Reward: -1223.46, avg loss: 0.000, eps: 0.955\n",
      "Episode: 40, Reward: -28.1, avg loss: 0.000, eps: 0.954\n",
      "Episode: 41, Reward: -1201.0, avg loss: 11357711.000, eps: 0.954\n",
      "Episode: 42, Reward: -1203.14, avg loss: 14526591.438, eps: 0.952\n",
      "Episode: 43, Reward: -45.41, avg loss: 2434356.500, eps: 0.951\n",
      "Episode: 44, Reward: -821.52, avg loss: 9691112.500, eps: 0.950\n",
      "Episode: 45, Reward: -1204.26, avg loss: 7891543.031, eps: 0.948\n",
      "Episode: 46, Reward: -1221.37, avg loss: 1920445.969, eps: 0.947\n",
      "Episode: 47, Reward: -21.16, avg loss: 11390752.750, eps: 0.947\n",
      "Episode: 48, Reward: -1223.12, avg loss: 10334095.833, eps: 0.945\n",
      "Episode: 49, Reward: -113.71, avg loss: 7034599.250, eps: 0.944\n",
      "Episode: 50, Reward: -1201.0, avg loss: 2881874.750, eps: 0.944\n",
      "Episode: 51, Reward: -317.11, avg loss: 2568868.250, eps: 0.943\n",
      "Episode: 52, Reward: -1201.0, avg loss: 11783023.000, eps: 0.942\n",
      "Episode: 53, Reward: -1201.0, avg loss: 1930558.125, eps: 0.942\n",
      "Episode: 54, Reward: -50.79, avg loss: 12708165.000, eps: 0.941\n",
      "Episode: 55, Reward: -1201.0, avg loss: 1598932.500, eps: 0.941\n",
      "Episode: 56, Reward: -404.63, avg loss: 4285399.443, eps: 0.940\n",
      "Episode: 57, Reward: -1201.0, avg loss: 5179567.500, eps: 0.939\n",
      "Episode: 58, Reward: -1201.0, avg loss: 4177181.000, eps: 0.939\n",
      "Episode: 59, Reward: -1201.0, avg loss: 42524.812, eps: 0.938\n",
      "Episode: 60, Reward: -1203.35, avg loss: 1897762.906, eps: 0.937\n",
      "Episode: 61, Reward: -1201.0, avg loss: 164008.672, eps: 0.937\n",
      "Episode: 62, Reward: -1207.11, avg loss: 801012.626, eps: 0.935\n",
      "Episode: 63, Reward: -1201.0, avg loss: 6887327.000, eps: 0.935\n",
      "Episode: 64, Reward: -1203.32, avg loss: 1080727.491, eps: 0.934\n",
      "Episode: 65, Reward: -1.2, avg loss: 5994142.500, eps: 0.934\n",
      "Episode: 66, Reward: -1207.6, avg loss: 3674711.682, eps: 0.932\n",
      "Episode: 67, Reward: -1122.5900000000001, avg loss: 360406.156, eps: 0.931\n",
      "Episode: 68, Reward: -1203.11, avg loss: 3902075.625, eps: 0.930\n",
      "Episode: 69, Reward: -1612.87, avg loss: 64378366.797, eps: 0.928\n",
      "Episode: 70, Reward: -1201.0, avg loss: 180385248.000, eps: 0.928\n",
      "Episode: 71, Reward: -1205.27, avg loss: 749342.584, eps: 0.926\n",
      "Episode: 72, Reward: -1201.0, avg loss: 47031352.000, eps: 0.926\n",
      "Episode: 73, Reward: -1227.73, avg loss: 6181035.083, eps: 0.924\n",
      "Episode: 74, Reward: -1201.0, avg loss: 2615809.750, eps: 0.924\n",
      "Episode: 75, Reward: -130.94, avg loss: 174217252.750, eps: 0.923\n",
      "Episode: 76, Reward: -1201.0, avg loss: 24798462.000, eps: 0.923\n",
      "Episode: 77, Reward: -1201.0, avg loss: 219323120.000, eps: 0.922\n",
      "Episode: 78, Reward: -1201.0, avg loss: 4086271.000, eps: 0.922\n",
      "Episode: 79, Reward: -50.510000000000005, avg loss: 21517204.000, eps: 0.921\n",
      "Episode: 80, Reward: -204.88, avg loss: 1241559.000, eps: 0.920\n",
      "Episode: 81, Reward: -1221.01, avg loss: 24437247.750, eps: 0.919\n",
      "Episode: 82, Reward: -1201.0, avg loss: 2381304.750, eps: 0.919\n",
      "Episode: 83, Reward: -0.78, avg loss: 9396387.475, eps: 0.918\n",
      "Episode: 84, Reward: -133.67, avg loss: 1011460.000, eps: 0.917\n",
      "Episode: 85, Reward: -1203.07, avg loss: 11789595.059, eps: 0.916\n",
      "Episode: 86, Reward: -2.2699999999999996, avg loss: 55375696.250, eps: 0.915\n",
      "Episode: 87, Reward: -1201.0, avg loss: 3295635.000, eps: 0.915\n",
      "Episode: 88, Reward: -81.67, avg loss: 1261086.051, eps: 0.914\n",
      "Episode: 89, Reward: -1201.0, avg loss: 4841924.500, eps: 0.913\n",
      "Episode: 90, Reward: -1208.86, avg loss: 12459889.194, eps: 0.911\n",
      "Episode: 91, Reward: -1201.0, avg loss: 39701828.000, eps: 0.911\n",
      "Episode: 92, Reward: -1205.17, avg loss: 2873960.438, eps: 0.909\n",
      "Episode: 93, Reward: -347.3, avg loss: 5266197.750, eps: 0.908\n",
      "Episode: 94, Reward: 1021.33, avg loss: 3843887.354, eps: 0.906\n",
      "Episode: 95, Reward: -1201.0, avg loss: 3441381.500, eps: 0.906\n",
      "Episode: 96, Reward: -38.85, avg loss: 2066059.125, eps: 0.905\n",
      "Episode: 97, Reward: -1203.17, avg loss: 1738847.062, eps: 0.904\n",
      "Episode: 98, Reward: -1201.0, avg loss: 5000315.500, eps: 0.904\n",
      "Episode: 99, Reward: -0.28, avg loss: 1391159.250, eps: 0.904\n",
      "Episode: 100, Reward: -1201.0, avg loss: 1005832.250, eps: 0.903\n",
      "Episode: 101, Reward: -1201.0, avg loss: 143956.500, eps: 0.903\n",
      "Episode: 102, Reward: -4.4, avg loss: 2617625.250, eps: 0.902\n",
      "Episode: 103, Reward: -1201.0, avg loss: 2638670.750, eps: 0.902\n",
      "Episode: 104, Reward: -1203.32, avg loss: 29200977.771, eps: 0.900\n",
      "Episode: 105, Reward: -0.29, avg loss: 2747730.500, eps: 0.900\n",
      "Episode: 106, Reward: -1201.0, avg loss: 898590.922, eps: 0.899\n",
      "Episode: 107, Reward: -1201.0, avg loss: 378755.906, eps: 0.899\n",
      "Episode: 108, Reward: -1201.0, avg loss: 2488203.500, eps: 0.898\n",
      "Episode: 109, Reward: -1218.27, avg loss: 10688636.146, eps: 0.897\n",
      "Episode: 110, Reward: -1.0, avg loss: 1106486.375, eps: 0.896\n",
      "Episode: 111, Reward: -1201.0, avg loss: 41508468.000, eps: 0.896\n",
      "Episode: 112, Reward: -2.0, avg loss: 948773.305, eps: 0.895\n",
      "Episode: 113, Reward: -21.53, avg loss: 429121.656, eps: 0.895\n",
      "Episode: 114, Reward: -1201.04, avg loss: 1555048.156, eps: 0.894\n",
      "Episode: 115, Reward: -1203.24, avg loss: 16398399.000, eps: 0.893\n",
      "Episode: 116, Reward: -615.31, avg loss: 7923093.812, eps: 0.891\n",
      "Episode: 117, Reward: -1203.06, avg loss: 17379468.625, eps: 0.890\n",
      "Episode: 118, Reward: -9.06, avg loss: 26560590.000, eps: 0.890\n",
      "Episode: 119, Reward: -1203.12, avg loss: 3554884.666, eps: 0.889\n",
      "Episode: 120, Reward: -4.34, avg loss: 5331264.292, eps: 0.888\n",
      "Episode: 121, Reward: 2043.3200000000002, avg loss: 35895817.979, eps: 0.885\n",
      "Episode: 122, Reward: -1203.0, avg loss: 67387586.000, eps: 0.884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 123, Reward: -1201.02, avg loss: 15282736.428, eps: 0.883\n",
      "Episode: 124, Reward: -1205.74, avg loss: 35892838.448, eps: 0.881\n",
      "Episode: 125, Reward: -1201.27, avg loss: 63484346.000, eps: 0.881\n",
      "Episode: 126, Reward: -1201.0, avg loss: 1796049.875, eps: 0.880\n",
      "Episode: 127, Reward: -18.28, avg loss: 24735750.785, eps: 0.879\n",
      "Episode: 128, Reward: -22.62, avg loss: 31791262.081, eps: 0.877\n",
      "Episode: 129, Reward: -1203.12, avg loss: 3618970.688, eps: 0.876\n",
      "Episode: 130, Reward: -739.63, avg loss: 12000920.650, eps: 0.874\n",
      "Episode: 131, Reward: -1203.62, avg loss: 4399335.913, eps: 0.872\n",
      "Episode: 132, Reward: -1201.0, avg loss: 3710706.750, eps: 0.872\n",
      "Episode: 133, Reward: -53.73, avg loss: 15195315.000, eps: 0.872\n",
      "Episode: 134, Reward: -1201.0, avg loss: 1137301.750, eps: 0.871\n",
      "Episode: 135, Reward: -1201.0, avg loss: 1179187.750, eps: 0.871\n",
      "Episode: 136, Reward: -1201.05, avg loss: 28252127.000, eps: 0.870\n",
      "Episode: 137, Reward: -1201.0, avg loss: 25338198.000, eps: 0.869\n",
      "Episode: 138, Reward: -1201.06, avg loss: 7680688.203, eps: 0.868\n",
      "Episode: 139, Reward: -130.07999999999998, avg loss: 15537091.571, eps: 0.865\n",
      "Episode: 140, Reward: -1203.0, avg loss: 63908550.000, eps: 0.864\n",
      "Episode: 141, Reward: -166.45, avg loss: 43246856.000, eps: 0.864\n",
      "Episode: 142, Reward: -1203.07, avg loss: 22913294.000, eps: 0.863\n",
      "Episode: 143, Reward: -1151.4, avg loss: 5366282.500, eps: 0.863\n",
      "Episode: 144, Reward: -140.5, avg loss: 53259637.000, eps: 0.862\n",
      "Episode: 145, Reward: 1023.95, avg loss: 5188031.323, eps: 0.860\n",
      "Episode: 146, Reward: -1203.14, avg loss: 13816123.945, eps: 0.859\n",
      "Episode: 147, Reward: -1206.73, avg loss: 39649613.500, eps: 0.858\n",
      "Episode: 148, Reward: -1201.0, avg loss: 7211473.000, eps: 0.858\n",
      "Episode: 149, Reward: -1201.0, avg loss: 91624216.000, eps: 0.857\n",
      "Episode: 150, Reward: -1202.32, avg loss: 24111943.917, eps: 0.856\n",
      "Episode: 151, Reward: -160.32999999999998, avg loss: 28353359.531, eps: 0.854\n",
      "Episode: 152, Reward: -0.04, avg loss: 66353084.000, eps: 0.854\n",
      "Episode: 153, Reward: -510.04999999999995, avg loss: 31019159.000, eps: 0.853\n",
      "Episode: 154, Reward: -0.04, avg loss: 13127095.672, eps: 0.852\n",
      "Episode: 155, Reward: -1203.01, avg loss: 13440900.000, eps: 0.851\n",
      "Episode: 156, Reward: -1201.0, avg loss: 25066700.000, eps: 0.851\n",
      "Episode: 157, Reward: -838.06, avg loss: 134850248.000, eps: 0.850\n",
      "Episode: 158, Reward: -1203.0, avg loss: 26379907.000, eps: 0.849\n",
      "Episode: 159, Reward: -1201.74, avg loss: 106950661.667, eps: 0.846\n",
      "Episode: 160, Reward: -5.09, avg loss: 54741368.333, eps: 0.845\n",
      "Episode: 161, Reward: -8.41, avg loss: 2253674.091, eps: 0.844\n",
      "Episode: 162, Reward: -310.72, avg loss: 14694235.281, eps: 0.842\n",
      "Episode: 163, Reward: -1202.51, avg loss: 15792202.113, eps: 0.840\n",
      "Episode: 164, Reward: -1201.0, avg loss: 652383.875, eps: 0.840\n",
      "Episode: 165, Reward: -0.01, avg loss: 6756322.594, eps: 0.839\n",
      "Episode: 166, Reward: -2.7600000000000002, avg loss: 4051422.161, eps: 0.837\n",
      "Episode: 167, Reward: -1201.0, avg loss: 687842.250, eps: 0.836\n",
      "Episode: 168, Reward: -7.7, avg loss: 2531374.250, eps: 0.835\n",
      "Episode: 169, Reward: -327.97, avg loss: 18429984.000, eps: 0.835\n",
      "Episode: 170, Reward: -1203.31, avg loss: 2330146.430, eps: 0.834\n",
      "Episode: 171, Reward: -0.01, avg loss: 5636929.014, eps: 0.833\n",
      "Episode: 172, Reward: -552.92, avg loss: 15282380.000, eps: 0.833\n",
      "Episode: 173, Reward: -1201.65, avg loss: 961685.305, eps: 0.832\n",
      "Episode: 174, Reward: -1203.0, avg loss: 24767949.500, eps: 0.831\n",
      "Episode: 175, Reward: -1201.0, avg loss: 17881040.000, eps: 0.831\n",
      "Episode: 176, Reward: -1203.66, avg loss: 12699419.000, eps: 0.830\n",
      "Episode: 177, Reward: -1201.0, avg loss: 12347181.000, eps: 0.830\n",
      "Episode: 178, Reward: -730.46, avg loss: 303274.156, eps: 0.829\n",
      "Episode: 179, Reward: -28.949999999999996, avg loss: 4449982.147, eps: 0.827\n",
      "Episode: 180, Reward: -30.89, avg loss: 25530452.167, eps: 0.825\n",
      "Episode: 181, Reward: -1203.0, avg loss: 2534002.438, eps: 0.825\n",
      "Episode: 182, Reward: -1201.0, avg loss: 10104606.000, eps: 0.824\n",
      "Episode: 183, Reward: -351.59999999999997, avg loss: 907535.344, eps: 0.823\n",
      "Episode: 184, Reward: -92.53, avg loss: 14302755.938, eps: 0.823\n",
      "Episode: 185, Reward: -1.11, avg loss: 2646607.375, eps: 0.822\n",
      "Episode: 186, Reward: -27.060000000000002, avg loss: 4394917.328, eps: 0.821\n",
      "Episode: 187, Reward: -1201.0, avg loss: 12666614.000, eps: 0.821\n",
      "Episode: 188, Reward: -1203.63, avg loss: 13631793.789, eps: 0.819\n",
      "Episode: 189, Reward: -1205.88, avg loss: 10268474.050, eps: 0.817\n",
      "Episode: 190, Reward: -1203.01, avg loss: 2119089.219, eps: 0.816\n",
      "Episode: 191, Reward: -1201.0, avg loss: 942652.688, eps: 0.816\n",
      "Episode: 192, Reward: -81.2, avg loss: 6264963.250, eps: 0.815\n",
      "Episode: 193, Reward: -247.42, avg loss: 3752217.000, eps: 0.814\n",
      "Episode: 194, Reward: -1202.99, avg loss: 7711909.414, eps: 0.814\n",
      "Episode: 195, Reward: -1201.0, avg loss: 10753078.000, eps: 0.813\n",
      "Episode: 196, Reward: -1201.02, avg loss: 1954406.020, eps: 0.812\n",
      "Episode: 197, Reward: -166.71, avg loss: 16963196.117, eps: 0.812\n",
      "Episode: 198, Reward: -1201.0, avg loss: 8382348.000, eps: 0.811\n",
      "Episode: 199, Reward: -1201.0, avg loss: 2503953.750, eps: 0.811\n",
      "Episode: 200, Reward: -1202.01, avg loss: 15428099.548, eps: 0.810\n",
      "Episode: 201, Reward: -6.069999999999999, avg loss: 2828855.534, eps: 0.809\n",
      "Episode: 202, Reward: -2.4699999999999998, avg loss: 7422583.875, eps: 0.808\n",
      "Episode: 203, Reward: -5.029999999999999, avg loss: 6359063.785, eps: 0.806\n",
      "Episode: 204, Reward: -1201.0, avg loss: 6337226.000, eps: 0.806\n",
      "Episode: 205, Reward: -49.190000000000005, avg loss: 13737303.875, eps: 0.805\n",
      "Episode: 206, Reward: -1203.27, avg loss: 7915228.625, eps: 0.804\n",
      "Episode: 207, Reward: -1201.12, avg loss: 29610734.667, eps: 0.803\n",
      "Episode: 208, Reward: -1201.0, avg loss: 67903696.000, eps: 0.803\n",
      "Episode: 209, Reward: -84.41, avg loss: 9118355.648, eps: 0.802\n",
      "Episode: 210, Reward: -1205.1, avg loss: 12259241.917, eps: 0.801\n",
      "Episode: 211, Reward: -1204.62, avg loss: 15034882.300, eps: 0.799\n",
      "Episode: 212, Reward: -23.91, avg loss: 314469664.000, eps: 0.799\n",
      "Episode: 213, Reward: -1201.77, avg loss: 7019570.938, eps: 0.798\n",
      "Episode: 214, Reward: -1201.66, avg loss: 3195569.391, eps: 0.797\n",
      "Episode: 215, Reward: -1203.33, avg loss: 2718300.612, eps: 0.795\n",
      "Episode: 216, Reward: -0.13, avg loss: 5743700.750, eps: 0.794\n",
      "Episode: 217, Reward: -716.45, avg loss: 247665.969, eps: 0.794\n",
      "Episode: 218, Reward: -282.90999999999997, avg loss: 36575931.333, eps: 0.793\n",
      "Episode: 219, Reward: -405.90999999999997, avg loss: 13341198.386, eps: 0.790\n",
      "Episode: 220, Reward: -1201.0, avg loss: 7174623.500, eps: 0.790\n",
      "Episode: 221, Reward: -1201.0, avg loss: 7302065.500, eps: 0.789\n",
      "Episode: 222, Reward: -1221.45, avg loss: 1299670.941, eps: 0.788\n",
      "Episode: 223, Reward: -1203.0, avg loss: 2688570.438, eps: 0.788\n",
      "Episode: 224, Reward: -12.29, avg loss: 109200.734, eps: 0.787\n",
      "Episode: 225, Reward: -495.17, avg loss: 19216667.875, eps: 0.786\n",
      "Episode: 226, Reward: -1201.12, avg loss: 6482304.750, eps: 0.785\n",
      "Episode: 227, Reward: -1201.0, avg loss: 18549616.000, eps: 0.784\n",
      "Episode: 228, Reward: -1201.0, avg loss: 6115755.000, eps: 0.784\n",
      "Episode: 229, Reward: -1201.01, avg loss: 4679459.854, eps: 0.783\n",
      "Episode: 230, Reward: -1203.0, avg loss: 10857049.333, eps: 0.782\n",
      "Episode: 231, Reward: -1791.02, avg loss: 37278908.038, eps: 0.781\n",
      "Episode: 232, Reward: -83.13000000000001, avg loss: 19195760.587, eps: 0.778\n",
      "Episode: 233, Reward: -1201.0, avg loss: 881768.062, eps: 0.777\n",
      "Episode: 234, Reward: -1203.0, avg loss: 36366224.167, eps: 0.776\n",
      "Episode: 235, Reward: -20.67, avg loss: 53090404.667, eps: 0.775\n",
      "Episode: 236, Reward: -45.7, avg loss: 1812782.500, eps: 0.774\n",
      "Episode: 237, Reward: -1221.01, avg loss: 1378059.938, eps: 0.774\n",
      "Episode: 238, Reward: -1201.0, avg loss: 4043342.000, eps: 0.773\n",
      "Episode: 239, Reward: -1201.0, avg loss: 207323.359, eps: 0.773\n",
      "Episode: 240, Reward: -1205.12, avg loss: 8468923.812, eps: 0.771\n",
      "Episode: 241, Reward: -1201.0, avg loss: 4981657.250, eps: 0.770\n",
      "Episode: 242, Reward: -1203.0, avg loss: 2751010.417, eps: 0.769\n",
      "Episode: 243, Reward: -1201.0, avg loss: 833651.750, eps: 0.768\n",
      "Episode: 244, Reward: -1201.1, avg loss: 105697736.000, eps: 0.768\n",
      "Episode: 245, Reward: -1201.37, avg loss: 106676897.383, eps: 0.766\n",
      "Episode: 246, Reward: -1201.0, avg loss: 1811723.125, eps: 0.766\n",
      "Episode: 247, Reward: -1201.0, avg loss: 1373808.625, eps: 0.765\n",
      "Episode: 248, Reward: -8.58, avg loss: 1181980.000, eps: 0.765\n",
      "Episode: 249, Reward: -1203.05, avg loss: 8142214.460, eps: 0.763\n",
      "Episode: 250, Reward: -921.54, avg loss: 19492549.177, eps: 0.762\n",
      "Episode: 251, Reward: -1223.13, avg loss: 2354502.190, eps: 0.761\n",
      "Episode: 252, Reward: -1214.55, avg loss: 49577709.958, eps: 0.760\n",
      "Episode: 253, Reward: -1221.07, avg loss: 52218659.625, eps: 0.759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 254, Reward: -1212.85, avg loss: 9499249.447, eps: 0.756\n",
      "Episode: 255, Reward: -784.52, avg loss: 2354206.719, eps: 0.756\n",
      "Episode: 256, Reward: -7.39, avg loss: 3860271.173, eps: 0.755\n",
      "Episode: 257, Reward: -457.53, avg loss: 198426.281, eps: 0.754\n",
      "Episode: 258, Reward: -1209.28, avg loss: 1984254.525, eps: 0.752\n",
      "Episode: 259, Reward: -1203.0, avg loss: 2100561.062, eps: 0.751\n",
      "Episode: 260, Reward: -3.19, avg loss: 373277.703, eps: 0.750\n",
      "Episode: 261, Reward: -70.42, avg loss: 2256490.573, eps: 0.749\n",
      "Episode: 262, Reward: -319.71, avg loss: 5344283.000, eps: 0.748\n",
      "Episode: 263, Reward: -1201.0, avg loss: 89097136.000, eps: 0.748\n",
      "Episode: 264, Reward: -1201.0, avg loss: 298454.625, eps: 0.747\n",
      "Episode: 265, Reward: -1203.01, avg loss: 20679555.906, eps: 0.747\n",
      "Episode: 266, Reward: -1203.08, avg loss: 11389942.000, eps: 0.745\n",
      "Episode: 267, Reward: -4.58, avg loss: 2097582.548, eps: 0.744\n",
      "Episode: 268, Reward: -1201.0, avg loss: 1659439.125, eps: 0.744\n",
      "Episode: 269, Reward: -8.32, avg loss: 7852483.562, eps: 0.742\n",
      "Episode: 270, Reward: -22.2, avg loss: 480888.875, eps: 0.742\n",
      "Episode: 271, Reward: -1201.74, avg loss: 1906587.957, eps: 0.740\n",
      "Episode: 272, Reward: -550.48, avg loss: 2142666.500, eps: 0.740\n",
      "Episode: 273, Reward: -730.5, avg loss: 1763950.258, eps: 0.739\n",
      "Episode: 274, Reward: -1201.07, avg loss: 262519.703, eps: 0.738\n",
      "Episode: 275, Reward: -1201.0, avg loss: 199154.094, eps: 0.738\n",
      "Episode: 276, Reward: -1203.0, avg loss: 2251033.826, eps: 0.736\n",
      "Episode: 277, Reward: -1202.23, avg loss: 6430028.805, eps: 0.735\n",
      "Episode: 278, Reward: -70.02, avg loss: 1589797.926, eps: 0.734\n",
      "Episode: 279, Reward: -1203.31, avg loss: 2165388.406, eps: 0.733\n",
      "Episode: 280, Reward: -1201.0, avg loss: 1653681.875, eps: 0.732\n",
      "Episode: 281, Reward: -969.57, avg loss: 4469115.833, eps: 0.731\n",
      "Episode: 282, Reward: -1203.0, avg loss: 171907.898, eps: 0.731\n",
      "Episode: 283, Reward: -763.01, avg loss: 830641.949, eps: 0.729\n",
      "Episode: 284, Reward: -0.04, avg loss: 393005.375, eps: 0.729\n",
      "Episode: 285, Reward: -1205.77, avg loss: 1449036.027, eps: 0.727\n",
      "Episode: 286, Reward: -0.11, avg loss: 1049913.875, eps: 0.727\n",
      "Episode: 287, Reward: -1201.0, avg loss: 1038300.062, eps: 0.727\n",
      "Episode: 288, Reward: -1227.78, avg loss: 6336428.903, eps: 0.725\n",
      "Episode: 289, Reward: -4.92, avg loss: 244525.000, eps: 0.725\n",
      "Episode: 290, Reward: -6.12, avg loss: 455278.247, eps: 0.724\n",
      "Episode: 291, Reward: -1201.0, avg loss: 19459.918, eps: 0.723\n",
      "Episode: 292, Reward: -1226.84, avg loss: 1604996.141, eps: 0.720\n",
      "Episode: 293, Reward: -2.05, avg loss: 1395930.573, eps: 0.719\n",
      "Episode: 294, Reward: -0.23, avg loss: 171023.391, eps: 0.719\n",
      "Episode: 295, Reward: -1201.0, avg loss: 7012.475, eps: 0.719\n",
      "Episode: 296, Reward: -1160.7, avg loss: 3305357.750, eps: 0.718\n",
      "Episode: 297, Reward: -59.95, avg loss: 2371224.055, eps: 0.717\n",
      "Episode: 298, Reward: -1201.0, avg loss: 765596.089, eps: 0.716\n",
      "Episode: 299, Reward: -3.57, avg loss: 429102.674, eps: 0.715\n",
      "Episode: 300, Reward: -1203.02, avg loss: 581106.366, eps: 0.714\n",
      "Episode: 301, Reward: -1201.0, avg loss: 364757.031, eps: 0.714\n",
      "Episode: 302, Reward: -844.94, avg loss: 189649.281, eps: 0.713\n",
      "Episode: 303, Reward: -1201.0, avg loss: 258735.922, eps: 0.713\n",
      "Episode: 304, Reward: -1.65, avg loss: 617284.938, eps: 0.713\n",
      "Episode: 305, Reward: -0.69, avg loss: 9225.599, eps: 0.712\n",
      "Episode: 306, Reward: -1203.04, avg loss: 87326.789, eps: 0.711\n",
      "Episode: 307, Reward: -1201.0, avg loss: 2044891.750, eps: 0.711\n",
      "Episode: 308, Reward: -1203.07, avg loss: 221093.966, eps: 0.710\n",
      "Episode: 309, Reward: 335.0, avg loss: 56653.516, eps: 0.708\n",
      "Episode: 310, Reward: -77.39, avg loss: 10185.329, eps: 0.708\n",
      "Episode: 311, Reward: -602.91, avg loss: 791266.447, eps: 0.707\n",
      "Episode: 312, Reward: -142.6, avg loss: 1113452.721, eps: 0.707\n",
      "Episode: 313, Reward: -29.380000000000003, avg loss: 653390.891, eps: 0.706\n",
      "Episode: 314, Reward: -67.26, avg loss: 26988.861, eps: 0.706\n",
      "Episode: 315, Reward: -22.3, avg loss: 2499689.271, eps: 0.705\n",
      "Episode: 316, Reward: -1228.36, avg loss: 441039.402, eps: 0.703\n",
      "Episode: 317, Reward: -271.64, avg loss: 842469.375, eps: 0.702\n",
      "Episode: 318, Reward: -240.89, avg loss: 10769.227, eps: 0.702\n",
      "Episode: 319, Reward: -1205.88, avg loss: 28066.197, eps: 0.701\n",
      "Episode: 320, Reward: -1.09, avg loss: 80194.570, eps: 0.700\n",
      "Episode: 321, Reward: -35.900000000000006, avg loss: 967486.425, eps: 0.700\n",
      "Episode: 322, Reward: -416.13, avg loss: 203847.234, eps: 0.699\n",
      "Episode: 323, Reward: -3.78, avg loss: 985320.266, eps: 0.699\n",
      "Episode: 324, Reward: -1201.0, avg loss: 8426.117, eps: 0.698\n",
      "Episode: 325, Reward: -128.02, avg loss: 1133083.148, eps: 0.698\n",
      "Episode: 326, Reward: -43.82, avg loss: 68705.609, eps: 0.697\n",
      "Episode: 327, Reward: -12.36, avg loss: 185458.560, eps: 0.696\n",
      "Episode: 328, Reward: -149.51, avg loss: 49587.695, eps: 0.695\n",
      "Episode: 329, Reward: -2.1699999999999995, avg loss: 284655.391, eps: 0.693\n",
      "Episode: 330, Reward: -0.19, avg loss: 30406110.000, eps: 0.693\n",
      "Episode: 331, Reward: -1203.46, avg loss: 10968429.323, eps: 0.692\n",
      "Episode: 332, Reward: -3.21, avg loss: 268404.125, eps: 0.691\n",
      "Episode: 333, Reward: -111.0, avg loss: 5386251.803, eps: 0.690\n",
      "Episode: 334, Reward: -317.26, avg loss: 410182.494, eps: 0.689\n",
      "Episode: 335, Reward: -1203.03, avg loss: 207909.944, eps: 0.688\n",
      "Episode: 336, Reward: -1201.03, avg loss: 1706349.640, eps: 0.687\n",
      "Episode: 337, Reward: -141.63, avg loss: 4918747.928, eps: 0.685\n",
      "Episode: 338, Reward: -1201.0, avg loss: 172251.109, eps: 0.685\n",
      "Episode: 339, Reward: -0.45, avg loss: 533325.000, eps: 0.684\n",
      "Episode: 340, Reward: -1205.04, avg loss: 74891.479, eps: 0.683\n",
      "Episode: 341, Reward: -11.93, avg loss: 160387.996, eps: 0.682\n",
      "Episode: 342, Reward: -1234.6, avg loss: 4698618.487, eps: 0.680\n",
      "Episode: 343, Reward: -0.92, avg loss: 247839.047, eps: 0.680\n",
      "Episode: 344, Reward: -147.78, avg loss: 85826.803, eps: 0.679\n",
      "Episode: 345, Reward: -56.34, avg loss: 1511552.250, eps: 0.679\n",
      "Episode: 346, Reward: 0.0, avg loss: 1501412.500, eps: 0.679\n",
      "Episode: 347, Reward: -1270.36, avg loss: 1417319.839, eps: 0.676\n",
      "Episode: 348, Reward: -683.82, avg loss: 10627.668, eps: 0.676\n",
      "Episode: 349, Reward: -212.17, avg loss: 472281.883, eps: 0.675\n",
      "Episode: 350, Reward: -81.02, avg loss: 1489364.750, eps: 0.675\n",
      "Episode: 351, Reward: -1201.98, avg loss: 20498.219, eps: 0.674\n",
      "Episode: 352, Reward: -1203.32, avg loss: 339246.586, eps: 0.673\n",
      "Episode: 353, Reward: -1205.01, avg loss: 497688.938, eps: 0.672\n",
      "Episode: 354, Reward: -1201.95, avg loss: 2454282.207, eps: 0.671\n",
      "Episode: 355, Reward: -1201.0, avg loss: 179022.156, eps: 0.671\n",
      "Episode: 356, Reward: -1205.66, avg loss: 3895601.750, eps: 0.669\n",
      "Episode: 357, Reward: -4.34, avg loss: 13315.994, eps: 0.668\n",
      "Episode: 358, Reward: -1203.19, avg loss: 148489.337, eps: 0.667\n",
      "Episode: 359, Reward: -12.849999999999998, avg loss: 976256.034, eps: 0.665\n",
      "Episode: 360, Reward: -17.27, avg loss: 667276.112, eps: 0.663\n",
      "Episode: 361, Reward: -49.74, avg loss: 783918.904, eps: 0.662\n",
      "Episode: 362, Reward: -1077.21, avg loss: 765884.445, eps: 0.661\n",
      "Episode: 363, Reward: -889.4, avg loss: 512538.314, eps: 0.660\n",
      "Episode: 364, Reward: -1201.0, avg loss: 59522.438, eps: 0.660\n",
      "Episode: 365, Reward: -0.99, avg loss: 3403593.500, eps: 0.659\n",
      "Episode: 366, Reward: -1201.0, avg loss: 2611864.750, eps: 0.659\n",
      "Episode: 367, Reward: -2.2199999999999998, avg loss: 1027346.276, eps: 0.658\n",
      "Episode: 368, Reward: -1702.13, avg loss: 92924.329, eps: 0.657\n",
      "Episode: 369, Reward: -1201.0, avg loss: 340093.406, eps: 0.657\n",
      "Episode: 370, Reward: -1203.07, avg loss: 3839030.875, eps: 0.656\n",
      "Episode: 371, Reward: -1201.0, avg loss: 5874069.500, eps: 0.656\n",
      "Episode: 372, Reward: -13.94, avg loss: 683922.594, eps: 0.655\n",
      "Episode: 373, Reward: -1201.0, avg loss: 20128.855, eps: 0.655\n",
      "Episode: 374, Reward: -4.4, avg loss: 205070998.727, eps: 0.653\n",
      "Episode: 375, Reward: -0.74, avg loss: 1385817.875, eps: 0.653\n",
      "Episode: 376, Reward: -2.96, avg loss: 325955.250, eps: 0.652\n",
      "Episode: 377, Reward: -161.69, avg loss: 2286731.262, eps: 0.651\n",
      "Episode: 378, Reward: -146.3, avg loss: 5899749.406, eps: 0.650\n",
      "Episode: 379, Reward: -1201.68, avg loss: 3618798.785, eps: 0.649\n",
      "Episode: 380, Reward: -5.9, avg loss: 698260.221, eps: 0.648\n",
      "Episode: 381, Reward: -24.799999999999997, avg loss: 7079642.664, eps: 0.647\n",
      "Episode: 382, Reward: -1210.32, avg loss: 55474040.977, eps: 0.643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 383, Reward: -1203.88, avg loss: 128990.312, eps: 0.643\n",
      "Episode: 384, Reward: -1209.58, avg loss: 64389301.973, eps: 0.642\n",
      "Episode: 385, Reward: -1203.66, avg loss: 31718.506, eps: 0.641\n",
      "Episode: 386, Reward: -32.37, avg loss: 1970990.113, eps: 0.640\n",
      "Episode: 387, Reward: -1201.0, avg loss: 845344.188, eps: 0.640\n",
      "Episode: 388, Reward: -1207.35, avg loss: 6849851.766, eps: 0.638\n",
      "Episode: 389, Reward: -1134.4, avg loss: 13352889.125, eps: 0.637\n",
      "Episode: 390, Reward: -1207.37, avg loss: 4359497.438, eps: 0.636\n",
      "Episode: 391, Reward: -1201.0, avg loss: 17903902.000, eps: 0.635\n",
      "Episode: 392, Reward: -1201.0, avg loss: 25079.934, eps: 0.635\n",
      "Episode: 393, Reward: -1209.11, avg loss: 212761183.150, eps: 0.632\n",
      "Episode: 394, Reward: -56.93, avg loss: 12188419.000, eps: 0.632\n",
      "Episode: 395, Reward: -1204.47, avg loss: 8871898.250, eps: 0.632\n",
      "Episode: 396, Reward: -1201.0, avg loss: 6700988.500, eps: 0.631\n",
      "Episode: 397, Reward: -1225.23, avg loss: 55525450.250, eps: 0.631\n",
      "Episode: 398, Reward: -1204.73, avg loss: 73979944.271, eps: 0.630\n",
      "Episode: 399, Reward: -1208.07, avg loss: 54760502.006, eps: 0.627\n",
      "Episode: 400, Reward: -1201.06, avg loss: 12137984.250, eps: 0.626\n",
      "Episode: 401, Reward: -195.86, avg loss: 3122193.531, eps: 0.624\n",
      "Episode: 402, Reward: -0.17, avg loss: 55396284.000, eps: 0.624\n",
      "Episode: 403, Reward: -1203.17, avg loss: 32842906.290, eps: 0.622\n",
      "Episode: 404, Reward: -1201.41, avg loss: 37908.098, eps: 0.622\n",
      "Episode: 405, Reward: -457.23, avg loss: 28725902.500, eps: 0.621\n",
      "Episode: 406, Reward: -1227.18, avg loss: 3023492.451, eps: 0.619\n",
      "Episode: 407, Reward: -1201.0, avg loss: 474643.938, eps: 0.619\n",
      "Episode: 408, Reward: -1201.0, avg loss: 51189848.000, eps: 0.619\n",
      "Episode: 409, Reward: -51.0, avg loss: 2869025.375, eps: 0.617\n",
      "Episode: 410, Reward: -416.75, avg loss: 1936028.667, eps: 0.617\n",
      "Episode: 411, Reward: -1152.6799999999998, avg loss: 1230340.500, eps: 0.616\n",
      "Episode: 412, Reward: -122.44, avg loss: 9013949.094, eps: 0.615\n",
      "Episode: 413, Reward: -1.15, avg loss: 9591781.000, eps: 0.615\n",
      "Episode: 414, Reward: -20.480000000000004, avg loss: 128501031.768, eps: 0.614\n",
      "Episode: 415, Reward: -1201.0, avg loss: 1021311.000, eps: 0.613\n",
      "Episode: 416, Reward: -1201.04, avg loss: 76424925.500, eps: 0.613\n",
      "Episode: 417, Reward: -977.04, avg loss: 626239.750, eps: 0.613\n",
      "Episode: 418, Reward: -947.21, avg loss: 8553370.656, eps: 0.612\n",
      "Episode: 419, Reward: -3.1, avg loss: 2517925.031, eps: 0.611\n",
      "Episode: 420, Reward: -1201.0, avg loss: 121365.547, eps: 0.611\n",
      "Episode: 421, Reward: -868.4499999999999, avg loss: 310524.484, eps: 0.610\n",
      "Episode: 422, Reward: -68.53999999999999, avg loss: 9393319.555, eps: 0.608\n",
      "Episode: 423, Reward: -5.6899999999999995, avg loss: 24536457.047, eps: 0.607\n",
      "Episode: 424, Reward: -1201.23, avg loss: 7201884.973, eps: 0.606\n",
      "Episode: 425, Reward: -1201.92, avg loss: 108925380.531, eps: 0.605\n",
      "Episode: 426, Reward: -1224.12, avg loss: 22710839.364, eps: 0.603\n",
      "Episode: 427, Reward: -2.08, avg loss: 17939417.000, eps: 0.603\n",
      "Episode: 428, Reward: -296.84, avg loss: 358858038.000, eps: 0.602\n",
      "Episode: 429, Reward: -1170.69, avg loss: 9958197.396, eps: 0.601\n",
      "Episode: 430, Reward: -1225.58, avg loss: 4032980.255, eps: 0.599\n",
      "Episode: 431, Reward: -1226.02, avg loss: 2568900.927, eps: 0.598\n",
      "Episode: 432, Reward: -1201.0, avg loss: 203181.234, eps: 0.598\n",
      "Episode: 433, Reward: -24.490000000000002, avg loss: 22650378.809, eps: 0.595\n",
      "Episode: 434, Reward: -125.91, avg loss: 32087423.000, eps: 0.595\n",
      "Episode: 435, Reward: -36.96, avg loss: 11674606.000, eps: 0.594\n",
      "Episode: 436, Reward: -1201.0, avg loss: 138305280.000, eps: 0.594\n",
      "Episode: 437, Reward: -1201.0, avg loss: 7028141.500, eps: 0.594\n",
      "Episode: 438, Reward: -1201.0, avg loss: 4715032.000, eps: 0.594\n",
      "Episode: 439, Reward: -381.05, avg loss: 18914338.000, eps: 0.593\n",
      "Episode: 440, Reward: -63.07, avg loss: 141346491.955, eps: 0.592\n",
      "Episode: 441, Reward: -77.50999999999999, avg loss: 24856381.550, eps: 0.590\n",
      "Episode: 442, Reward: -1203.0, avg loss: 32200680.676, eps: 0.589\n",
      "Episode: 443, Reward: -1205.06, avg loss: 1269718.017, eps: 0.587\n",
      "Episode: 444, Reward: -250.66, avg loss: 7293031.000, eps: 0.587\n",
      "Episode: 445, Reward: -2.8299999999999996, avg loss: 15903811.000, eps: 0.586\n",
      "Episode: 446, Reward: -33.94, avg loss: 2624003.224, eps: 0.585\n",
      "Episode: 447, Reward: -1203.02, avg loss: 2499101.663, eps: 0.584\n",
      "Episode: 448, Reward: -1203.43, avg loss: 22928215.688, eps: 0.583\n",
      "Episode: 449, Reward: -28.299999999999997, avg loss: 22540075.000, eps: 0.583\n",
      "Episode: 450, Reward: -1221.18, avg loss: 28840096.917, eps: 0.582\n",
      "Episode: 451, Reward: -6.5, avg loss: 156958800.000, eps: 0.581\n",
      "Episode: 452, Reward: -590.43, avg loss: 335642.031, eps: 0.581\n",
      "Episode: 453, Reward: -1205.12, avg loss: 5782664.469, eps: 0.580\n",
      "Episode: 454, Reward: -1208.62, avg loss: 1730547.882, eps: 0.579\n",
      "Episode: 455, Reward: -22.34, avg loss: 19945455.667, eps: 0.578\n",
      "Episode: 456, Reward: -1201.0, avg loss: 606791232.000, eps: 0.577\n",
      "Episode: 457, Reward: -2.4, avg loss: 4480546.807, eps: 0.577\n",
      "Episode: 458, Reward: -1201.0, avg loss: 1300807.375, eps: 0.576\n",
      "Episode: 459, Reward: -752.5999999999999, avg loss: 5841334.586, eps: 0.576\n",
      "Episode: 460, Reward: -3.37, avg loss: 4119779.938, eps: 0.575\n",
      "Episode: 461, Reward: -1204.11, avg loss: 3920385.842, eps: 0.574\n",
      "Episode: 462, Reward: -659.37, avg loss: 1029394.633, eps: 0.573\n",
      "Episode: 463, Reward: -1223.31, avg loss: 60380189.539, eps: 0.572\n",
      "Episode: 464, Reward: -27.120000000000005, avg loss: 3552419.548, eps: 0.570\n",
      "Episode: 465, Reward: -1223.13, avg loss: 1155943.423, eps: 0.569\n",
      "Episode: 466, Reward: -1203.38, avg loss: 2272083.854, eps: 0.568\n",
      "Episode: 467, Reward: -2.95, avg loss: 3639685.250, eps: 0.568\n",
      "Episode: 468, Reward: -1213.25, avg loss: 39675756.688, eps: 0.566\n",
      "Episode: 469, Reward: -1209.84, avg loss: 27754974.886, eps: 0.563\n",
      "Episode: 470, Reward: -1205.21, avg loss: 5248332.021, eps: 0.562\n",
      "Episode: 471, Reward: -1201.0, avg loss: 127208712.000, eps: 0.562\n",
      "Episode: 472, Reward: -869.12, avg loss: 43797247.250, eps: 0.561\n",
      "Episode: 473, Reward: -2.13, avg loss: 12608314.000, eps: 0.560\n",
      "Episode: 474, Reward: -1203.21, avg loss: 59309030.000, eps: 0.560\n",
      "Episode: 475, Reward: -1.25, avg loss: 4899578.787, eps: 0.559\n",
      "Episode: 476, Reward: -212.63, avg loss: 24015025.000, eps: 0.559\n",
      "Episode: 477, Reward: -1.39, avg loss: 7139656.695, eps: 0.558\n",
      "Episode: 478, Reward: -1201.0, avg loss: 68603160.000, eps: 0.557\n",
      "Episode: 479, Reward: -602.38, avg loss: 115001328.000, eps: 0.557\n",
      "Episode: 480, Reward: -1201.0, avg loss: 648663.000, eps: 0.557\n",
      "Episode: 481, Reward: -100.27, avg loss: 13165758.000, eps: 0.557\n",
      "Episode: 482, Reward: -1223.37, avg loss: 22809775.927, eps: 0.555\n",
      "Episode: 483, Reward: -1201.0, avg loss: 10991247.000, eps: 0.555\n",
      "Episode: 484, Reward: -5.44, avg loss: 116741.781, eps: 0.554\n",
      "Episode: 485, Reward: -37.209999999999994, avg loss: 1970433.475, eps: 0.551\n",
      "Episode: 486, Reward: -127.57, avg loss: 7747675.242, eps: 0.549\n",
      "Episode: 487, Reward: -1221.16, avg loss: 9377434.583, eps: 0.548\n",
      "Episode: 488, Reward: -1201.05, avg loss: 1183042.531, eps: 0.548\n",
      "Episode: 489, Reward: -1215.09, avg loss: 9275406.431, eps: 0.546\n",
      "Episode: 490, Reward: -7.91, avg loss: 463215.604, eps: 0.545\n",
      "Episode: 491, Reward: -763.77, avg loss: 20413510.000, eps: 0.544\n",
      "Episode: 492, Reward: -10.96, avg loss: 13197186.000, eps: 0.544\n",
      "Episode: 493, Reward: -1201.0, avg loss: 203434.672, eps: 0.544\n",
      "Episode: 494, Reward: -1205.85, avg loss: 1085165.125, eps: 0.543\n",
      "Episode: 495, Reward: -1203.12, avg loss: 2138736.125, eps: 0.543\n",
      "Episode: 496, Reward: -110.27000000000001, avg loss: 980423.688, eps: 0.542\n",
      "Episode: 497, Reward: -1201.0, avg loss: 409590.094, eps: 0.542\n",
      "Episode: 498, Reward: -1203.08, avg loss: 2108716.766, eps: 0.541\n",
      "Episode: 499, Reward: -177.0, avg loss: 129567330.208, eps: 0.540\n",
      "Episode: 500, Reward: -255.58, avg loss: 52602268.000, eps: 0.540\n",
      "Episode: 501, Reward: -1201.0, avg loss: 41588.414, eps: 0.540\n",
      "Episode: 502, Reward: -1203.83, avg loss: 60126798.891, eps: 0.539\n",
      "Episode: 503, Reward: -0.28, avg loss: 41676524.667, eps: 0.538\n",
      "Episode: 504, Reward: -1205.24, avg loss: 22246390.051, eps: 0.537\n",
      "Episode: 505, Reward: -5.569999999999999, avg loss: 9742994.697, eps: 0.535\n",
      "Episode: 506, Reward: -1203.14, avg loss: 3971984.625, eps: 0.535\n",
      "Episode: 507, Reward: -482.43, avg loss: 1776169.500, eps: 0.534\n",
      "Episode: 508, Reward: -224.4, avg loss: 2042007.500, eps: 0.534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 509, Reward: -1212.07, avg loss: 19058488.408, eps: 0.532\n",
      "Episode: 510, Reward: -1203.02, avg loss: 18087772.000, eps: 0.532\n",
      "Episode: 511, Reward: -1203.06, avg loss: 1182457.531, eps: 0.531\n",
      "Episode: 512, Reward: -1225.1, avg loss: 8494050.650, eps: 0.530\n",
      "Episode: 513, Reward: -592.12, avg loss: 61321884.250, eps: 0.529\n",
      "Episode: 514, Reward: -29.48, avg loss: 14261851.375, eps: 0.528\n",
      "Episode: 515, Reward: -88.74000000000001, avg loss: 2747715.100, eps: 0.526\n",
      "Episode: 516, Reward: -1201.0, avg loss: 6182827.000, eps: 0.526\n",
      "Episode: 517, Reward: 4607.610000000001, avg loss: 6531176.037, eps: 0.523\n",
      "Episode: 518, Reward: -136.31, avg loss: 1675941.898, eps: 0.522\n",
      "Episode: 519, Reward: -1203.11, avg loss: 12412695.109, eps: 0.521\n",
      "Episode: 520, Reward: -2186.9399999999996, avg loss: 73533173.057, eps: 0.519\n",
      "Episode: 521, Reward: -1203.18, avg loss: 23256630.000, eps: 0.518\n",
      "Episode: 522, Reward: -231.23, avg loss: 36372368.375, eps: 0.517\n",
      "Episode: 523, Reward: -20.44, avg loss: 114226075.167, eps: 0.516\n",
      "Episode: 524, Reward: -438.06, avg loss: 767949045.200, eps: 0.515\n",
      "Episode: 525, Reward: -1205.28, avg loss: 75755529.625, eps: 0.514\n",
      "Episode: 526, Reward: -1201.0, avg loss: 153686208.000, eps: 0.514\n",
      "Episode: 527, Reward: -1.2, avg loss: 45038584.000, eps: 0.514\n",
      "Episode: 528, Reward: -26.820000000000007, avg loss: 14348830353.969, eps: 0.511\n",
      "Episode: 529, Reward: -223.3, avg loss: 984468878.875, eps: 0.510\n",
      "Episode: 530, Reward: -1227.76, avg loss: 11420186540.071, eps: 0.508\n",
      "Episode: 531, Reward: -481.21, avg loss: 10606641989.281, eps: 0.506\n",
      "Episode: 532, Reward: -1205.98, avg loss: 51938766.821, eps: 0.504\n",
      "Episode: 533, Reward: -1225.42, avg loss: 475949853.000, eps: 0.503\n",
      "Episode: 534, Reward: -1201.0, avg loss: 1068800064.000, eps: 0.503\n",
      "Episode: 535, Reward: -42.980000000000004, avg loss: 5125114814.500, eps: 0.501\n",
      "Episode: 536, Reward: -1201.0, avg loss: 476051776.000, eps: 0.501\n",
      "Episode: 537, Reward: -94.46000000000001, avg loss: 3847251327.833, eps: 0.499\n",
      "Episode: 538, Reward: -35.79, avg loss: 1606853952.000, eps: 0.499\n",
      "Episode: 539, Reward: -1201.0, avg loss: 551568384.000, eps: 0.498\n",
      "Episode: 540, Reward: -1209.1, avg loss: 1270108961.200, eps: 0.497\n",
      "Episode: 541, Reward: -1207.41, avg loss: 26879644130.583, eps: 0.496\n",
      "Episode: 542, Reward: -1217.37, avg loss: 6283807510.136, eps: 0.493\n",
      "Episode: 543, Reward: -1201.0, avg loss: 49583828.000, eps: 0.493\n",
      "Episode: 544, Reward: -248.04999999999998, avg loss: 59647314.000, eps: 0.492\n",
      "Episode: 545, Reward: -1201.0, avg loss: 95633145856.000, eps: 0.492\n",
      "Episode: 546, Reward: -1221.05, avg loss: 96427695776.000, eps: 0.492\n",
      "Episode: 547, Reward: -95.08, avg loss: 235514049.583, eps: 0.491\n",
      "Episode: 548, Reward: -1233.61, avg loss: 1305424930.143, eps: 0.489\n",
      "Episode: 549, Reward: -1235.47, avg loss: 15031125894.364, eps: 0.487\n",
      "Episode: 550, Reward: -1091.91, avg loss: 149937988.000, eps: 0.486\n",
      "Episode: 551, Reward: -35.230000000000004, avg loss: 1228613096.000, eps: 0.484\n",
      "Episode: 552, Reward: -1201.0, avg loss: 30939179008.000, eps: 0.484\n",
      "Episode: 553, Reward: -1203.13, avg loss: 39079658.458, eps: 0.483\n",
      "Episode: 554, Reward: -1201.0, avg loss: 699113.938, eps: 0.483\n",
      "Episode: 555, Reward: -1201.0, avg loss: 2329927.750, eps: 0.483\n",
      "Episode: 556, Reward: -445.40000000000003, avg loss: 107529132.000, eps: 0.482\n",
      "Episode: 557, Reward: -21.57, avg loss: 6405195146.667, eps: 0.482\n",
      "Episode: 558, Reward: -15.39, avg loss: 663231091.100, eps: 0.480\n",
      "Episode: 559, Reward: -203.32, avg loss: 24878260754.667, eps: 0.480\n",
      "Episode: 560, Reward: -1205.09, avg loss: 567030376.000, eps: 0.479\n",
      "Episode: 561, Reward: -0.03, avg loss: 5111868.000, eps: 0.479\n",
      "Episode: 562, Reward: -1209.85, avg loss: 17250480712.000, eps: 0.477\n",
      "Episode: 563, Reward: -1210.0, avg loss: 953261088.909, eps: 0.475\n",
      "Episode: 564, Reward: -1229.67, avg loss: 14538040882.591, eps: 0.472\n",
      "Episode: 565, Reward: -5.43, avg loss: 128976686470.000, eps: 0.472\n",
      "Episode: 566, Reward: -1201.0, avg loss: 426640896.000, eps: 0.471\n",
      "Episode: 567, Reward: -1220.55, avg loss: 19024884518.000, eps: 0.469\n",
      "Episode: 568, Reward: -1444.76, avg loss: 47716601.833, eps: 0.469\n",
      "Episode: 569, Reward: -1203.0, avg loss: 494078228.000, eps: 0.468\n",
      "Episode: 570, Reward: -1231.22, avg loss: 6649462005.636, eps: 0.466\n",
      "Episode: 571, Reward: -1204.23, avg loss: 2315502270.333, eps: 0.464\n",
      "Episode: 572, Reward: -1201.05, avg loss: 130678340.500, eps: 0.464\n",
      "Episode: 573, Reward: -282.81, avg loss: 201644000.000, eps: 0.464\n",
      "Episode: 574, Reward: -201.36, avg loss: 505145630.400, eps: 0.462\n",
      "Episode: 575, Reward: -2.02, avg loss: 1532179328.000, eps: 0.462\n",
      "Episode: 576, Reward: -1201.0, avg loss: 246129088.000, eps: 0.462\n",
      "Episode: 577, Reward: -1201.0, avg loss: 2194585600.000, eps: 0.462\n",
      "Episode: 578, Reward: -24.98, avg loss: 12879391.000, eps: 0.461\n",
      "Episode: 579, Reward: -299.17, avg loss: 11321236811.638, eps: 0.459\n",
      "Episode: 580, Reward: -1201.11, avg loss: 522732392.000, eps: 0.458\n",
      "Episode: 581, Reward: -27.650000000000002, avg loss: 7772977277.352, eps: 0.456\n",
      "Episode: 582, Reward: -1201.0, avg loss: 8569330.000, eps: 0.456\n",
      "Episode: 583, Reward: 0.0, avg loss: 1581702656.000, eps: 0.456\n",
      "Episode: 584, Reward: -1201.0, avg loss: 39293032.000, eps: 0.456\n",
      "Episode: 585, Reward: -882.44, avg loss: 128091247.812, eps: 0.454\n",
      "Episode: 586, Reward: -1204.31, avg loss: 179803500.333, eps: 0.453\n",
      "Episode: 587, Reward: -281.4, avg loss: 2572898632.432, eps: 0.451\n",
      "Episode: 588, Reward: 0.0, avg loss: 781624256.000, eps: 0.450\n",
      "Episode: 589, Reward: -1222.15, avg loss: 1014276056.091, eps: 0.448\n",
      "Episode: 590, Reward: -1203.34, avg loss: 7010260400.000, eps: 0.447\n",
      "Episode: 591, Reward: -1203.01, avg loss: 2217616919.200, eps: 0.446\n",
      "Episode: 592, Reward: -1201.0, avg loss: 43752134.000, eps: 0.446\n",
      "Episode: 593, Reward: -1203.06, avg loss: 1975299236.391, eps: 0.445\n",
      "Episode: 594, Reward: -0.47, avg loss: 571126336.000, eps: 0.445\n",
      "Episode: 595, Reward: -32.760000000000005, avg loss: 116993068.381, eps: 0.442\n",
      "Episode: 596, Reward: -28.240000000000002, avg loss: 24679989.250, eps: 0.441\n",
      "Episode: 597, Reward: -1207.26, avg loss: 211682928.911, eps: 0.440\n",
      "Episode: 598, Reward: -5.55, avg loss: 27649916.500, eps: 0.439\n",
      "Episode: 599, Reward: -0.03, avg loss: 41529116.250, eps: 0.439\n",
      "Episode: 600, Reward: -1201.0, avg loss: 19810967552.000, eps: 0.438\n",
      "Episode: 601, Reward: -1201.0, avg loss: 35371151.750, eps: 0.438\n",
      "Episode: 602, Reward: -9.99, avg loss: 1929936841.333, eps: 0.437\n",
      "Episode: 603, Reward: -105.27, avg loss: 138342762.219, eps: 0.435\n",
      "Episode: 604, Reward: -1205.66, avg loss: 893459594.083, eps: 0.434\n",
      "Episode: 605, Reward: -1212.17, avg loss: 5798125874.261, eps: 0.431\n",
      "Episode: 606, Reward: 0.0, avg loss: 443574801.000, eps: 0.431\n",
      "Episode: 607, Reward: -1201.0, avg loss: 46662612.000, eps: 0.431\n",
      "Episode: 608, Reward: -1201.0, avg loss: 20631512.000, eps: 0.431\n",
      "Episode: 609, Reward: -1223.05, avg loss: 1568499563.833, eps: 0.429\n",
      "Episode: 610, Reward: -1229.16, avg loss: 3810234137.833, eps: 0.427\n",
      "Episode: 611, Reward: -88.96000000000001, avg loss: 2449495222.542, eps: 0.427\n",
      "Episode: 612, Reward: -18.11, avg loss: 6759572.000, eps: 0.427\n",
      "Episode: 613, Reward: -367.2, avg loss: 40878580.750, eps: 0.426\n",
      "Episode: 614, Reward: -385.65, avg loss: 154779426.875, eps: 0.424\n",
      "Episode: 615, Reward: -74.59, avg loss: 16675491.500, eps: 0.424\n",
      "Episode: 616, Reward: -1201.06, avg loss: 1176899238.000, eps: 0.423\n",
      "Episode: 617, Reward: -1225.11, avg loss: 75429497.179, eps: 0.422\n",
      "Episode: 618, Reward: -17.509999999999998, avg loss: 49585721.000, eps: 0.421\n",
      "Episode: 619, Reward: -1233.67, avg loss: 194190385.469, eps: 0.420\n",
      "Episode: 620, Reward: -1201.0, avg loss: 11006399488.000, eps: 0.420\n",
      "Episode: 621, Reward: -79.61, avg loss: 1190563413.025, eps: 0.418\n",
      "Episode: 622, Reward: -276.05, avg loss: 18656161.875, eps: 0.417\n",
      "Episode: 623, Reward: -1238.12, avg loss: 50194595.273, eps: 0.415\n",
      "Episode: 624, Reward: -1223.34, avg loss: 80873144.667, eps: 0.415\n",
      "Episode: 625, Reward: -24.45, avg loss: 808320863.156, eps: 0.413\n",
      "Episode: 626, Reward: -0.03, avg loss: 69509136.000, eps: 0.413\n",
      "Episode: 627, Reward: -1221.01, avg loss: 3241738.875, eps: 0.412\n",
      "Episode: 628, Reward: -1235.34, avg loss: 33547911.400, eps: 0.411\n",
      "Episode: 629, Reward: -9.51, avg loss: 100091.547, eps: 0.411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 630, Reward: -1214.66, avg loss: 55496305.812, eps: 0.410\n",
      "Episode: 631, Reward: -257.16, avg loss: 551216612.350, eps: 0.409\n",
      "Episode: 632, Reward: -682.0699999999999, avg loss: 1032312989.333, eps: 0.408\n",
      "Episode: 633, Reward: -1201.0, avg loss: 103307424.000, eps: 0.408\n",
      "Episode: 634, Reward: -1205.59, avg loss: 1971981242.750, eps: 0.408\n",
      "Episode: 635, Reward: -1201.0, avg loss: 1049465344.000, eps: 0.407\n",
      "Episode: 636, Reward: -1205.66, avg loss: 336090119.833, eps: 0.407\n",
      "Episode: 637, Reward: -2.5, avg loss: 84435960.000, eps: 0.406\n",
      "Episode: 638, Reward: -364.28, avg loss: 453192859.667, eps: 0.406\n",
      "Episode: 639, Reward: 505.96, avg loss: 49002490.083, eps: 0.405\n",
      "Episode: 640, Reward: -1230.03, avg loss: 186690597.568, eps: 0.403\n",
      "Episode: 641, Reward: -1231.85, avg loss: 78072840.861, eps: 0.401\n",
      "Episode: 642, Reward: -547.05, avg loss: 1733960576.000, eps: 0.401\n",
      "Episode: 643, Reward: -1217.18, avg loss: 592486000.789, eps: 0.400\n",
      "Episode: 644, Reward: -441.25, avg loss: 13972882.500, eps: 0.400\n",
      "Episode: 645, Reward: -214.6, avg loss: 18091478.707, eps: 0.398\n",
      "Episode: 646, Reward: -1201.0, avg loss: 10450158.000, eps: 0.398\n",
      "Episode: 647, Reward: -1174.41, avg loss: 35121287.951, eps: 0.397\n",
      "Episode: 648, Reward: -1219.65, avg loss: 29410893.516, eps: 0.396\n",
      "Episode: 649, Reward: 165.16000000000003, avg loss: 51197668.000, eps: 0.395\n",
      "Episode: 650, Reward: -1201.0, avg loss: 189375936.000, eps: 0.395\n",
      "Episode: 651, Reward: -1202.11, avg loss: 128844120.833, eps: 0.394\n",
      "Episode: 652, Reward: -1222.0, avg loss: 2265924150.143, eps: 0.393\n",
      "Episode: 653, Reward: 510.53, avg loss: 462935857.312, eps: 0.393\n",
      "Episode: 654, Reward: -1203.74, avg loss: 1405640426.656, eps: 0.392\n",
      "Episode: 655, Reward: -1203.0, avg loss: 320416949.500, eps: 0.392\n",
      "Episode: 656, Reward: -1206.0, avg loss: 4863402.191, eps: 0.391\n",
      "Episode: 657, Reward: -1203.05, avg loss: 7050420.292, eps: 0.390\n",
      "Episode: 658, Reward: -1205.2, avg loss: 30809421.984, eps: 0.389\n",
      "Episode: 659, Reward: -1201.0, avg loss: 1750518.875, eps: 0.389\n",
      "Episode: 660, Reward: -1201.0, avg loss: 34083708.000, eps: 0.389\n",
      "Episode: 661, Reward: -1014.6, avg loss: 70456532.000, eps: 0.389\n",
      "Episode: 662, Reward: -1201.0, avg loss: 7458785.000, eps: 0.388\n",
      "Episode: 663, Reward: -454.39, avg loss: 5765545.562, eps: 0.388\n",
      "Episode: 664, Reward: -595.02, avg loss: 8856036.149, eps: 0.386\n",
      "Episode: 665, Reward: -650.03, avg loss: 34216692.000, eps: 0.386\n",
      "Episode: 666, Reward: -32.4, avg loss: 736982460.278, eps: 0.384\n",
      "Episode: 667, Reward: -9.68, avg loss: 15707793.696, eps: 0.382\n",
      "Episode: 668, Reward: -0.86, avg loss: 1411715.000, eps: 0.382\n",
      "Episode: 669, Reward: -1203.7, avg loss: 1567363.938, eps: 0.382\n",
      "Episode: 670, Reward: -1288.84, avg loss: 30457690.821, eps: 0.380\n",
      "Episode: 671, Reward: -1202.14, avg loss: 25926107.500, eps: 0.380\n",
      "Episode: 672, Reward: -1203.69, avg loss: 8022920.586, eps: 0.379\n",
      "Episode: 673, Reward: -1203.0, avg loss: 1215823568.000, eps: 0.379\n",
      "Episode: 674, Reward: -1201.0, avg loss: 60497916.000, eps: 0.379\n",
      "Episode: 675, Reward: -403.78999999999996, avg loss: 21638319.800, eps: 0.377\n",
      "Episode: 676, Reward: -201.56, avg loss: 139428129.000, eps: 0.375\n",
      "Episode: 677, Reward: -2.0, avg loss: 207713000.000, eps: 0.375\n",
      "Episode: 678, Reward: -187.62, avg loss: 106022396.000, eps: 0.375\n",
      "Episode: 679, Reward: -311.2, avg loss: 35319980.000, eps: 0.375\n",
      "Episode: 680, Reward: -30.37, avg loss: 1814619.125, eps: 0.374\n",
      "Episode: 681, Reward: -0.1, avg loss: 23149204.000, eps: 0.374\n",
      "Episode: 682, Reward: -32.65, avg loss: 63331558.924, eps: 0.373\n",
      "Episode: 683, Reward: -1207.71, avg loss: 224397629.667, eps: 0.371\n",
      "Episode: 684, Reward: -7.47, avg loss: 20100326.000, eps: 0.371\n",
      "Episode: 685, Reward: -1203.0, avg loss: 33950762.375, eps: 0.371\n",
      "Episode: 686, Reward: -1203.0, avg loss: 16963069.000, eps: 0.371\n",
      "Episode: 687, Reward: -241.13, avg loss: 3029735.958, eps: 0.370\n",
      "Episode: 688, Reward: -942.43, avg loss: 8641847.000, eps: 0.370\n",
      "Episode: 689, Reward: -1221.9, avg loss: 310851133.799, eps: 0.368\n",
      "Episode: 690, Reward: -9.91, avg loss: 3433963.852, eps: 0.368\n",
      "Episode: 691, Reward: -13.51, avg loss: 152528.969, eps: 0.367\n",
      "Episode: 692, Reward: -1207.14, avg loss: 315862478.545, eps: 0.365\n",
      "Episode: 693, Reward: -1209.69, avg loss: 8199225.606, eps: 0.363\n",
      "Episode: 694, Reward: -43.26, avg loss: 1399657.000, eps: 0.363\n",
      "Episode: 695, Reward: -1201.0, avg loss: 22212018.000, eps: 0.363\n",
      "Episode: 696, Reward: -1203.23, avg loss: 30936487.500, eps: 0.362\n",
      "Episode: 697, Reward: -3.7799999999999994, avg loss: 19723767.811, eps: 0.361\n",
      "Episode: 698, Reward: -1211.77, avg loss: 10298739.777, eps: 0.360\n",
      "Episode: 699, Reward: -0.54, avg loss: 21985456.000, eps: 0.360\n",
      "Episode: 700, Reward: -160.29, avg loss: 47915034.000, eps: 0.358\n",
      "Episode: 701, Reward: -1201.0, avg loss: 1371459.875, eps: 0.357\n",
      "Episode: 702, Reward: -1201.0, avg loss: 4866485.250, eps: 0.357\n",
      "Episode: 703, Reward: -46.31, avg loss: 19666020.906, eps: 0.356\n",
      "Episode: 704, Reward: -75.5, avg loss: 26410873.246, eps: 0.354\n",
      "Episode: 705, Reward: -69.35, avg loss: 34777198.750, eps: 0.352\n",
      "Episode: 706, Reward: -0.09000000000000001, avg loss: 4066095.875, eps: 0.352\n",
      "Episode: 707, Reward: -295.0, avg loss: 21365295.500, eps: 0.351\n",
      "Episode: 708, Reward: -22.21, avg loss: 6001242.500, eps: 0.351\n",
      "Episode: 709, Reward: -1201.0, avg loss: 33504254.000, eps: 0.351\n",
      "Episode: 710, Reward: -7.25, avg loss: 36475355.442, eps: 0.349\n",
      "Episode: 711, Reward: -1209.85, avg loss: 289411669.797, eps: 0.348\n",
      "Episode: 712, Reward: -51.9, avg loss: 35897623.247, eps: 0.346\n",
      "Episode: 713, Reward: -98.98, avg loss: 28781618.000, eps: 0.346\n",
      "Episode: 714, Reward: -1542.34, avg loss: 41996558.786, eps: 0.345\n",
      "Episode: 715, Reward: -60.769999999999996, avg loss: 161775092.181, eps: 0.344\n",
      "Episode: 716, Reward: -1201.64, avg loss: 8358272.250, eps: 0.344\n",
      "Episode: 717, Reward: -1231.5, avg loss: 28926705.841, eps: 0.342\n",
      "Episode: 718, Reward: -375.37, avg loss: 33140592.609, eps: 0.341\n",
      "Episode: 719, Reward: -1201.07, avg loss: 812474082.500, eps: 0.341\n",
      "Episode: 720, Reward: -8.54, avg loss: 70175486.500, eps: 0.340\n",
      "Episode: 721, Reward: -117.94, avg loss: 511759996.000, eps: 0.340\n",
      "Episode: 722, Reward: -1232.34, avg loss: 1508457294.160, eps: 0.339\n",
      "Episode: 723, Reward: -1201.0, avg loss: 25915502.125, eps: 0.339\n",
      "Episode: 724, Reward: -7.43, avg loss: 503804325.708, eps: 0.338\n",
      "Episode: 725, Reward: -14.5, avg loss: 591976603.420, eps: 0.336\n",
      "Episode: 726, Reward: -78.07, avg loss: 664453498.591, eps: 0.334\n",
      "Episode: 727, Reward: -1227.94, avg loss: 1568909999.000, eps: 0.333\n",
      "Episode: 728, Reward: -1229.39, avg loss: 188688027.543, eps: 0.331\n",
      "Episode: 729, Reward: -1223.21, avg loss: 2660353.792, eps: 0.331\n",
      "Episode: 730, Reward: -15.819999999999999, avg loss: 9499493.250, eps: 0.330\n",
      "Episode: 731, Reward: -209.88, avg loss: 23337029.422, eps: 0.330\n",
      "Episode: 732, Reward: -129.28, avg loss: 81967669.083, eps: 0.329\n",
      "Episode: 733, Reward: 0.0, avg loss: 4940679.500, eps: 0.329\n",
      "Episode: 734, Reward: -1249.25, avg loss: 20891273.668, eps: 0.327\n",
      "Episode: 735, Reward: -1231.82, avg loss: 527664751.000, eps: 0.326\n",
      "Episode: 736, Reward: -2.61, avg loss: 102898969.938, eps: 0.325\n",
      "Episode: 737, Reward: -1232.42, avg loss: 94107043.429, eps: 0.324\n",
      "Episode: 738, Reward: -445.42, avg loss: 712099046.261, eps: 0.322\n",
      "Episode: 739, Reward: -3.0, avg loss: 2220513019.000, eps: 0.322\n",
      "Episode: 740, Reward: -111.48, avg loss: 10679572.000, eps: 0.322\n",
      "Episode: 741, Reward: -1471.1399999999999, avg loss: 131569347.488, eps: 0.320\n",
      "Episode: 742, Reward: -1213.15, avg loss: 89181628.258, eps: 0.319\n",
      "Episode: 743, Reward: -273.56, avg loss: 31670106.900, eps: 0.318\n",
      "Episode: 744, Reward: -1213.68, avg loss: 303411316.600, eps: 0.316\n",
      "Episode: 745, Reward: -36.17, avg loss: 525017527.591, eps: 0.315\n",
      "Episode: 746, Reward: -0.06, avg loss: 3145722.750, eps: 0.315\n",
      "Episode: 747, Reward: -1208.35, avg loss: 515239241.016, eps: 0.314\n",
      "Episode: 748, Reward: -1203.12, avg loss: 130943764.000, eps: 0.314\n",
      "Episode: 749, Reward: -1211.21, avg loss: 364858485.889, eps: 0.312\n",
      "Episode: 750, Reward: -1209.67, avg loss: 39342960.800, eps: 0.311\n",
      "Episode: 751, Reward: -1207.42, avg loss: 12600972.125, eps: 0.311\n",
      "Episode: 752, Reward: -34.699999999999996, avg loss: 267979968.996, eps: 0.309\n",
      "Episode: 753, Reward: -1210.18, avg loss: 8656022.161, eps: 0.308\n",
      "Episode: 754, Reward: -467.39, avg loss: 148934416.000, eps: 0.308\n",
      "Episode: 755, Reward: -38.04, avg loss: 26189902.773, eps: 0.306\n",
      "Episode: 756, Reward: -55.68, avg loss: 17946974.667, eps: 0.306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 757, Reward: -1201.0, avg loss: 3486783.000, eps: 0.306\n",
      "Episode: 758, Reward: -8.71, avg loss: 331611450.966, eps: 0.304\n",
      "Episode: 759, Reward: -16.4, avg loss: 72065184.330, eps: 0.303\n",
      "Episode: 760, Reward: -15.09, avg loss: 57744732.267, eps: 0.301\n",
      "Episode: 761, Reward: -712.39, avg loss: 23583958.000, eps: 0.301\n",
      "Episode: 762, Reward: -41.23, avg loss: 873225.062, eps: 0.301\n",
      "Episode: 763, Reward: -1235.42, avg loss: 251079632.275, eps: 0.299\n",
      "Episode: 764, Reward: -1207.2, avg loss: 492816769.250, eps: 0.299\n",
      "Episode: 765, Reward: -1203.27, avg loss: 12274697.750, eps: 0.298\n",
      "Episode: 766, Reward: -1207.47, avg loss: 18834976.938, eps: 0.297\n",
      "Episode: 767, Reward: -701.72, avg loss: 61083202.667, eps: 0.297\n",
      "Episode: 768, Reward: -434.17, avg loss: 155419455.617, eps: 0.296\n",
      "Episode: 769, Reward: -33.95, avg loss: 17152084.300, eps: 0.295\n",
      "Episode: 770, Reward: -1207.75, avg loss: 11835821.738, eps: 0.293\n",
      "Episode: 771, Reward: -287.69, avg loss: 179926643.977, eps: 0.292\n",
      "Episode: 772, Reward: -276.47, avg loss: 145129160.733, eps: 0.290\n",
      "Episode: 773, Reward: -207.4, avg loss: 38085680.500, eps: 0.289\n",
      "Episode: 774, Reward: -1209.84, avg loss: 19341000.614, eps: 0.287\n",
      "Episode: 775, Reward: -1008.2, avg loss: 53017556.000, eps: 0.287\n",
      "Episode: 776, Reward: -20.410000000000004, avg loss: 49023754.750, eps: 0.287\n",
      "Episode: 777, Reward: -1218.77, avg loss: 7307742.875, eps: 0.286\n",
      "Episode: 778, Reward: -12.57, avg loss: 2509075.875, eps: 0.286\n",
      "Episode: 779, Reward: -1231.82, avg loss: 136533182.011, eps: 0.284\n",
      "Episode: 780, Reward: -1201.0, avg loss: 12243241.000, eps: 0.284\n",
      "Episode: 781, Reward: -33.15, avg loss: 24193465.125, eps: 0.284\n",
      "Episode: 782, Reward: -1201.0, avg loss: 5859698.000, eps: 0.284\n",
      "Episode: 783, Reward: -13.36, avg loss: 5209753.375, eps: 0.283\n",
      "Episode: 784, Reward: -1209.46, avg loss: 102019979.554, eps: 0.282\n",
      "Episode: 785, Reward: -1203.12, avg loss: 1726172.250, eps: 0.282\n",
      "Episode: 786, Reward: -127.12, avg loss: 15545862.625, eps: 0.281\n",
      "Episode: 787, Reward: -35.22, avg loss: 6752440.750, eps: 0.279\n",
      "Episode: 788, Reward: -1201.0, avg loss: 80683384.000, eps: 0.279\n",
      "Episode: 789, Reward: -106.95999999999998, avg loss: 5521165.830, eps: 0.278\n",
      "Episode: 790, Reward: -1503.7999999999997, avg loss: 14900262.010, eps: 0.276\n",
      "Episode: 791, Reward: -1207.24, avg loss: 22735367.625, eps: 0.276\n",
      "Episode: 792, Reward: -82.74, avg loss: 3945600.108, eps: 0.274\n",
      "Episode: 793, Reward: -172.85999999999999, avg loss: 117511744.580, eps: 0.273\n",
      "Episode: 794, Reward: -836.5899999999999, avg loss: 18078556.274, eps: 0.271\n",
      "Episode: 795, Reward: -1207.86, avg loss: 11808630.692, eps: 0.270\n",
      "Episode: 796, Reward: -1209.17, avg loss: 3503685.266, eps: 0.270\n",
      "Episode: 797, Reward: -9.06, avg loss: 33379218.016, eps: 0.269\n",
      "Episode: 798, Reward: -124.72, avg loss: 46458252.000, eps: 0.269\n",
      "Episode: 799, Reward: -1201.0, avg loss: 26653682.000, eps: 0.269\n",
      "Episode: 800, Reward: -1209.7, avg loss: 15424359.000, eps: 0.268\n",
      "Episode: 801, Reward: -34.730000000000004, avg loss: 26449220.233, eps: 0.267\n",
      "Episode: 802, Reward: -1201.0, avg loss: 22237216.000, eps: 0.267\n",
      "Episode: 803, Reward: -1207.17, avg loss: 238964170.375, eps: 0.266\n",
      "Episode: 804, Reward: -1215.25, avg loss: 64610539.250, eps: 0.264\n",
      "Episode: 805, Reward: -1205.92, avg loss: 17807342.250, eps: 0.264\n",
      "Episode: 806, Reward: -8.579999999999998, avg loss: 3063782.285, eps: 0.263\n",
      "Episode: 807, Reward: -1205.42, avg loss: 8285151.958, eps: 0.263\n",
      "Episode: 808, Reward: -1213.47, avg loss: 24274966.068, eps: 0.262\n",
      "Episode: 809, Reward: -1230.0, avg loss: 5768710.477, eps: 0.260\n",
      "Episode: 810, Reward: -74.08999999999999, avg loss: 4337779.333, eps: 0.260\n",
      "Episode: 811, Reward: -1209.51, avg loss: 11002035.787, eps: 0.259\n",
      "Episode: 812, Reward: -1209.87, avg loss: 12296322.358, eps: 0.257\n",
      "Episode: 813, Reward: -1222.53, avg loss: 1828222.246, eps: 0.256\n",
      "Episode: 814, Reward: -64.69, avg loss: 20477290.958, eps: 0.256\n",
      "Episode: 815, Reward: -53.16, avg loss: 10443408.129, eps: 0.255\n",
      "Episode: 816, Reward: -1083.6000000000001, avg loss: 31352249.810, eps: 0.254\n",
      "Episode: 817, Reward: -2.52, avg loss: 788913.625, eps: 0.254\n",
      "Episode: 818, Reward: -707.08, avg loss: 326824763.812, eps: 0.253\n",
      "Episode: 819, Reward: -608.9, avg loss: 525294.875, eps: 0.253\n",
      "Episode: 820, Reward: -1201.0, avg loss: 160577.234, eps: 0.253\n",
      "Episode: 821, Reward: -149.86, avg loss: 47773125.000, eps: 0.253\n",
      "Episode: 822, Reward: -439.69, avg loss: 4773288.781, eps: 0.252\n",
      "Episode: 823, Reward: -6.72, avg loss: 12535721.367, eps: 0.252\n",
      "Episode: 824, Reward: -1214.8, avg loss: 9499169.396, eps: 0.251\n",
      "Episode: 825, Reward: -0.9400000000000001, avg loss: 1173198.229, eps: 0.251\n",
      "Episode: 826, Reward: -3.25, avg loss: 9885099.026, eps: 0.250\n",
      "Episode: 827, Reward: -13.24, avg loss: 21815499.792, eps: 0.250\n",
      "Episode: 828, Reward: -1203.0, avg loss: 2668888.583, eps: 0.249\n",
      "Episode: 829, Reward: -44.55, avg loss: 3279743.542, eps: 0.249\n",
      "Episode: 830, Reward: -1201.0, avg loss: 3392276.250, eps: 0.249\n",
      "Episode: 831, Reward: -3.33, avg loss: 2608456.167, eps: 0.248\n",
      "Episode: 832, Reward: -1207.81, avg loss: 11961011.318, eps: 0.247\n",
      "Episode: 833, Reward: -4.07, avg loss: 2693091.750, eps: 0.247\n",
      "Episode: 834, Reward: -41.53, avg loss: 10897102.969, eps: 0.246\n",
      "Episode: 835, Reward: -1262.92, avg loss: 73393163.854, eps: 0.244\n",
      "Episode: 836, Reward: -7.5200000000000005, avg loss: 109489486.635, eps: 0.244\n",
      "Episode: 837, Reward: -2071.65, avg loss: 40132248.464, eps: 0.242\n",
      "Episode: 838, Reward: -1201.0, avg loss: 5147216.500, eps: 0.242\n",
      "Episode: 839, Reward: -4.2299999999999995, avg loss: 70684986.611, eps: 0.241\n",
      "Episode: 840, Reward: -53.08, avg loss: 18892674.125, eps: 0.241\n",
      "Episode: 841, Reward: -5.71, avg loss: 2369951.417, eps: 0.240\n",
      "Episode: 842, Reward: -10.030000000000001, avg loss: 31539061.631, eps: 0.240\n",
      "Episode: 843, Reward: -245.73, avg loss: 13869768.375, eps: 0.239\n",
      "Episode: 844, Reward: -130.91, avg loss: 35658438.621, eps: 0.238\n",
      "Episode: 845, Reward: -1234.1, avg loss: 14777941.504, eps: 0.237\n",
      "Episode: 846, Reward: -420.71999999999997, avg loss: 8100635.569, eps: 0.236\n",
      "Episode: 847, Reward: -1205.16, avg loss: 15715150.109, eps: 0.235\n",
      "Episode: 848, Reward: -1201.0, avg loss: 10833978.000, eps: 0.235\n",
      "Episode: 849, Reward: -571.08, avg loss: 4172905.450, eps: 0.235\n",
      "Episode: 850, Reward: -1203.01, avg loss: 8949717.250, eps: 0.234\n",
      "Episode: 851, Reward: -1209.42, avg loss: 37091363.973, eps: 0.234\n",
      "Episode: 852, Reward: -1201.03, avg loss: 5250231.359, eps: 0.233\n",
      "Episode: 853, Reward: -2.6100000000000003, avg loss: 12541983.750, eps: 0.233\n",
      "Episode: 854, Reward: -1163.23, avg loss: 68142126.835, eps: 0.232\n",
      "Episode: 855, Reward: -1207.35, avg loss: 9514834.350, eps: 0.231\n",
      "Episode: 856, Reward: -726.0999999999999, avg loss: 10867320.958, eps: 0.231\n",
      "Episode: 857, Reward: -179.19, avg loss: 67583825.688, eps: 0.231\n",
      "Episode: 858, Reward: -23.92, avg loss: 1275978.573, eps: 0.230\n",
      "Episode: 859, Reward: -1201.0, avg loss: 20710728.000, eps: 0.230\n",
      "Episode: 860, Reward: -50.38999999999999, avg loss: 100267681.906, eps: 0.230\n",
      "Episode: 861, Reward: -1230.27, avg loss: 43104442.323, eps: 0.229\n",
      "Episode: 862, Reward: -1205.13, avg loss: 25341268.583, eps: 0.228\n",
      "Episode: 863, Reward: -1233.13, avg loss: 21543954.018, eps: 0.227\n",
      "Episode: 864, Reward: -0.060000000000000005, avg loss: 1982879.812, eps: 0.227\n",
      "Episode: 865, Reward: -54.11, avg loss: 22063493.088, eps: 0.226\n",
      "Episode: 866, Reward: -1206.78, avg loss: 1289657.792, eps: 0.225\n",
      "Episode: 867, Reward: -1214.74, avg loss: 7180718.846, eps: 0.225\n",
      "Episode: 868, Reward: -1209.09, avg loss: 8527090.206, eps: 0.224\n",
      "Episode: 869, Reward: -1224.6, avg loss: 3582935.982, eps: 0.223\n",
      "Episode: 870, Reward: -1209.82, avg loss: 8557065.497, eps: 0.222\n",
      "Episode: 871, Reward: -8.45, avg loss: 10829474.078, eps: 0.221\n",
      "Episode: 872, Reward: -1203.32, avg loss: 404139610.229, eps: 0.221\n",
      "Episode: 873, Reward: -1203.23, avg loss: 1170420.359, eps: 0.220\n",
      "Episode: 874, Reward: -1203.16, avg loss: 3231306.938, eps: 0.220\n",
      "Episode: 875, Reward: -1212.29, avg loss: 23204695.736, eps: 0.219\n",
      "Episode: 876, Reward: -1211.98, avg loss: 50517549.442, eps: 0.218\n",
      "Episode: 877, Reward: -355.54, avg loss: 19277058.727, eps: 0.217\n",
      "Episode: 878, Reward: -35.51, avg loss: 14319802.416, eps: 0.216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 879, Reward: -1208.72, avg loss: 15361184.917, eps: 0.215\n",
      "Episode: 880, Reward: -1227.17, avg loss: 53975615.750, eps: 0.215\n",
      "Episode: 881, Reward: 1536.0, avg loss: 7206998.188, eps: 0.214\n",
      "Episode: 882, Reward: -745.24, avg loss: 66154926.051, eps: 0.213\n",
      "Episode: 883, Reward: -1203.02, avg loss: 1671999.625, eps: 0.213\n",
      "Episode: 884, Reward: -1213.88, avg loss: 11536455.955, eps: 0.212\n",
      "Episode: 885, Reward: -1201.0, avg loss: 1077987.375, eps: 0.212\n",
      "Episode: 886, Reward: -533.75, avg loss: 55541305.364, eps: 0.211\n",
      "Episode: 887, Reward: -1253.57, avg loss: 222603985.159, eps: 0.210\n",
      "Episode: 888, Reward: -1209.23, avg loss: 5345841.044, eps: 0.209\n",
      "Episode: 889, Reward: -213.36999999999998, avg loss: 53852247.083, eps: 0.209\n",
      "Episode: 890, Reward: -4.41, avg loss: 40823687.163, eps: 0.209\n",
      "Episode: 891, Reward: -61.12, avg loss: 31473946.977, eps: 0.207\n",
      "Episode: 892, Reward: -1205.24, avg loss: 15091833.729, eps: 0.207\n",
      "Episode: 893, Reward: -1205.19, avg loss: 3218648.312, eps: 0.207\n",
      "Episode: 894, Reward: -1210.62, avg loss: 5119770.731, eps: 0.206\n",
      "Episode: 895, Reward: -1205.15, avg loss: 49311315.469, eps: 0.206\n",
      "Episode: 896, Reward: -67.42, avg loss: 19663659.531, eps: 0.205\n",
      "Episode: 897, Reward: -6.6, avg loss: 52663479.616, eps: 0.204\n",
      "Episode: 898, Reward: 4061.52, avg loss: 11918265.757, eps: 0.203\n",
      "Episode: 899, Reward: -1210.0, avg loss: 10890517.688, eps: 0.202\n",
      "Episode: 900, Reward: -1208.15, avg loss: 5604903.562, eps: 0.201\n",
      "Episode: 901, Reward: -46.91, avg loss: 6740371.685, eps: 0.201\n",
      "Episode: 902, Reward: -1.35, avg loss: 3795754.000, eps: 0.201\n",
      "Episode: 903, Reward: -192.47, avg loss: 64420948.000, eps: 0.201\n",
      "Episode: 904, Reward: -1205.51, avg loss: 2702673.000, eps: 0.201\n",
      "Episode: 905, Reward: -1222.24, avg loss: 35304690.852, eps: 0.199\n",
      "Episode: 906, Reward: -2.34, avg loss: 3356193.750, eps: 0.199\n",
      "Episode: 907, Reward: -33.49, avg loss: 22482987.911, eps: 0.198\n",
      "Episode: 908, Reward: -11.57, avg loss: 11389230.300, eps: 0.198\n",
      "Episode: 909, Reward: -298.58, avg loss: 63494077.719, eps: 0.197\n",
      "Episode: 910, Reward: -8.92, avg loss: 45930276.426, eps: 0.196\n",
      "Episode: 911, Reward: -1205.14, avg loss: 1241107.323, eps: 0.196\n",
      "Episode: 912, Reward: -1232.09, avg loss: 26630210.828, eps: 0.195\n",
      "Episode: 913, Reward: -22.610000000000003, avg loss: 5882385.489, eps: 0.194\n",
      "Episode: 914, Reward: -1213.11, avg loss: 8595543.761, eps: 0.193\n",
      "Episode: 915, Reward: -17.23, avg loss: 7109324.330, eps: 0.192\n",
      "Episode: 916, Reward: -1215.03, avg loss: 4041531.884, eps: 0.191\n",
      "Episode: 917, Reward: -452.02, avg loss: 2732060.040, eps: 0.190\n",
      "Episode: 918, Reward: -47.73, avg loss: 935256.125, eps: 0.190\n",
      "Episode: 919, Reward: -112.82, avg loss: 2203882.393, eps: 0.190\n",
      "Episode: 920, Reward: -118.77000000000001, avg loss: 7994650.830, eps: 0.189\n",
      "Episode: 921, Reward: -1209.76, avg loss: 13113918.625, eps: 0.188\n",
      "Episode: 922, Reward: -1207.5, avg loss: 1139202.599, eps: 0.187\n",
      "Episode: 923, Reward: -97.63, avg loss: 16254835.897, eps: 0.187\n",
      "Episode: 924, Reward: -3.4499999999999997, avg loss: 8321705.784, eps: 0.186\n",
      "Episode: 925, Reward: 0.0, avg loss: 1565092.750, eps: 0.186\n",
      "Episode: 926, Reward: -1207.29, avg loss: 4094718.362, eps: 0.185\n",
      "Episode: 927, Reward: -1207.06, avg loss: 2566590.676, eps: 0.184\n",
      "Episode: 928, Reward: -35.67, avg loss: 1989338.664, eps: 0.184\n",
      "Episode: 929, Reward: -1201.02, avg loss: 345555.547, eps: 0.184\n",
      "Episode: 930, Reward: -1228.48, avg loss: 17623477.222, eps: 0.183\n",
      "Episode: 931, Reward: -1210.89, avg loss: 11183776.678, eps: 0.182\n",
      "Episode: 932, Reward: -0.32, avg loss: 59511224.958, eps: 0.182\n",
      "Episode: 933, Reward: -1201.0, avg loss: 529916.188, eps: 0.182\n",
      "Episode: 934, Reward: -1144.59, avg loss: 11525840.415, eps: 0.181\n",
      "Episode: 935, Reward: -1209.83, avg loss: 3550037.375, eps: 0.180\n",
      "Episode: 936, Reward: -1267.2, avg loss: 46505887.354, eps: 0.179\n",
      "Episode: 937, Reward: -1235.84, avg loss: 36879648.540, eps: 0.178\n",
      "Episode: 938, Reward: -1209.54, avg loss: 110571117.868, eps: 0.177\n",
      "Episode: 939, Reward: -64.3, avg loss: 280241376.000, eps: 0.177\n",
      "Episode: 940, Reward: -100.9, avg loss: 11512794.355, eps: 0.177\n",
      "Episode: 941, Reward: -27.45, avg loss: 5541837.452, eps: 0.176\n",
      "Episode: 942, Reward: -53.97, avg loss: 22744500.585, eps: 0.175\n",
      "Episode: 943, Reward: -7.4, avg loss: 44330355.571, eps: 0.174\n",
      "Episode: 944, Reward: -1232.22, avg loss: 23669751.705, eps: 0.173\n",
      "Episode: 945, Reward: -11.729999999999999, avg loss: 173077751.227, eps: 0.173\n",
      "Episode: 946, Reward: -1207.38, avg loss: 697082680.600, eps: 0.172\n",
      "Episode: 947, Reward: -362.80999999999995, avg loss: 27040669.443, eps: 0.171\n",
      "Episode: 948, Reward: -292.45, avg loss: 26277040.000, eps: 0.171\n",
      "Episode: 949, Reward: -1234.78, avg loss: 40047400.375, eps: 0.170\n",
      "Episode: 950, Reward: -1210.13, avg loss: 270019844.857, eps: 0.170\n",
      "Episode: 951, Reward: -13.389999999999999, avg loss: 9926233.781, eps: 0.169\n",
      "Episode: 952, Reward: -1203.43, avg loss: 92109082.000, eps: 0.169\n",
      "Episode: 953, Reward: -1203.07, avg loss: 3457672.646, eps: 0.169\n",
      "Episode: 954, Reward: -7.79, avg loss: 22053806.000, eps: 0.168\n",
      "Episode: 955, Reward: -1203.17, avg loss: 59790070.000, eps: 0.168\n",
      "Episode: 956, Reward: -1210.39, avg loss: 29893816.562, eps: 0.167\n",
      "Episode: 957, Reward: -9.01, avg loss: 52708972.852, eps: 0.167\n",
      "Episode: 958, Reward: -1211.03, avg loss: 28521363.932, eps: 0.166\n",
      "Episode: 959, Reward: -1213.44, avg loss: 187075392.134, eps: 0.165\n",
      "Episode: 960, Reward: -0.35000000000000003, avg loss: 16790143.844, eps: 0.165\n",
      "Episode: 961, Reward: -8.209999999999999, avg loss: 87055015.850, eps: 0.164\n",
      "Episode: 962, Reward: -1205.21, avg loss: 78058808.833, eps: 0.164\n",
      "Episode: 963, Reward: -1201.0, avg loss: 19848676.000, eps: 0.163\n",
      "Episode: 964, Reward: -1201.01, avg loss: 13256291.719, eps: 0.163\n",
      "Episode: 965, Reward: -1230.01, avg loss: 39757702.784, eps: 0.162\n",
      "Episode: 966, Reward: -1225.46, avg loss: 18468960.136, eps: 0.162\n",
      "Episode: 967, Reward: -9.43, avg loss: 54596565.750, eps: 0.161\n",
      "Episode: 968, Reward: -1202.99, avg loss: 4304453.125, eps: 0.161\n",
      "Episode: 969, Reward: -1224.5, avg loss: 3920241.212, eps: 0.160\n",
      "Episode: 970, Reward: -1209.55, avg loss: 28791039.006, eps: 0.159\n",
      "Episode: 971, Reward: -88.41, avg loss: 33748034.744, eps: 0.159\n",
      "Episode: 972, Reward: -27.71, avg loss: 13893267.748, eps: 0.158\n",
      "Episode: 973, Reward: -1201.0, avg loss: 3261115.250, eps: 0.158\n",
      "Episode: 974, Reward: -1204.88, avg loss: 32134719.040, eps: 0.157\n",
      "Episode: 975, Reward: -1201.0, avg loss: 4847410.500, eps: 0.157\n",
      "Episode: 976, Reward: -94.88, avg loss: 109944694.994, eps: 0.156\n",
      "Episode: 977, Reward: -1296.6100000000001, avg loss: 15253104.634, eps: 0.156\n",
      "Episode: 978, Reward: -29.03, avg loss: 218520731.000, eps: 0.155\n",
      "Episode: 979, Reward: -18.95, avg loss: 407604.969, eps: 0.155\n",
      "Episode: 980, Reward: -1204.97, avg loss: 10797083.994, eps: 0.155\n",
      "Episode: 981, Reward: -1229.48, avg loss: 5787995.536, eps: 0.154\n",
      "Episode: 982, Reward: -10.79, avg loss: 38305319.394, eps: 0.153\n",
      "Episode: 983, Reward: -13.78, avg loss: 30561804.759, eps: 0.153\n",
      "Episode: 984, Reward: -66.93, avg loss: 4147688.450, eps: 0.153\n",
      "Episode: 985, Reward: -1206.51, avg loss: 10506052.588, eps: 0.152\n",
      "Episode: 986, Reward: -1201.0, avg loss: 1990673.000, eps: 0.152\n",
      "Episode: 987, Reward: -1217.01, avg loss: 50033556.193, eps: 0.151\n",
      "Episode: 988, Reward: -282.42, avg loss: 10475575.665, eps: 0.151\n",
      "Episode: 989, Reward: -1214.02, avg loss: 7669496.222, eps: 0.150\n",
      "Episode: 990, Reward: -181.83999999999997, avg loss: 14960566.400, eps: 0.149\n",
      "Episode: 991, Reward: -698.15, avg loss: 34742833.781, eps: 0.149\n",
      "Episode: 992, Reward: -1334.72, avg loss: 9977248.900, eps: 0.149\n",
      "Episode: 993, Reward: -45.92, avg loss: 24409955.414, eps: 0.148\n",
      "Episode: 994, Reward: -1213.3, avg loss: 9672617.239, eps: 0.147\n",
      "Episode: 995, Reward: -1201.0, avg loss: 32448272.000, eps: 0.147\n",
      "Episode: 996, Reward: -1225.26, avg loss: 10932543.113, eps: 0.147\n",
      "Episode: 997, Reward: -1233.13, avg loss: 17484501.946, eps: 0.146\n",
      "Episode: 998, Reward: -372.25000000000006, avg loss: 12792758.459, eps: 0.146\n",
      "Episode: 999, Reward: -1207.78, avg loss: 42652855.236, eps: 0.145\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "results() missing 1 required positional argument: 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-290-af4a14d387c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmDDQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddqn_get_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mDDQN_clf_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmDDQN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDDQN_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmDDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: results() missing 1 required positional argument: 'verbose'"
     ]
    }
   ],
   "source": [
    "%run ddqn.ipynb\n",
    "\n",
    "mDDQN = ddqn_get_model(train_data)\n",
    "DDQN_clf_r = results(mDDQN,clf_data)\n",
    "_,DDQN_f1 = mDDQN(np.array([],dtype=int),[])\n",
    "\n",
    "clf_DDQN = clf.get_clf(clf_data,DDQN_clf_r, DDQN_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "setexcoef(2.5)\n",
    "r=results(mDDQN,data_all,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n",
      "(3.1, [7, 3, 0, 18])\n"
     ]
    }
   ],
   "source": [
    "for r in w3:\n",
    "    print(w3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(mp(len,r[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[239.48, list([7, 1, 8, 4, 0, 6, 9, 2, 5, 3, 18])],\n",
       "       [9.62, list([7, 1, 8, 4, 0, 6, 9, 2, 5, 3, 18])],\n",
       "       [1212.05, list([7, 8, 4, 5, 9, 2, 1, 0, 6, 3, 6])],\n",
       "       ...,\n",
       "       [41.25, list([7, 8, 5, 9, 4, 2, 0, 6, 1, 3, 10])],\n",
       "       [1210.2, list([7, 1, 8, 4, 0, 6, 9, 2, 5, 3, 6])],\n",
       "       [379.39, list([7, 9, 1, 0, 2, 8, 5, 6, 4, 3, 10])]], dtype=object)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303.207571251549"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"ddqn1.csv\",fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"ddqn2.csv\",r[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([7, 1, 8, 4, 0, 6, 9, 2, 5, 3, 18]),\n",
       "       list([7, 1, 8, 4, 0, 6, 9, 2, 5, 3, 18]),\n",
       "       list([7, 8, 4, 5, 9, 2, 1, 0, 6, 3, 6]), ...,\n",
       "       list([7, 8, 5, 9, 4, 2, 0, 6, 1, 3, 10]),\n",
       "       list([7, 1, 8, 4, 0, 6, 9, 2, 5, 3, 6]),\n",
       "       list([7, 9, 1, 0, 2, 8, 5, 6, 4, 3, 10])], dtype=object)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(1027.1399999999999, [2, 5, 3, 7, 9, 6, 8, 4, 0, 1, 26]),\n",
       "       (46.79, [2, 5, 3, 7, 6, 9, 8, 4, 0, 1, 17]),\n",
       "       (12.190000000000001, [2, 5, 6, 0, 4, 3, 8, 1, 9, 7, 15]), ...,\n",
       "       (1235.48, [2, 5, 6, 0, 8, 4, 9, 3, 1, 7, 24]),\n",
       "       (1210.2, [2, 5, 6, 0, 8, 1, 7, 3, 4, 9, 24]),\n",
       "       (143.82999999999998, [2, 5, 6, 0, 8, 4, 9, 3, 1, 7, 7])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626.4766666666668"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=np.arange(0,20,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_c = data_all.copy()\n",
    "np.random.shuffle(data_all_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = []\n",
    "for v in vs:\n",
    "    setexcoef(v)\n",
    "    r=results(mDDQN,data_all_c[1:100],True)\n",
    "    m=np.mean(r[:,0])\n",
    "    fn.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319.27820941759603"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(vs,np.array(fn))\n",
    "plt.title(\"tuning DDQN exploration coefficient\")\n",
    "plt.xlabel(\"exploration coefficient\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.savefig(\"ddqnexpcoef.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5 4 9 2 1 28 \n",
      "8 1 9 5 18 \n",
      "8 9 5 1 4 2 6 28 \n",
      "8 28 \n",
      "8 1 9 5 18 \n",
      "8 5 2 7 4 1 9 6 3 0 22 \n",
      "8 9 1 5 4 2 28 \n",
      "8 28 \n",
      "8 1 9 5 7 2 18 \n",
      "8 1 9 2 5 28 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 4 0 22 \n",
      "8 28 \n",
      "8 1 9 5 28 \n",
      "8 1 9 2 5 28 \n",
      "8 1 9 5 4 0 22 \n",
      "8 1 9 5 0 22 \n",
      "8 1 9 5 28 \n",
      "8 28 \n",
      "8 1 9 5 18 \n",
      "8 5 4 9 2 1 28 \n",
      "8 9 1 5 2 4 7 6 3 0 22 \n",
      "8 28 \n",
      "8 1 9 2 5 28 \n",
      "8 28 \n",
      "8 5 7 4 2 1 9 6 0 22 \n",
      "8 1 9 0 2 5 28 \n",
      "8 5 22 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 28 \n",
      "8 1 9 2 5 28 \n",
      "8 1 9 2 5 7 4 18 \n",
      "8 1 9 5 4 0 22 \n",
      "8 1 9 5 0 22 \n",
      "8 1 9 5 18 \n",
      "8 5 22 \n",
      "8 1 9 2 5 28 \n",
      "8 1 9 5 28 \n",
      "8 1 9 5 28 \n",
      "8 1 9 5 18 \n",
      "8 28 \n",
      "8 5 4 1 2 6 9 23 \n",
      "8 1 9 2 5 28 \n",
      "8 28 \n",
      "8 1 9 5 28 \n",
      "8 28 \n",
      "8 28 \n",
      "8 28 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 28 \n",
      "8 1 9 2 5 28 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 4 2 7 6 0 18 \n",
      "8 1 9 5 18 \n",
      "8 1 9 2 5 28 \n",
      "8 28 \n",
      "8 28 \n",
      "8 5 22 \n",
      "8 1 9 2 5 28 \n",
      "8 1 9 5 28 \n",
      "8 5 22 \n",
      "8 5 18 \n",
      "8 1 9 5 18 \n",
      "8 28 \n",
      "8 5 4 2 9 7 1 0 22 \n",
      "8 5 9 4 1 28 \n",
      "8 1 9 5 0 22 \n",
      "8 28 \n",
      "8 28 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 18 \n",
      "8 5 4 7 9 2 1 6 3 0 22 \n",
      "8 1 9 5 2 7 \n",
      "8 9 1 5 18 \n",
      "8 1 9 0 22 \n",
      "8 1 9 2 5 28 \n",
      "8 28 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 0 22 \n",
      "8 28 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 18 \n",
      "8 1 9 5 18 \n",
      "8 28 \n",
      "8 1 9 2 5 28 \n",
      "8 28 \n",
      "8 1 9 0 22 \n",
      "8 5 7 4 2 1 9 0 22 \n",
      "8 1 9 2 5 28 \n",
      "8 28 \n",
      "8 9 1 5 2 4 7 6 0 22 \n",
      "8 28 \n",
      "8 1 9 5 18 \n",
      "8 1 9 0 22 \n",
      "8 1 9 2 5 28 \n",
      "8 9 1 5 2 7 4 6 3 0 22 \n",
      "8 1 9 2 5 28 \n",
      "8 28 \n",
      "8 28 \n"
     ]
    }
   ],
   "source": [
    "for row in r[:,1]:\n",
    "    for v in row:\n",
    "        print(v,end =\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_clf(truth, model,clf,SATzilla_t):\n",
    "    Bs = np.array([],dtype=int)\n",
    "    bs = []\n",
    "    timesofar = 0\n",
    "    first = True\n",
    "    while True:\n",
    "        typ, i = model(Bs,bs)\n",
    "        if typ == \"explore\":\n",
    "            Fv = Fvs[i]\n",
    "            Ft = Fts[i]\n",
    "            fv = truth[Fv]\n",
    "            ft = truth[Ft]\n",
    "\n",
    "            timesofar += ft\n",
    "            Bs = np.concatenate((Bs,Fv,[Ft]))\n",
    "            bs = np.concatenate((bs,fv,[ft]))\n",
    "            \n",
    "            if first==True\n",
    "                first==False\n",
    "            if clf(fv,ft) == \"SATzilla\":\n",
    "                return SATzilla_t\n",
    "        if typ == \"exploit\":\n",
    "            timesofar += truth[Rts[i]]\n",
    "            return timesofar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_based = results(mMVG,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_results = results(mMVG,class_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(class_data,class_results, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_model(Ks,ks):\n",
    "    if classifier("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
